{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42bd24d-9b17-445b-966a-ec07cd23f893",
   "metadata": {},
   "source": [
    "## CUNY DATA698\n",
    "### Topic Modeling for Forensics Analysis of Text-Based Conversations\n",
    "#### Michael Ippolito\n",
    "#### May 2024\n",
    "\n",
    "This is part of a series of Python Jupyter notebooks in support of my master's capstone project. The aim of the project is to study various methods of preprocessing, topic modeling, and postprocessing text-based conversation data often extracted from electronic devices recovered during criminal or cybersecurity investigations.\n",
    "\n",
    "The Jupyter notebooks used in this project are as follows:\n",
    "\n",
    "| Module | Purpose |\n",
    "|--------|---------|\n",
    "| eda1.ipynb | Exploratory data analysis of the four datasets used in the study. |\n",
    "| modeling1.ipynb | Loads and preprocesses the datasets, performs various topic models, postprocesses the topic representations. |\n",
    "| survey1.ipynb | Generates conversation text and topic representations to submit to Mechanical Turk. It later parses the results and incorporates them into my own hand-labeled results. |\n",
    "| survey2.ipynb | Loads Mechanical Turk survey results and evaluates them for quality based on reading speed and attention questinos. |\n",
    "| eval1.ipynb | Evaluates the topic modeling and survey results based on topic coherence, semantic quality, and topic relevance. |\n",
    "\n",
    "The study uses the following four datasets:\n",
    "\n",
    "1. Chitchat\n",
    "2. Topical Chat\n",
    "3. Ubuntu Dialogue\n",
    "4. Enron Email\n",
    "\n",
    "For further details and attribution, see my paper in this github repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c7adc-97da-4565-a38c-3d6f1d663cfc",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "#### modeling1.ipynb\n",
    "\n",
    "The code in this module loads and preprocesses the datasets, performs various topic models, and postprocesses the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcda66c-b173-4a98-94c6-81f70fc2fe43",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "This section does the following:\n",
    "\n",
    "1. Loads required libraries\n",
    "2. Sets global parameters used throughout the module\n",
    "3. Defines functions to load custom word-lists for spelling checking and for allow-listing certain words we don't want to spell-check\n",
    "4. Initialises the Spacy module used for tokenising\n",
    "5. Loads the spell-checker, custom word list, and allow-list\n",
    "6. Loads the text-speak conversion dictionary\n",
    "7. Initialises the part-of-speech mapping\n",
    "8. Loads stopword lists\n",
    "9. Initialises the sentence transformer model\n",
    "10. Loads pretrained Word2Vec, GloVe, and FastText word embeddings (takes a long time and a lot of memory resources)\n",
    "11. Loads Google's flan-t5-base text generator for postprocessing topic representations\n",
    "12. Defines functions to preprocess conversations by extracting keyphrases\n",
    "13. Defines functions to tokenise conversations using Spacy's tokeniser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dceeee26-7520-4eb5-a5c1-f606dd970c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import platform\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en import stop_words\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import ipywidgets\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import bertopic\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import TextGeneration, KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import chitchat_dataset as ccc\n",
    "from collections import Counter\n",
    "import random\n",
    "from gensim.models import fasttext\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6d8628-9e2c-4ab7-a87e-912fcbe19fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.12.2\n",
      "spacy version: 3.7.4\n",
      "NLTK version: 3.8.1\n",
      "bertopic version: 0.16.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "print(f'python version: {platform.python_version()}')\n",
    "print(f'spacy version: {spacy.__version__}')\n",
    "print(f'NLTK version: {nltk.__version__}')\n",
    "print(f'bertopic version: {bertopic.__version__}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d5c64e-9faa-45d9-97c1-03698e82acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "debug = False\n",
    "spacy_model = 'en_core_web_md'\n",
    "spell_distance = 1  # default distance=2, reduce to 1 for long lists of words\n",
    "topn_topic_words = 100\n",
    "embeddir = 'C:/Users/micha/Documents/698/embeddings/'\n",
    "embedmodel_word2vec = 'google-news/GoogleNews-vectors-negative300.bin'\n",
    "embedmodel_glove = 'stanford-glove/glove.6B.300d.txt'\n",
    "embedmodel_fasttext = 'fasttext/cc.en.300.bin.gz'\n",
    "transmodeldir = 'C:/Users/micha/Documents/698/transformers/'\n",
    "transmodel = 'all-MiniLM-L6-v2'\n",
    "pickle_dir = 'C:/tmp/pickles'\n",
    "num_topics_for_flan = 30  # Number of topics to use for flan representations; maybe vary this, e.g. [10, 25, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced69caa-c30e-4ba6-8cf3-1bad6a18654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load additional word dictionary\n",
    "def load_spell_addtl():\n",
    "    \"\"\"\n",
    "    Purpose:     To load custom spell-check words.\n",
    "    Parameters:  None\n",
    "    Returns:     None (words will be loaded into global variable \"spell\")\n",
    "    \"\"\"\n",
    "    with open('words_dictionary.json', 'r') as fh:\n",
    "        j = json.load(fh)\n",
    "    spell.word_frequency.load_words(j.keys())\n",
    "\n",
    "# Function to load spell deny list\n",
    "def load_spell_deny():\n",
    "    \"\"\"\n",
    "    Purpose:     To load the list of words we don't want to spell-check.\n",
    "    Parameters:  None\n",
    "    Returns:     None (denied words will be loaded into global list \"spell_deny\")\n",
    "    \"\"\"\n",
    "    global spell_deny\n",
    "    spell_deny = []\n",
    "    with open('spell_deny.txt', 'r') as fh:\n",
    "        while True:\n",
    "            l = fh.readline()\n",
    "            if not l: break\n",
    "            l = l.strip().replace('    \"', '').replace('\"', '')\n",
    "            aa = l.split(',')\n",
    "            spell_deny.append(aa[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb67bea-7bfc-4fa2-91f8-f1c9c1f37151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy model en_core_web_md\n",
      "loading spacy model en_core_web_md - tokeniser only\n",
      "\tall pipes: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "\tdisabling: ['parser', 'ner']\n",
      "loading spell checker\n",
      "loading additional spelling words\n",
      "loading spell deny list\n",
      "loading textspeak dict from textspeak.csv\n",
      "loading stopwords\n",
      "loading sentence transformer model all-MiniLM-L6-v2\n",
      "\t SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "instantiating chitchat dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load spacy model\n",
    "print(f'loading spacy model {spacy_model}')\n",
    "nlp = spacy.load(spacy_model)\n",
    "\n",
    "# Load spacy model - tokeniser only\n",
    "#print(f'loading spacy model {spacy_model} - tokeniser only')\n",
    "#nlp_tokens = spacy.load(spacy_model)\n",
    "#print('\\tall pipes:', [pipe for pipe in nlp_tokens.pipe_names])\n",
    "#all_but_tokeniser = [pipe for pipe in nlp_tokens.pipe_names if pipe != 'tok2vec']\n",
    "#all_but_tokeniser = [pipe for pipe in nlp_tokens.pipe_names]\n",
    "#nlp_tokens.disable_pipes(*all_but_tokeniser)\n",
    "\n",
    "# Load spacy model - tokeniser only\n",
    "print(f'loading spacy model {spacy_model} - tokeniser only')\n",
    "nlp_tokens = spacy.load(spacy_model)\n",
    "print('\\tall pipes:', [pipe for pipe in nlp_tokens.pipe_names])\n",
    "all_but_tagger = [pipe for pipe in nlp_tokens.pipe_names if pipe not in ['tagger', 'lemmatizer', 'tok2vec', 'attribute_ruler', 'morphologizer']]\n",
    "print('\\tdisabling:', all_but_tagger)\n",
    "nlp_tokens.disable_pipes(*all_but_tagger)\n",
    "#nlp_tokens.add_pipe('parser')\n",
    "nlp_tokens.add_pipe('sentencizer')  # Add sentencizer\n",
    "nlp_tokens.max_length = 13000000  # Increase the max length\n",
    "\n",
    "# Init spell checker\n",
    "print('loading spell checker')\n",
    "spell = SpellChecker(distance=spell_distance)\n",
    "\n",
    "# Load additional spelling words\n",
    "print('loading additional spelling words')\n",
    "load_spell_addtl()\n",
    "\n",
    "# Load spell deny list\n",
    "print('loading spell deny list')\n",
    "load_spell_deny()\n",
    "\n",
    "# Load textspeak dict\n",
    "print('loading textspeak dict from textspeak.csv')\n",
    "dfts = pd.read_csv('textspeak.csv', header=None, names=['text', 'trans'])\n",
    "dfts = dfts.map(str.lower)\n",
    "dfts = dfts.set_index('text')\n",
    "textspeak = dfts['trans'].to_dict()\n",
    "\n",
    "# Load part of speech map (spacy -> wordnet)\n",
    "posmap = {\n",
    "    'NOUN': 'n',\n",
    "    'PROPN': 'n',\n",
    "    'VERB': 'v',\n",
    "    'ADJ': 'a',\n",
    "    'ADV': 'r'\n",
    "}\n",
    "\n",
    "# Load stopwords\n",
    "print('loading stopwords')\n",
    "stopwords = stop_words.STOP_WORDS\n",
    "stopwords.update(['think', 'know', 'want', 'like', 'mean', 'lot', 's', 'day', 'days', 'going', 'eyes'])\n",
    "\n",
    "# Init sentence embedding model\n",
    "print(f'loading sentence transformer model {transmodel}')\n",
    "sentence_model = SentenceTransformer(transmodeldir + transmodel)\n",
    "print('\\t', sentence_model)\n",
    "\n",
    "# Instantiate chitchat dataset\n",
    "print('instantiating chitchat dataset')\n",
    "ccds = ccc.Dataset()\n",
    "print()\n",
    "\n",
    "# Custom stopwords (based on enron emails)\n",
    "custom_stop = ['message', 'subject', 'version', 'to', 'encoding', 'content', 'ect@ect', 'date', 'time', 'type', 'bcc', 'cc', 'text', 'transfer', 'charset', 'mime',\n",
    "                'folder', 'origin', '-0800', '-0700', 'pdt', 'pst', 'mail', 'enron', 'filename', 'corp', 'ascii', 'enron@enron', \n",
    "                'folders\\\\all', 'forwarded', 'folders\\\\\\'sent', 'ees@ees', 'email', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun', 'ect', 'hou']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b7ed52f1-e9ab-4a46-83b3-30a02c321094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained word2vec embeddings from C:/Users/micha/Documents/698/embeddings/google-news/GoogleNews-vectors-negative300.bin\n",
      "\tsize of pretrained model: 3000000 tokens with 300 vectors\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained word2vec embeddings\n",
    "print(f'loading pretrained word2vec embeddings from {embeddir}{embedmodel_word2vec}')\n",
    "ptmodel_word2vec = KeyedVectors.load_word2vec_format(embeddir + embedmodel_word2vec, binary=True)\n",
    "print(f'\\tsize of pretrained model: {len(ptmodel_word2vec)} tokens with {ptmodel_word2vec.vector_size} vectors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8e2b29a9-738d-46ba-81f4-c039a19a8f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained GloVe embeddings from C:/Users/micha/Documents/698/embeddings/stanford-glove/glove.6B.300d.txt\n",
      "\tsize of pretrained model: 400000 tokens with 300 vectors\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained GloVe embeddings\n",
    "print(f'loading pretrained GloVe embeddings from {embeddir}{embedmodel_glove}')\n",
    "ptmodel_glove = KeyedVectors.load_word2vec_format(embeddir + embedmodel_glove, binary=False, no_header=True)\n",
    "print(f'\\tsize of pretrained model: {len(ptmodel_glove)} tokens with {ptmodel_glove.vector_size} vectors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "86acac7a-1bd6-427f-aab4-30c8341cf19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained Fasttext embeddings from C:/Users/micha/Documents/698/embeddings/fasttext/cc.en.300.bin.gz\n",
      "\tsize of pretrained model: 2000000 tokens with 300 vectors\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Fasttext embeddings\n",
    "print(f'loading pretrained Fasttext embeddings from {embeddir}{embedmodel_fasttext}')\n",
    "ptmodel_fasttext = fasttext.load_facebook_model(f'{embeddir}{embedmodel_fasttext}')\n",
    "print(f'\\tsize of pretrained model: {len(ptmodel_fasttext.wv)} tokens with {ptmodel_fasttext.vector_size} vectors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b09a3576-84b3-49dc-8d54-bcffc698aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\tmp\\p312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init google flan-t5-base text generator\n",
    "generator = pipeline('text2text-generation', model=transmodeldir + 'google/flan-t5-base', max_new_tokens=300)\n",
    "\n",
    "# Init BERT c-tfidf model and sklearn's vectorizer model to remove stopwords in transformer models\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f5321c-8c23-40ff-9fc6-61e8463a32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyphrase extraction pipeline\n",
    "# (from https://huggingface.co/ml6team/keyphrase-extraction-kbir-inspec)\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    \"\"\"\n",
    "    Purpose:     To return keyphrases using keyphrase extraction LLM model by ml6team\n",
    "    \"\"\"\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        results = super().postprocess(\n",
    "            all_outputs=all_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "\n",
    "# Load pipeline\n",
    "keyphrase_model_name = \"C:/Users/micha/Documents/698/transformers/ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model=keyphrase_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c889286d-732c-4d3a-9a87-da3bada69bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 3 [('sentence', 2), ('Jim', 1)] ['sentence', 'Jim', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenise with spacy\n",
    "def do_tokenise(txt, topn, pos_list=None):\n",
    "    \"\"\"\n",
    "    Purpose:          Tokenises text using Spacy.\n",
    "    Parameters:\n",
    "        txt           String-type text to be tokenised.\n",
    "        topn          The number of most frequent words to return.\n",
    "        post_list     List of Spacy-formated parts of speech tags.\n",
    "    Returns:\n",
    "        ct_words      The word count.\n",
    "        ct_sents      The number of sentences in the text.\n",
    "        topn_words    The top-n most frequent words.\n",
    "        words         The tokens tokenised from the text (in Spacy format).\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenise\n",
    "    tokens = nlp_tokens(txt)\n",
    "    if not pos_list is None:\n",
    "        words = [token.text for token in tokens if not token.is_stop and not token.is_punct and not token.text == ' ' and len(token.text) > 2 and token.pos_ in pos_list and \\\n",
    "                token.text.lower() not in custom_stop]\n",
    "    else:\n",
    "        words = [token.text for token in tokens if not token.is_stop and not token.is_punct and not token.text == ' ' and len(token.text) > 2]\n",
    "    ct_words = len(tokens)\n",
    "    word_freq = Counter(words)\n",
    "    topn_words = word_freq.most_common(topn)\n",
    "    \n",
    "    # Sentencise\n",
    "    sents = tokens.sents\n",
    "    ct_sents = len(Counter(sents))\n",
    "\n",
    "    return ct_words, ct_sents, topn_words, words\n",
    "\n",
    "# Test suite\n",
    "ct_words, ct_sents, topn_words, tokens = do_tokenise(\"This is a sentence. Hello, Jim. This is another sentence.\", 5, ['NOUN', 'PROPN', 'VERB'])\n",
    "print(ct_words, ct_sents, topn_words, tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338b8d4-cd52-4a24-841b-d62591592fec",
   "metadata": {},
   "source": [
    "### Chitchat dataset\n",
    "\n",
    "This section loads the Chitchat dataset into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9e61d83-9ffe-487e-8d2c-fb693bb14baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Chitchat dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Chitchat dataset\n",
    "def load_chitchat(num_docs):\n",
    "    \"\"\"\n",
    "    Purpose:             To load the Chitchat dataset into memory.\n",
    "    Parameters:\n",
    "        num_docs         The number of conversations to load.\n",
    "    Returns:\n",
    "        docs_txt         List of conversations in plain-text format.\n",
    "        ct_msgs_corpus   Count of messages (conversations) in the corpus.\n",
    "        ct_chats_corpus  Count of chats within the corpus (each message can contain multiple chats).\n",
    "    \"\"\"\n",
    "\n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "    \n",
    "    # Iterate through items in dataset\n",
    "    ct = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_chats_corpus = []\n",
    "    for convo_id, convo in ccds.items():\n",
    "    \n",
    "        # Conversation header info\n",
    "        #print(convo_id, convo['ratings'], convo['start'], convo['prompt'], '\\n')\n",
    "        ct += 1\n",
    "    \n",
    "        # Init \n",
    "        doc_msgs = ''\n",
    "        \n",
    "        # Iterate through messages; each message is from a single person and contain multiple chats, e.g.:\n",
    "        # {\"messages\": [[{\"text\": \"Hello\", \"timestamp\": \"2018-05-02T19:38:15Z\", \"sender\": \"720840be-e522-47ba-9e9f-143f66372673\"}...\n",
    "        ct_msgs = 0\n",
    "        ct_chats = []\n",
    "        for msg in convo['messages']:\n",
    "    \n",
    "            # Concatenate all chats within this message\n",
    "            msg_chats = [chat['text'] for chat in msg]\n",
    "            ct_chat = len(msg_chats)\n",
    "            ct_chats.append(ct_chat)\n",
    "            msg_chats = ' '.join(msg_chats)            \n",
    "            doc_msgs += msg_chats + ' '\n",
    "            ct_msgs += 1\n",
    "\n",
    "        # Append count to the overall corpus counts (for stats purposes)\n",
    "        #print(ct_msgs)\n",
    "        #print(ct_chats)\n",
    "        ct_msgs_corpus.append(ct_msgs)\n",
    "        ct_chats_corpus.append(ct_chats)\n",
    "    \n",
    "        # Append to docs list\n",
    "        docs_txt.append(doc_msgs)\n",
    "    \n",
    "        # Show first few docs\n",
    "        \"\"\"\n",
    "        if ct < 6:\n",
    "            print(ct)\n",
    "            print(doc_msgs)\n",
    "            print()\n",
    "        \"\"\"\n",
    "    \n",
    "        # Break early\n",
    "        if num_docs > 0 and ct >= num_docs: break\n",
    "    \n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {len(docs_txt)}')\n",
    "\n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus, ct_chats_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80603c13-97c3-4db7-bf58-3c90e6a13fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs (conversations): 7168\n",
      "Hello How are you doing today? whats up MD im doing good how are you doing? Im alright, I just took a nap. But it was one of those naps that doesnt help anything. It just makes everything worse and you question all your life choices oh wow haha so you still feel tired huh? Yeah did you go to bed late? I have more of a head ache than anything else dude thats terrible No, Its just adjusting to a time change But enough about me What are you up to? im refining my skills on Sketch App have you heard of Sketch? I havent What is it? you know what adobe illustrator is right? Yes its a vector graphic editor like illustrator but its a lot more light weight and its for UX designers to design UI Oh nice! What have you been practicing designing Ive been trying to learn more tools on sketch and just get faster at it] Im doing a free lance job for a professor for his app idea and he needs a mockup of what the app will look like for his pitch Wow that is cool! When is that all due by? this weekend haha Do you think you will finish or is that a pushy deadline? its a deadline I set haha but I think Ill be able to finish I hope... if not I can push it back a little Well I wish you the best of luck with that deadline thanks do you have any personal projects you are working on\"? Not really projects. I finished editing a bunch of engagement pictures. So now all I am really doing is just practicing some sketching I can always practice and improve that skill oh coooool! was this a paid photoshoot? Yep, I had a few that I did before I went home for the summer engagement shoots or weddings are goooood money but i hear in utah its super competitive Yeah there are so many, every other person takes pictures Its easy to do it for friends and people you know but past that it is hard to get more business yeah true do you think if you wanted to pursue your photography you wuold have to move out of utah? Um maybe, People need pictures everywhere But I dont really want to do that, its fine for the side, but too hard to make a living on Remind me what you want to do career wise? I want to become a UX desiner if you werent Christian what religion would you be in if at all? Hm Maybe Buddhist What about you? I think its  a really peaceful practice and it would match my personality I probably would be buddhist culturally because IM korean but if i had a choice I wouldn't label myself as any religion egnostic? is that the term? I think that is a term but I dont know exactly what it means Thats really cool I need to go. Thanks for chatting! Hope you have a good day! \n",
      "35\n",
      "[2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 2, 2, 2, 1, 2, 2, 2, 3, 2, 4, 3, 2, 1, 1]\n",
      "(7168, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load chitchat dataset\n",
    "cc_txt, cc_msg_ct, cc_chat_ct = load_chitchat(0)\n",
    "print(cc_txt[0])\n",
    "print(cc_msg_ct[0])\n",
    "print(cc_chat_ct[0])\n",
    "\n",
    "# Make dataframe\n",
    "dfcc = pd.DataFrame({'txt': cc_txt, 'msg_ct': cc_msg_ct, 'chat_ct': cc_chat_ct})\n",
    "print(dfcc.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dfcc = dfcc[dfcc['msg_ct'] > 5].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ebacf0d-032b-4688-8c2e-b8a8d44eacef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello How are you doing today? whats up MD im ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hi anyone here hey whats up yeah how are you i...</td>\n",
       "      <td>57</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Hey! Hey I'm gonna close the other window if t...</td>\n",
       "      <td>18</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>I don't know what falafel is In fact I don't e...</td>\n",
       "      <td>25</td>\n",
       "      <td>[7, 2, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Helllo!!! Hello!  I think this program is bugg...</td>\n",
       "      <td>140</td>\n",
       "      <td>[1, 1, 3, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>7152</td>\n",
       "      <td>Hello? Hey Adam! How's it going? Did the baby ...</td>\n",
       "      <td>48</td>\n",
       "      <td>[1, 2, 2, 1, 1, 2, 2, 2, 4, 2, 1, 1, 1, 1, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>7154</td>\n",
       "      <td>Hi how are you hey doing great man how are you...</td>\n",
       "      <td>27</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>7157</td>\n",
       "      <td>If I were in power, I'd like to balance the bu...</td>\n",
       "      <td>29</td>\n",
       "      <td>[2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>7159</td>\n",
       "      <td>Hello hey! Kanye idk my dad? good choice thoug...</td>\n",
       "      <td>12</td>\n",
       "      <td>[1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>7165</td>\n",
       "      <td>Hm... I'd have to say I would spend it either ...</td>\n",
       "      <td>21</td>\n",
       "      <td>[2, 3, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                txt  msg_ct  \\\n",
       "0         0  Hello How are you doing today? whats up MD im ...      35   \n",
       "1         1  hi anyone here hey whats up yeah how are you i...      57   \n",
       "2        11  Hey! Hey I'm gonna close the other window if t...      18   \n",
       "3        14  I don't know what falafel is In fact I don't e...      25   \n",
       "4        20  Helllo!!! Hello!  I think this program is bugg...     140   \n",
       "...     ...                                                ...     ...   \n",
       "2958   7152  Hello? Hey Adam! How's it going? Did the baby ...      48   \n",
       "2959   7154  Hi how are you hey doing great man how are you...      27   \n",
       "2960   7157  If I were in power, I'd like to balance the bu...      29   \n",
       "2961   7159  Hello hey! Kanye idk my dad? good choice thoug...      12   \n",
       "2962   7165  Hm... I'd have to say I would spend it either ...      21   \n",
       "\n",
       "                                                chat_ct  \n",
       "0     [2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 2, ...  \n",
       "1     [2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, ...  \n",
       "2     [1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1, 2, ...  \n",
       "3     [7, 2, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 4, ...  \n",
       "4     [1, 1, 3, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 1, ...  \n",
       "...                                                 ...  \n",
       "2958  [1, 2, 2, 1, 1, 2, 2, 2, 4, 2, 1, 1, 1, 1, 3, ...  \n",
       "2959  [1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, ...  \n",
       "2960  [2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, ...  \n",
       "2961               [1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2]  \n",
       "2962  [2, 3, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, ...  \n",
       "\n",
       "[2963 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(dfcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe1132-666e-4865-ab7f-1a4884596811",
   "metadata": {},
   "source": [
    "### Topical Chat dataset\n",
    "\n",
    "This section loads the Topical Chat dataset into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c270c5-8e5d-4102-a4f8-b2c7fbed8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Topical Chat dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Topical Chat dataset\n",
    "def load_topical(num_docs):\n",
    "    \"\"\"\n",
    "    Purpose:             To load the Topical Chat dataset into memory.\n",
    "    Parameters:\n",
    "        num_docs         The number of conversations to load.\n",
    "    Returns:\n",
    "        docs_txt         List of conversations in plain-text format.\n",
    "        ct_msgs_corpus   Count of messages (conversations) in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path\n",
    "    path_to_docs = 'C:/Users/micha/Documents/698/corpora/topical_chat/train.json'\n",
    "    \n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "\n",
    "    # Load file\n",
    "    with open(path_to_docs, 'r', encoding='latin-1') as fh:\n",
    "\n",
    "        j = json.load(fh)\n",
    "\n",
    "    # Iterate through items in dataset\n",
    "    ct = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_chats_corpus = []\n",
    "    for k in j.keys():\n",
    "\n",
    "        ct += 1\n",
    "\n",
    "        # Get conversation\n",
    "        conv = j[k]['content']\n",
    "\n",
    "        # Init \n",
    "        doc_msgs = ''\n",
    "        \n",
    "        # Iterate over each message in the conversation; each message is by one person and can contain multiple sentences\n",
    "        ct_msgs = 0\n",
    "        for msg in conv:\n",
    "\n",
    "            # Concatenate all chats within this message\n",
    "            msg_txt = msg['message']\n",
    "            doc_msgs += msg_txt + ' '\n",
    "            ct_msgs += 1\n",
    "\n",
    "        # Append count to the overall corpus counts (for stats purposes)\n",
    "        ct_msgs_corpus.append(ct_msgs)\n",
    "    \n",
    "        # Append to docs list\n",
    "        docs_txt.append(doc_msgs)\n",
    "    \n",
    "        # Break early\n",
    "        if num_docs > 0 and ct >= num_docs: break\n",
    "    \n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {len(docs_txt)}')\n",
    "\n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf30a56e-5589-408f-9de2-232990643308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs (conversations): 3000\n",
      "(3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load topical chat dataset\n",
    "tc_txt, tc_msg_ct = load_topical(3000)\n",
    "\n",
    "# Make dataframe\n",
    "dftc = pd.DataFrame({'txt': tc_txt, 'msg_ct': tc_msg_ct, 'chat_ct': 1})\n",
    "print(dftc.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dftc = dftc[dftc['msg_ct'] > 5].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c63c1ebc-167e-4230-a397-48176a33d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Are you a fan of Google or Microsoft? Both are...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>do you like dance? Yes  I do. Did you know Bru...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey what's up do use Google very often?I reall...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi!  do you like to dance? I love to dance a l...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>do you like dance? I love it. Did you know Bru...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>do you like fiction? I sure do. Did you know t...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>Hello! Do you like to read? I love reading! Yo...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>Did you know that the sun takes up 99.86% of t...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>Greetings! I hope you are well. These readings...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>I'm reading a sad article this time. Hello,  h...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                txt  msg_ct  \\\n",
       "0         0  Are you a fan of Google or Microsoft? Both are...      21   \n",
       "1         1  do you like dance? Yes  I do. Did you know Bru...      21   \n",
       "2         2  Hey what's up do use Google very often?I reall...      21   \n",
       "3         3  Hi!  do you like to dance? I love to dance a l...      23   \n",
       "4         4  do you like dance? I love it. Did you know Bru...      21   \n",
       "...     ...                                                ...     ...   \n",
       "2995   2995  do you like fiction? I sure do. Did you know t...      21   \n",
       "2996   2996  Hello! Do you like to read? I love reading! Yo...      21   \n",
       "2997   2997  Did you know that the sun takes up 99.86% of t...      22   \n",
       "2998   2998  Greetings! I hope you are well. These readings...      22   \n",
       "2999   2999  I'm reading a sad article this time. Hello,  h...      22   \n",
       "\n",
       "      chat_ct  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "2995        1  \n",
       "2996        1  \n",
       "2997        1  \n",
       "2998        1  \n",
       "2999        1  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(dftc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c9c7c-3856-4d1f-ba19-33c1df556057",
   "metadata": {},
   "source": [
    "### Ubuntu Dialogue dataset\n",
    "\n",
    "This section loads the Ubuntu Dialogue dataset into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8eb56d4-f5e6-49b1-add6-9e1e91ad860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Ubuntu Dialogue dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Ubuntu Dialogue dataset\n",
    "def load_ubuntu(num_docs):\n",
    "    \"\"\"\n",
    "    Purpose:             To load the Ubuntu Dialogue dataset into memory.\n",
    "    Parameters:\n",
    "        num_docs         The number of conversations to load.\n",
    "    Returns:\n",
    "        docs_txt         List of conversations in plain-text format.\n",
    "        ct_msgs_corpus   Count of messages (conversations) in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path\n",
    "    path_to_docs = 'C:/Users/micha/Documents/698/corpora/ubuntu_dialogues/dialogs'\n",
    "    \n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "\n",
    "    # Iterate over directories\n",
    "    ct_conv = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_err = 0\n",
    "    for d in os.listdir(path_to_docs):\n",
    "\n",
    "        # Iterate over files in directory\n",
    "        print('dir', d)\n",
    "        for f in os.listdir(path_to_docs + '/' + d):\n",
    "\n",
    "            # Verify it's a file\n",
    "            fn = path_to_docs + '/' + d + '/' + f\n",
    "            if os.path.isfile(fn):\n",
    "\n",
    "                # Init \n",
    "                doc_msgs = ''\n",
    "        \n",
    "                # Read the file\n",
    "                with open(fn, 'r', encoding='latin-1') as fh:\n",
    "\n",
    "                    # Reach each line; each line is a separate message from a single user\n",
    "                    ct_conv += 1\n",
    "                    ct_msgs = 0\n",
    "                    while True:\n",
    "\n",
    "                        # Read line\n",
    "                        l = fh.readline()\n",
    "                        if not l:\n",
    "                            break\n",
    "\n",
    "                        # Split to an array; each line will be in this format: timestamp[tab]sender[tab]receiver[tab]message\n",
    "                        # e.g.: 2005-05-26T16:54:00.000Z[tab]lifeless[tab]we2by[tab]calm down please\n",
    "                        tmp = l.strip().split('\\t')\n",
    "                        if len(tmp) == 4:\n",
    "\n",
    "                            # Concatenate to all the messages in the conversation\n",
    "                            doc_msgs += tmp[3] + ' '\n",
    "                            ct_msgs += 1\n",
    "                            \n",
    "                        else:\n",
    "                            # Not the right number of fields in this message\n",
    "                            ct_err += 1\n",
    "                            \n",
    "                # Append count to the overall corpus counts (for stats purposes)\n",
    "                ct_msgs_corpus.append(ct_msgs)\n",
    "\n",
    "                # Append to docs list\n",
    "                docs_txt.append(doc_msgs)\n",
    "\n",
    "            # Break early\n",
    "            if ct_conv % 1000 == 0: print(ct_conv)\n",
    "            if num_docs > 0 and ct_conv >= num_docs:\n",
    "                break\n",
    "                \n",
    "        # Break early\n",
    "        if ct_conv % 1000 == 0: print(ct_conv)\n",
    "        if num_docs > 0 and ct_conv >= num_docs:\n",
    "            break\n",
    "\n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {ct_conv}')\n",
    "    print(f'Number of errs: {ct_err}')\n",
    "    \n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92fd73f2-501f-4935-9310-2ee288ad0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 10\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "10000\n",
      "Number of docs (conversations): 10000\n",
      "Number of errs: 12\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load Ubuntu Dialogue data set\n",
    "ud_txt, ud_msg_ct = load_ubuntu(10000)\n",
    "\n",
    "# Make dataframe\n",
    "dfud = pd.DataFrame({'txt': ud_txt, 'msg_ct': ud_msg_ct, 'chat_ct': 1})\n",
    "print(dfud.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dfud = dfud[dfud['msg_ct'] > 5].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf2a335a-40d5-4d1d-a520-2fafb7c14ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hi sudo echo Y &gt; /sys/module/usbcore/parameter...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hmm Why doesn't GLX work with X.Org (I just ch...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hi can someone tell me where shell prompt name...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How do I boot in safe mode with 12.04?  you me...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hello, I have a minimal linux system: how can ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>I have 2 partitions, one as / and the other as...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>hey can you dual boot ubuntu server? now will ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>hi ppl Q: I've got a program running in wine, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>Q: Whats the best alternative to mIrc or hydra...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>hey all. I just accidentally sudo make install...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                txt  msg_ct  \\\n",
       "0         0  hi sudo echo Y > /sys/module/usbcore/parameter...      10   \n",
       "1         1  Hmm Why doesn't GLX work with X.Org (I just ch...      10   \n",
       "2         2  hi can someone tell me where shell prompt name...      10   \n",
       "3         3  How do I boot in safe mode with 12.04?  you me...      10   \n",
       "4         4  Hello, I have a minimal linux system: how can ...      10   \n",
       "...     ...                                                ...     ...   \n",
       "9995   9995  I have 2 partitions, one as / and the other as...      10   \n",
       "9996   9996  hey can you dual boot ubuntu server? now will ...      10   \n",
       "9997   9997  hi ppl Q: I've got a program running in wine, ...      10   \n",
       "9998   9998  Q: Whats the best alternative to mIrc or hydra...      10   \n",
       "9999   9999  hey all. I just accidentally sudo make install...      10   \n",
       "\n",
       "      chat_ct  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "9995        1  \n",
       "9996        1  \n",
       "9997        1  \n",
       "9998        1  \n",
       "9999        1  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(dfud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13ca98-08e5-4b40-b29e-650f2037b699",
   "metadata": {},
   "source": [
    "### Enron Email dataset\n",
    "\n",
    "This section loads the Enron Email dataset into memory. It also defines a function to process the data in order to remove email headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "721fc95c-c447-42f0-b98c-bcafae824e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Enron Email dataset\n",
    "#############################################\n",
    "\n",
    "# Preload enron emails into dataframe\n",
    "path_to_emails = 'C:/Users/micha/Documents/698/corpora/enron_emails/emails.csv'\n",
    "dfee = pd.read_csv(path_to_emails)\n",
    "dfee.drop(columns=['file'], inplace=True)\n",
    "dfee['msg_ct'] = 1  # just assume each email = 1 message = 1 conversation\n",
    "dfee['chat_ct'] = 1\n",
    "dfee.rename(columns={'message': 'txt'}, inplace=True)\n",
    "dfee = dfee[:5000]\n",
    "dfee['index'] = dfee.index\n",
    "print(dfee.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34b35fd6-6a69-4a95-be87-5553c8169ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary,\n",
      "\n",
      " It is OK to buy a carpet shampooer.\n",
      " About the W-2's, how would you\n",
      "***************************************************************\n",
      "***************************************************************\n",
      "***************************************************************\n",
      "Message-ID: <17449361.1075855672476.JavaMail.evans@thyme>\n",
      "Date: Mon, 31 Dec 1979 16:00:00 -0800 (PST)\n",
      "From: phillip.allen@enron.com\n",
      "To: maryrichards7@hotmail.com\n",
      "Subject: Re:\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: \"mary richards\" <maryrichards7@hotmail.com> @ ENRON\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Dec2000\\Notes Folders\\All documents\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen.nsf\n",
      "\n",
      "Mary,\n",
      "\n",
      " It is OK to buy a carpet shampooer.\n",
      " About the W-2's, how would you \n"
     ]
    }
   ],
   "source": [
    "# Function to remove header info from Enron emails\n",
    "def proc_email(msg):\n",
    "    \"\"\"\n",
    "    Purpose:             To remove email headers from Enron emails.\n",
    "    Parameters:\n",
    "        num_docs         The text of the message.\n",
    "    Returns:\n",
    "        r                The text of the message with header information stripped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init processed msg\n",
    "    r = ''\n",
    "\n",
    "    # Find subject\n",
    "    subj = ''\n",
    "    i = msg.find('Subject: ')\n",
    "    if i > -1:\n",
    "        j = msg.find('\\n', i)\n",
    "        if j > -1:\n",
    "            subj = msg[i + len('Subject: '):j]\n",
    "\n",
    "    # Strip off 'Re:' and 'Fwd:'\n",
    "    while True:\n",
    "        if subj[0:3].lower() == 're:':\n",
    "            subj = subj[3:]\n",
    "        elif subj[0:4].lower() == 'fwd:':\n",
    "            subj = subj[4:]\n",
    "        else:\n",
    "            break\n",
    "    subj = subj.strip()\n",
    "\n",
    "    # Find the first double \\n; this should be the start of the message\n",
    "    i = msg.find('\\n\\n')\n",
    "    r = subj + '\\n\\n'\n",
    "    if i > -1:\n",
    "        r += msg[i + 2:]\n",
    "\n",
    "    # Return\n",
    "    return r.strip()\n",
    "\n",
    "# Test suite\n",
    "msg_before = dfee.loc[dfee.index == random.randint(0, 5000), 'txt'].values[0]\n",
    "msg_after = proc_email(msg_before)\n",
    "print(msg_after)\n",
    "print('***************************************************************')\n",
    "print('***************************************************************')\n",
    "print('***************************************************************')\n",
    "print(msg_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88c17e5e-64e5-4050-8b75-ca470e7415c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header info from Enron emails\n",
    "dfee['txt'] = dfee['txt'].apply(proc_email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5448470c-52a0-4091-bc60-821063fad9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test\\n\\ntest successful.  way to go!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello\\n\\nLet's shoot for Tuesday at 11:45.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Enron Mentions\\n\\nSTOCKWATCH Enron higher afte...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Back office issues\\n\\nWE ARE HAVING A MEETING ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>APB\\n\\nPlease click on the link below for a Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>25% ACROSS THE BOARD REDUCTION IN ENE HEADCOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Please Register to Attend the Enron Management...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt  msg_ct  chat_ct  \\\n",
       "0                                  Here is our forecast       1        1   \n",
       "1     Traveling to have a business meeting takes the...       1        1   \n",
       "2                test\\n\\ntest successful.  way to go!!!       1        1   \n",
       "3     Randy,\\n\\n Can you send me a schedule of the s...       1        1   \n",
       "4            Hello\\n\\nLet's shoot for Tuesday at 11:45.       1        1   \n",
       "...                                                 ...     ...      ...   \n",
       "4995  Enron Mentions\\n\\nSTOCKWATCH Enron higher afte...       1        1   \n",
       "4996  Back office issues\\n\\nWE ARE HAVING A MEETING ...       1        1   \n",
       "4997  APB\\n\\nPlease click on the link below for a Ha...       1        1   \n",
       "4998  25% ACROSS THE BOARD REDUCTION IN ENE HEADCOUN...       1        1   \n",
       "4999  Please Register to Attend the Enron Management...       1        1   \n",
       "\n",
       "      index  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         3  \n",
       "4         4  \n",
       "...     ...  \n",
       "4995   4995  \n",
       "4996   4996  \n",
       "4997   4997  \n",
       "4998   4998  \n",
       "4999   4999  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(dfee)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82715d-b451-4027-ab9d-2ce39f522d7c",
   "metadata": {},
   "source": [
    "### Preprocessing and helper functions\n",
    "\n",
    "This section defines several functions to help with preprocessing conversations:\n",
    "\n",
    "| Function Name | Purpose |\n",
    "|---------------|---------|\n",
    "| check_spelling() | Corrects spelling errors |\n",
    "| trans_textspeak() | Converts text-speak and slang into standard English |\n",
    "| get_synonyms() | Adds synonyms to conversations |\n",
    "| get_hypernyms() | Adds hypernyms to conversations |\n",
    "| extract_keyphrases() | Extracts keyphrases from conversations using LLM |\n",
    "| preprocess_doc() | Preprocesses a conversation according to the specified parameters |\n",
    "| preprocess_docs() | Preprocesses a set of conversations by iterating through the conversations and sending each one to preprocess_doc() |\n",
    "| get_coherence_scores() | Calcuates the specified coherence scores for a given list of topics against a set of documents. |\n",
    "| run_model() | Runs the specified model against a set of documents and calculates coherence scores. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1959df11-4c60-42cf-aa48-4fa0e42002ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want some pizza with mushooms--and I mean all kinds of mushooms!--on it. What the hel, you just sol me taht thing.\n",
      "I want some pizza with mushrooms--and I mean all kinds of mushrooms!--on it. What the hel, you just sol me that thing.\n",
      "\n",
      "Just came back from Brazil. First, I was in Rome, and Larry David was there. Shelby, too.\n",
      "Just came back from Brazil. First, I was in Rome, and Larry David was there. Shelby, too.\n",
      "\n",
      "Kerry aikman program(s are rich.\n",
      "Kerry aikman programs are rich.\n",
      "\n",
      "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage. Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the 2010 marriage license application, according to court documents. Prosecutors said the marriages were part of an immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages. Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted. The case was referred to the Bronx District Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n"
     ]
    }
   ],
   "source": [
    "# Function to check spelling on a plain-text doc, returns spell-checked plain text\n",
    "def check_spelling(doc, print_corrs=False):\n",
    "    \"\"\"\n",
    "    Purpose:         To correct spelling errors in the given text.\n",
    "    Parameters:\n",
    "        doc          The plain-text document in string format.\n",
    "        print_corrs  Boolean indicating whether to print spelling corrections to STDOUT, useful for debugging or adding words to the deny-list.\n",
    "    Returns:\n",
    "        r            Spell-checked plain-text document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return\n",
    "    doc = doc.replace('\\r\\n', ' ').replace('\\n', ' ')\n",
    "    r = doc\n",
    "\n",
    "    # Get tokenised doc\n",
    "    tokens = nlp(doc)\n",
    "\n",
    "    # Get lowercase entities; this will be a list of entities, but they could be ngrams so need to split by spaces\n",
    "    lower_ents_tmp = [token.text.lower() for token in tokens.ents]\n",
    "    lower_ents = []\n",
    "    [lower_ents.extend(ent.split(' ')) for ent in lower_ents_tmp]\n",
    "\n",
    "    # Check spelling, removing any named entities\n",
    "    misspelled = spell.unknown([token.text.lower() for token in tokens if token.text.lower() not in lower_ents])\n",
    "\n",
    "    # Replace misspelled words\n",
    "    for word in misspelled:\n",
    "    \n",
    "        # Find correct spelling\n",
    "        corr = spell.correction(word)\n",
    "    \n",
    "        # If there is a corrected spelling, add it to the new sentence fragment\n",
    "        if corr is not None and word not in spell_deny:\n",
    "\n",
    "            # Replace the misspelled word\n",
    "            if print_corrs:\n",
    "                print(f\"    \\\"{word}\\\",  # {corr}\")\n",
    "            #r = r.replace(word, corr)\n",
    "            r = re.sub(re.escape(word), corr, r, flags=re.I)  # to make the replacement case-insensitive\n",
    "\n",
    "    # Return spell-checked tokens\n",
    "    if print_corrs: print()\n",
    "    return r\n",
    "\n",
    "# Test suite\n",
    "docs = [\n",
    "    \"I want some pizza with mushooms--and I mean all kinds of mushooms!--on it. What the hel, you just sol me taht thing.\",\n",
    "    \"Just came back from Brazil. First, I was in Rome, and Larry David was there. Shelby, too.\",\n",
    "    #\"Hello How are you doing today? whats up MD im doing good how are you doing? Im alright, I just took a nap. But it was one of those naps that doesnt help anything. It just makes everything worse and you question all your life choices oh wow haha so you still feel tired huh? Yeah did you go to bed late? I have more of a head ache than anything else dude thats terrible No, Its just adjusting to a time change But enough about me What are you up to? im refining my skills on Sketch App have you heard of Sketch? I havent What is it? you know what adobe illustrator is right? Yes its a vector graphic editor like illustrator but its a lot more light weight and its for UX designers to design UI Oh nice! What have you been practicing designing Ive been trying to learn more tools on sketch and just get faster at it] Im doing a free lance job for a professor for his app idea and he needs a mockup of what the app will look like for his pitch Wow that is cool! When is that all due by? this weekend haha Do you think you will finish or is that a pushy deadline? its a deadline I set haha but I think Ill be able to finish I hope... if not I can push it back a little Well I wish you the best of luck with that deadline thanks do you have any personal projects you are working on? Not really projects. I finished editing a bunch of engagement pictures. So now all I am really doing is just practicing some sketching I can always practice and improve that skill oh coooool! was this a paid photoshoot? Yep, I had a few that I did before I went home for the summer engagement shoots or weddings are goooood money but i hear in utah its super competitive Yeah there are so many, every other person takes pictures Its easy to do it for friends and people you know but past that it is hard to get more business yeah true do you think if you wanted to pursue your photography you wuold have to move out of utah? Um maybe, People need pictures everywhere But I dont really want to do that, its fine for the side, but too hard to make a living on Remind me what you want to do career wise? I want to become a UX desiner if you werent Christian what religion would you be in if at all? Hm Maybe Buddhist What about you? I think its  a really peaceful practice and it would match my personality I probably would be buddhist culturally because IM korean but if i had a choice I wouldn't label myself as any religion egnostic? is that the term? I think that is a term but I dont know exactly what it means Thats really cool I need to go. Thanks for chatting! Hope you have a good day!\"\n",
    "    \"Kerry aikman program(s are rich.\"\n",
    "]\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "    proc_doc = check_spelling(doc, print_corrs=False)\n",
    "    print(proc_doc)\n",
    "    print()\n",
    "\n",
    "doc = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "proc_doc = check_spelling(doc, print_corrs=False)\n",
    "print(proc_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88ca096-cab1-49d5-9852-cc8f10b6bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk, wtf is what I say, brb.\n",
      "kk\n",
      "\n",
      "okay, cool, what the fuck is what I say, be right back.\n",
      "okay, cool\n",
      "\n",
      "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
      "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
      "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more tears in my eyess, sometears in my eyess only within two weeks of each other.\n",
      "In 2010, she married once more, this tears in my eyes in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
      "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
      "2010 marriage license application, according to court documents.\n",
      "Prosecutors said the marriages were part of an immigration scam.\n",
      "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
      "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
      "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 tears in my eyess, with nine of her marriages occurring between 1999 and 2002.\n",
      "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one tears in my eyes, she was married to eight men at once, prosecutors say.\n",
      "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
      "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
      "The case was referred to the Bronx District Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\n",
      "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
      "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
      "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n"
     ]
    }
   ],
   "source": [
    "# Function to return translated version of textspeak\n",
    "def trans_textspeak(doc):\n",
    "    \"\"\"\n",
    "    Purpose:      To convert slang and text-speak to standard English in the given text.\n",
    "    Parameters:\n",
    "        doc       The plain-text document in string format.\n",
    "    Returns:\n",
    "        r         Plain-text document with slang and text-speak converted to standard English.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return\n",
    "    r = doc\n",
    "\n",
    "    # Tokenise doc\n",
    "    tokens = nlp_tokens(doc)\n",
    "\n",
    "    # Iterate through tokens\n",
    "    for token in tokens:\n",
    "\n",
    "        # Get translation if it exists\n",
    "        if token.text in textspeak.keys():\n",
    "\n",
    "            # Replace textspeak\n",
    "            r = r.replace(token.text, textspeak[token.text])\n",
    "\n",
    "    # Return\n",
    "    return r\n",
    "\n",
    "# Text suite\n",
    "doc = \"\"\"kk, wtf is what I say, brb.\n",
    "kk\n",
    "\"\"\"\n",
    "print(doc)\n",
    "proc_doc = trans_textspeak(doc)\n",
    "print(proc_doc)\n",
    "\n",
    "doc = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "proc_doc = trans_textspeak(doc)\n",
    "print(proc_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77895e29-314f-4466-a83a-fa2c39d27c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want some pizza with mushrooms on it.\n",
      "['mushroom', 'pizza pie', 'pizza', 'mushroom-shaped cloud', 'mushroom cloud']\n"
     ]
    }
   ],
   "source": [
    "# Function to find synonyms for a list of strings\n",
    "def get_synonyms(doc):\n",
    "    \"\"\"\n",
    "    Purpose:      To add synonyms to a given document.\n",
    "    Parameters:\n",
    "        doc       A Spacy-tokenized document.\n",
    "    Returns:\n",
    "        syns      Spacy-tokenized document appended with synonyms.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return\n",
    "    syns = []\n",
    "\n",
    "    # Iterate through tokens in the doc\n",
    "    for token in doc:\n",
    "\n",
    "        # We're only interested in POS's for which there are synonyms (noun, verb, adj, adv)\n",
    "        t = token.text.lower()\n",
    "        #print('\\t', t)\n",
    "        if t not in stopwords and token.pos_ in ('NOUN', 'VERB', 'ADJ', 'ADV'):\n",
    "\n",
    "            # Get synsets for this word\n",
    "            for ss in wn.synsets(token.text, pos=posmap[token.pos_]):\n",
    "\n",
    "                # Iterate through lemmas for this synset\n",
    "                #print(ss)\n",
    "                for lemma in ss.lemmas():\n",
    "\n",
    "                    # Get all synonyms and synsets for this lemma; this is necessary because wn.synonyms() only takes a word and not a synset,\n",
    "                    # so it returns all parts of speech, and there's not way to tell it to only return the POS we're interested in.\n",
    "                    # So to work around this, all the synonyms are returned, but it is a list of lists, with each outer list\n",
    "                    # corresponding to a lemma, so the POS can be traced back.\n",
    "                    #print('\\t', lemma)\n",
    "                    ss_lemma = wn.synsets(lemma.name())\n",
    "                    all_syns = wn.synonyms(lemma.name())\n",
    "\n",
    "                    # Iterate over all lemmas in this synset\n",
    "                    for i, lemma2 in enumerate(ss_lemma):\n",
    "\n",
    "                        # Add the synonym if it is the POS we want\n",
    "                        if lemma2.pos() == posmap[token.pos_]:\n",
    "                            #print('\\t\\tadding', i, all_syns[i])\n",
    "                            syns.extend(all_syns[i])\n",
    "\n",
    "    # Remove duplicates, replace underscores, check for stopwords\n",
    "    syns = list(set(syns))\n",
    "    syns = [x.replace('_', ' ') for x in syns]\n",
    "    [syns.remove(x) for x in syns if x in stopwords]\n",
    "    \n",
    "    # Return\n",
    "    return syns\n",
    "\n",
    "# Test suite\n",
    "doc = nlp('I want some pizza with mushrooms on it.')\n",
    "print(doc)\n",
    "proc_doc = get_synonyms(doc)\n",
    "print(proc_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68c9850-6125-437f-9bbb-95b76e874934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want some pizza with mushrooms on it.\n",
      "['basidiomycete', 'veggie', 'basidiomycetous fungi', 'vegetable', 'cloud', 'veg', 'agaric', 'dish']\n"
     ]
    }
   ],
   "source": [
    "# Function to return hypernyms (the categories of a given word)\n",
    "def get_hypernyms(doc):\n",
    "    \"\"\"\n",
    "    Purpose:       To add hypernyms to a given document.\n",
    "    Parameters:\n",
    "        doc        A Spacy-tokenized document.\n",
    "    Returns:\n",
    "        hypernyms  Spacy-tokenized document appended with hypernyms.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return\n",
    "    hypernyms = []\n",
    "\n",
    "    # Iterate through tokens in the doc\n",
    "    for token in doc:\n",
    "\n",
    "        # Only use this word if it is not a stop word and if the POS is one of [NOUN, PROPN, VERB, ADJ, ADV]\n",
    "        t = token.text.lower()\n",
    "        if t not in stopwords and token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']:\n",
    "        \n",
    "            # Print hypernyms - what the categories of this word are\n",
    "            #print('\\t', t)\n",
    "            ss = wn.synsets(t, pos=posmap[token.pos_])\n",
    "\n",
    "            # Iterate through synsets\n",
    "            for s in ss:\n",
    "\n",
    "                # Iterate through hypernyms\n",
    "                #print('\\t\\t', s)\n",
    "                for hypernym in s.hypernyms():\n",
    "\n",
    "                    # Add to list\n",
    "                    #print('\\t\\t\\t', hypernym.lemma_names())\n",
    "                    hypernyms.extend(hypernym.lemma_names())\n",
    "\n",
    "    # Remove duplicates, replace underscores, check for stopwords\n",
    "    hypernyms = list(set(hypernyms))\n",
    "    hypernyms = [x.replace('_', ' ') for x in hypernyms]\n",
    "    [hypernyms.remove(x) for x in hypernyms if x in stopwords]\n",
    "\n",
    "    # Return\n",
    "    return hypernyms\n",
    "\n",
    "# Test suite\n",
    "doc = nlp('I want some pizza with mushrooms on it.')\n",
    "print(doc)\n",
    "proc_doc = get_hypernyms(doc)\n",
    "print(proc_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb821da3-b2cf-40cd-a1c6-b4ee51e9b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords using ml6team/keyphrase-extraction\n",
    "def extract_keyphrases(doc):\n",
    "    \"\"\"\n",
    "    Purpose:      To substitute keyphrases for a given document using ml6team/keyphrase-extraction LLM.\n",
    "    Parameters:\n",
    "        doc       A plain-text document in string format.\n",
    "    Returns:\n",
    "        r         The document represented as a list of keyphrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract keyphrases\n",
    "    r = extractor(doc)\n",
    "    \n",
    "    # Return\n",
    "    return list(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297b4f3c-c8d3-4a6a-92c8-943fc1ab6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform preprocessing on a spacy NLP document.\n",
    "def preprocess_doc(doc, do_remove_stopwords, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, \\\n",
    "                   do_remove_punct, do_extract_keyphrases, print_spell_corrs):\n",
    "    \"\"\"\n",
    "    Purpose:                     To preprocess a conversation using the specified preprocessing steps.\n",
    "    Parameters:\n",
    "        doc                      A plain-text document in string format.\n",
    "        do_remove_stopwords      Boolean indicating whether to remove stopword tokens.\n",
    "        do_check_spelling        Boolean indicating whether to correct spelling errors.\n",
    "        do_add_synonyms          Boolean indicating whether to append synonyms to the list of tokens returned.\n",
    "        do_add_trans_textspeak   Boolean indicating whether to convert slang and text-speak to standard English.\n",
    "        do_add_hypernyms         Boolean indicating whether to append hypernyms to the list of tokens returned.\n",
    "        do_remove_punct          Boolean indicating whether to remove punctuation tokens.\n",
    "        do_extract_keyphrases    Boolean indicating whether to return the document as a list of keyphrases.\n",
    "        print_spell_corrs        Boolean indicating whether to print spelling corrections to STDOUT for debugging.\n",
    "    Returns:\n",
    "        tokens                   The preprocessed document as a tokenised list of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Translate textspeak\n",
    "    if do_trans_textspeak:\n",
    "        doc = trans_textspeak(doc)\n",
    "\n",
    "    # Spell check\n",
    "    if do_check_spelling:\n",
    "        doc = check_spelling(doc, print_spell_corrs)\n",
    "\n",
    "    # Keyphrase extraction\n",
    "    if do_extract_keyphrases:\n",
    "        tokens = extract_keyphrases(doc)\n",
    "\n",
    "    # If extracting keyphrases, we don't want to do any of the other proprocessing, because it would break up n-gram phrases\n",
    "    else:\n",
    "\n",
    "        # Tokenise\n",
    "        tokens = nlp(doc)\n",
    "    \n",
    "        # Synonyms\n",
    "        syns = []\n",
    "        if do_add_synonyms:\n",
    "            syns = get_synonyms(tokens)\n",
    "    \n",
    "        # Hypernyms\n",
    "        hypernyms = []\n",
    "        if do_add_hypernyms:\n",
    "            hypernyms = get_hypernyms(tokens)\n",
    "    \n",
    "        # Retokenise, removing punctuation if specified; we're also only interested in nouns, proper nouns, and verbs\n",
    "        if do_remove_punct:\n",
    "            tokens = [token.lemma_ for token in tokens if (token.pos_=='NOUN' or token.pos_=='PROPN' or token.pos_=='VERB') and not token.is_punct]\n",
    "        else:\n",
    "            tokens = [token.lemma_ for token in tokens if (token.pos_=='NOUN' or token.pos_=='PROPN' or token.pos_=='VERB')]\n",
    "        \n",
    "        # Add synonyms and hypernyms, remove duplicates\n",
    "        tokens.extend(syns)\n",
    "        tokens.extend(hypernyms)\n",
    "        tokens = list(set(tokens))\n",
    "    \n",
    "        # Remove stopwords\n",
    "        if do_remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Return the processed doc\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e093e648-fe94-4587-bb11-35b08ae809f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Court', 'Joint Terrorism Task Force', 'criminal trespass', 'immigration scam', 'marriage license', 'permanent residence status']\n",
      "\n",
      "<class 'list'>\n",
      "['York', 'refer', 'Security', 'New', 'prosecute', 'schedule', 'spokeswoman', 'investigation', 'occur', 'offer', 'application', 'Christopher', 'declare', 'Georgia', 'Egypt', 'scam', 'filing', 'license', 'Friday', 'emergency', 'Liana', 'Detective', 'Jersey', 'Office', 'comment', 'deport', 'Supreme', 'police', 'include', 'Task', 'subway', 'Department', 'May', 'Immigration', 'attorney', 'statement', 'residence', 'Barrientos', 'Division', 'Attorney', 'exit', 'status', 'accord', 'District', 'Turkey', 'Mali', 'convict', 'court', 'service', 'Court', 'approve', 'Terrorism', 'CNN)When', 'hitch', 'Customs', 'marry', 'State', 'Wright', 'prosecutor', 'Enforcement', 'eyess', 'prison', 'plead', 'involve', 'Rajput', 'Joint', 'divorce', 'Westchester', 'year', 'husband', 'Rashid', 'Island', 'flag', 'week', 'arrest', 'eye', 'appearance', 'County', 'happen', 'Homeland', 'leave', 'case', 'Pakistan', 'Investigation', 'Bronx', 'state', 'man', 'Long', 'trespass', 'marriage', 'file', 'degree', 'document', 'Force', 'sometear', 'Markowski', 'immigration', 'tear', 'believe', 'charge', 'count', 'decline', 'instrument', 'theft', 'sneak', 'Annette', 'country', 'face']\n",
      "\n",
      "2571 ['Sketch App', 'adobe illustrator', 'engagement pictures', 'free lance job', 'paid photoshoot', 'vector graphic editor']\n",
      "\n",
      "2987 ['BYU', 'brazil', 'europe', 'international student', 'nigeria', 'soccer', 'team', 'volleyball']\n",
      "\n",
      "24 []\n",
      "\n",
      "128 ['chat bubble']\n",
      "\n",
      "15 []\n",
      "\n",
      "6896 ['linear algebra']\n",
      "\n",
      "183 ['president nelson']\n",
      "\n",
      "362 ['Haiti', 'ponzi multi level marketing schemes']\n",
      "\n",
      "44 []\n",
      "\n",
      "322 ['oreo desert pizza']\n",
      "\n",
      "115 ['Queen of England', 'card']\n",
      "\n",
      "1783 ['Greatest Showman', 'charlie bit', 'dogs', 'heart', 'human trafficking', 'pt Barnum']\n",
      "\n",
      "2701 ['homesickness', 'military service', 'text bubbles']\n",
      "\n",
      "27 []\n",
      "\n",
      "1866 ['VANS', 'Vans', 'chickpeas', 'falafel', 'seasoning', 'vans']\n",
      "\n",
      "19 []\n",
      "\n",
      "13 []\n",
      "\n",
      "23 ['chocolate chip cookies']\n",
      "\n",
      "10 []\n",
      "\n",
      "20 []\n",
      "\n",
      "15899 ['final presentation', 'mission call', 'writing 150']\n",
      "\n",
      "1679 ['Italian', 'Italy', 'Portuguese', 'Spanish', 'spanish', 'visual']\n",
      "\n",
      "2631 ['tallage']\n",
      "\n",
      "86 []\n",
      "\n",
      "9 []\n",
      "\n",
      "145 ['civilization']\n",
      "\n",
      "1505 ['Art history', 'Caravaggio', 'Coldplay', 'German Romanticism', 'German romanticism', 'Katie Paterson', 'Tate modern', 'art history', 'baroque artist']\n",
      "\n",
      "107 ['education system']\n",
      "\n",
      "25 []\n",
      "\n",
      "16 []\n",
      "\n",
      "47 ['Twilight Zone']\n",
      "\n",
      "1479 ['Facebook', 'Shazam', 'facebook', 'google calendar']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test suite\n",
    "stmp = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "\n",
    "# With keyphrase extraction\n",
    "tmp = preprocess_doc(stmp, True, True, False, True, False, True, True, False)\n",
    "print(type(tmp))\n",
    "print(tmp)\n",
    "print()\n",
    "\n",
    "# Without keyphrase extraction\n",
    "tmp = preprocess_doc(stmp, True, True, False, True, False, True, False, False)\n",
    "print(type(tmp))\n",
    "print(tmp)\n",
    "print()\n",
    "\n",
    "# Chitchat docs with keyphrase extraction\n",
    "for i in range(0, 32):\n",
    "    print(len(cc_txt[i]), preprocess_doc(cc_txt[i], True, True, False, True, False, True, True, False))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68fa7ee9-1ecc-438c-b7c2-5783a76e5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform preprocessing on all docs in a specified dataset\n",
    "def preprocess_docs(dataset, num_docs, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, \\\n",
    "                   do_extract_keyphrases, print_spell_corrs, do_remove_stopwords, do_remove_punct, rnd_seed):\n",
    "    \"\"\"\n",
    "    Purpose:                     To preprocess a set of conversations using the specified preprocessing steps.\n",
    "    Parameters:\n",
    "        dataset                  The dataset containing the list of conversatins to preprocess.\n",
    "        num_docs                 The number of conversations in the dataset to preprocess.\n",
    "        do_check_spelling        Boolean indicating whether to correct spelling errors.\n",
    "        do_add_synonyms          Boolean indicating whether to append synonyms to the list of tokens returned.\n",
    "        do_add_trans_textspeak   Boolean indicating whether to convert slang and text-speak to standard English.\n",
    "        do_add_hypernyms         Boolean indicating whether to append hypernyms to the list of tokens returned.\n",
    "        do_extract_keyphrases    Boolean indicating whether to return the document as a list of keyphrases.\n",
    "        print_spell_corrs        Boolean indicating whether to print spelling corrections to STDOUT for debugging.\n",
    "        do_remove_stopwords      Boolean indicating whether to remove stopword tokens.\n",
    "        do_remove_punct          Boolean indicating whether to remove punctuation tokens.\n",
    "        rnd_seed                 The seed value to use when randomly selecting the num_docs from the dataset.\n",
    "    Returns:\n",
    "        proc_docs                The list of preprocessed documents lists of tokenised strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reload the spell deny list\n",
    "    if do_check_spelling:\n",
    "        load_spell_deny()\n",
    "    \n",
    "    # Get handle to dataset\n",
    "    docs_txt = []\n",
    "    print(f'loading {dataset} dataset')\n",
    "    if dataset == 'chitchat':\n",
    "        docs_txt = list(dfcc['txt'].values)\n",
    "    elif dataset == 'topical chat':\n",
    "        docs_txt = list(dftc['txt'].values)\n",
    "    elif dataset == 'ubuntu dialogue':\n",
    "        docs_txt = list(dfud['txt'].values)\n",
    "    elif dataset == 'enron email':\n",
    "        docs_txt = list(dfee['txt'].values)\n",
    "    else:\n",
    "        print(f'\\tunhandled dataset: {dataset}')\n",
    "        return\n",
    "\n",
    "    # Randomly sample num_docs documents\n",
    "    random.seed(rnd_seed)\n",
    "    docs_txt = random.sample(docs_txt, num_docs)\n",
    "\n",
    "    # Init list of processed docs\n",
    "    proc_docs = []\n",
    "    \n",
    "    # Iterate through raw docs to preprocess them\n",
    "    ct = 0\n",
    "    print('preprocessing documents')\n",
    "    for doc_txt in docs_txt:\n",
    "    \n",
    "        # Progress\n",
    "        ct += 1\n",
    "        if ct % 50 == 0: print('\\t', ct)\n",
    "\n",
    "        # Preprocess the doc\n",
    "        proc_doc = preprocess_doc(doc_txt, do_remove_stopwords, do_check_spelling, do_add_synonyms, \n",
    "                                  do_trans_textspeak, do_add_hypernyms, do_remove_punct, do_extract_keyphrases, print_spell_corrs=print_spell_corrs)\n",
    "        \n",
    "        # Append the processed doc to the list\n",
    "        proc_docs.append(proc_doc)\n",
    "\n",
    "    # Return\n",
    "    return proc_docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c68a7e-baf8-45bf-845a-396b7b306e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate coherence scores from model using Gensim\n",
    "def get_coherence_scores(which_coh_scores, the_model, proc_docs, corpus, dict, model_type):\n",
    "    \"\"\"\n",
    "    Purpose:                     To calculate coherence scores of topic representations against a given corpus of documents.\n",
    "    Parameters:\n",
    "        which_coh_scores         List of coherence metrics to calculate (one or more of ['c_v', 'c_uci', 'c_npmi', 'u_mass']).\n",
    "        the_model                The model object or list of topics; if the model_type is 'bow', \n",
    "                                     the_model should hold the model object, otherwise the list of topics for 'word2vec' and 'transformer' model types.\n",
    "        proc_docs                The list of processed documents.\n",
    "        corpus                   The Gensim corpus of all documents.\n",
    "        dict                     The Gensim corpus dictionary object (list of all tokens in the corpus).\n",
    "        model_type               The type of model ('bow' or 'word2vec').\n",
    "    Returns:\n",
    "        coh_scores               Dictionary keyed on each coherence score specified.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('calculating coherence scores')\n",
    "    coh_scores = {}\n",
    "\n",
    "    # Iterate through coherence scores\n",
    "    for coh_score in which_coh_scores:\n",
    "        print(f'\\t{coh_score} coherence score: ', end='')\n",
    "        try:\n",
    "            if coh_score == 'u_mass':  # U_Mass takes a corpus parameter\n",
    "                if model_type == 'bow':\n",
    "                    coh_model = CoherenceModel(model=the_model, corpus=corpus, dictionary=dict, coherence=coh_score)\n",
    "                else:\n",
    "                    coh_model = CoherenceModel(topics=the_model, corpus=corpus, dictionary=dict, coherence=coh_score)\n",
    "            else:  # Other coherence scores take a texts parameter\n",
    "                if model_type == 'bow':\n",
    "                    coh_model = CoherenceModel(model=the_model, texts=proc_docs, dictionary=dict, coherence=coh_score)\n",
    "                else:\n",
    "                    coh_model = CoherenceModel(topics=the_model, texts=proc_docs, dictionary=dict, coherence=coh_score)\n",
    "            coh_scores[coh_score] = coh_model.get_coherence()\n",
    "        except Exception as ex:\n",
    "            print(f'\\texception getting {coh_score} coherence score: {ex}')\n",
    "            coh_scores[coh_score] = np.nan\n",
    "        print(coh_scores[coh_score])\n",
    "    print()\n",
    "    \n",
    "    # Return\n",
    "    return coh_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89720511-92c1-48af-bd2e-877c2c4674e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the docs and runs the specified model.\n",
    "def run_model(proc_docs, the_model, rnd_seed, num_docs, model_params, which_coh_scores, print_docs_by_topic, timestamp, \n",
    "             dataset, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, do_extract_keyphrases, \n",
    "             print_spell_corrs, do_remove_stopwords, do_remove_punct):\n",
    "    \"\"\"\n",
    "    Purpose:                     To run the specified model against the specified documents using the specified preprocessing steps.\n",
    "    Parameters:\n",
    "        proc_docs                The set of preprocessed documents as lists of strings.\n",
    "        the_model                Which model to run.\n",
    "        rnd_seed                 The seed value to use when randomly selecting the num_docs from the dataset.\n",
    "        num_docs                 The number of conversations in the dataset to model.\n",
    "        model_params             Model-specific parameters to use.\n",
    "        which_coh_scores         List of coherence metrics to calculate (one or more of ['c_v', 'c_uci', 'c_npmi', 'u_mass']).\n",
    "        print_docs_by_topic      Boolean indicating whether to print the list of documents per topic.\n",
    "        timestamp                The starting timestamp (used to calculate runtime of the model).\n",
    "        dataset                  Which dataset the documents are from.\n",
    "        do_check_spelling        Boolean indicating whether to correct spelling errors.\n",
    "        do_add_synonyms          Boolean indicating whether to append synonyms to the list of tokens returned.\n",
    "        do_add_trans_textspeak   Boolean indicating whether to convert slang and text-speak to standard English.\n",
    "        do_add_hypernyms         Boolean indicating whether to append hypernyms to the list of tokens returned.\n",
    "        do_extract_keyphrases    Boolean indicating whether to return the document as a list of keyphrases.\n",
    "        print_spell_corrs        Boolean indicating whether to print spelling corrections to STDOUT for debugging.\n",
    "        do_remove_stopwords      Boolean indicating whether to remove stopword tokens.\n",
    "        do_remove_punct          Boolean indicating whether to remove punctuation tokens.\n",
    "    Returns:\n",
    "        none                     The results are added to global dataframe 'dfr'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Init\n",
    "    print('running model')\n",
    "    starttime = time.time()\n",
    "    \n",
    "    # Specify coherence scores\n",
    "    which_coh_scores = ['c_v', 'c_uci', 'c_npmi', 'u_mass']\n",
    "\n",
    "    # Choose model\n",
    "    if the_model == 'lda':\n",
    "        \n",
    "        # LDA modeling\n",
    "        ret_model, doc_topics, coh_scores, topic_words = \\\n",
    "            perform_bow_model('lda', proc_docs, topn_words=topn_topic_words, \n",
    "                        which_coh_scores=which_coh_scores, do_print_topics=False, model_params=model_params)\n",
    "\n",
    "    elif the_model == 'lsi':\n",
    "        \n",
    "        # LSI modeling\n",
    "        ret_model, doc_topics, coh_scores, topic_words = \\\n",
    "            perform_bow_model('lsi', proc_docs, topn_words=topn_topic_words, \n",
    "                        which_coh_scores=which_coh_scores, do_print_topics=False, model_params=model_params)\n",
    "        \n",
    "    elif the_model == 'nmf':\n",
    "        \n",
    "        # NMF modeling\n",
    "        ret_model, doc_topics, coh_scores, topic_words = \\\n",
    "            perform_bow_model('nmf', proc_docs, topn_words=topn_topic_words, \n",
    "                        which_coh_scores=which_coh_scores, do_print_topics=False, model_params=model_params)\n",
    "\n",
    "    elif the_model == 'word2vec':\n",
    "\n",
    "        # Word2Vec modeling\n",
    "        ret_model, doc_topics, coh_scores, topic_words = \\\n",
    "            perform_word2vec_model('word2vec', proc_docs, topn_words=topn_topic_words, \n",
    "                        which_coh_scores=which_coh_scores, do_print_topics=False, model_params=model_params)\n",
    "\n",
    "    elif the_model == 'bertopic':\n",
    "\n",
    "        # BERTopic modeling\n",
    "        ret_model, doc_topics, coh_scores, topic_words = \\\n",
    "            perform_transformer_model('bertopic', proc_docs, topn_words=topn_topic_words, \n",
    "                        which_coh_scores=which_coh_scores, do_print_topics=False, model_params=model_params)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Unhandled model\n",
    "        print(f'\\tunhandled model: {the_model}')\n",
    "        return\n",
    "    \n",
    "    # Print docs by topic\n",
    "    if print_docs_by_topic:\n",
    "        ct = 0\n",
    "        for topic in doc_topics.keys():\n",
    "            ct += 1\n",
    "            print('-------------------------------------------------------------')\n",
    "            print(f\"Topic {topic}: {ret_model.show_topic(topic, topn=topn_topic_words)}\\n\")\n",
    "            for i in doc_topics[topic]:\n",
    "                if print_docs_by_topic:\n",
    "                    print('doc #:', i)\n",
    "                    print(docs_txt[i][0:255])\n",
    "                    print()\n",
    "            print()\n",
    "\n",
    "    # Add to result df\n",
    "    dfr.loc[len(dfr.index)] = {\n",
    "        'Dataset': dataset,\n",
    "        'Num_docs': num_docs,\n",
    "        'Rnd_seed': rnd_seed,\n",
    "        'Model': the_model,\n",
    "        'Num_topics': len(doc_topics),\n",
    "        'Model_params': model_params,\n",
    "        'Cv_score': coh_scores['c_v'],\n",
    "        'Cuci_score': coh_scores['c_uci'],\n",
    "        'Cnpmi_score': coh_scores['c_npmi'],\n",
    "        'Umass_score': coh_scores['u_mass'],\n",
    "        'Spell_checked': do_check_spelling,\n",
    "        'Text_speak': do_trans_textspeak,\n",
    "        'Synonyms': do_add_synonyms,\n",
    "        'Hypernyms': do_add_hypernyms,\n",
    "        'Keyphrases': do_extract_keyphrases,\n",
    "        'Runtime': time.time() - starttime,\n",
    "        'Timestamp': timestamp,\n",
    "        'Topic_words': topic_words,\n",
    "        'Doc_topics': doc_topics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9e02a6b-46dd-489d-b7cf-369b50c208c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to init result dataframe\n",
    "def init_result_df():\n",
    "    \"\"\"\n",
    "    Purpose:                     To create and initialise a dataframe to store the results of modeling runs.\n",
    "    Parameters:                  None.\n",
    "    Returns:                     None (the dataframe 'dfr' is global).\n",
    "    \"\"\"\n",
    "\n",
    "    global dfr\n",
    "    dfr = None\n",
    "    dfr = pd.DataFrame(columns=[\n",
    "        'Dataset',\n",
    "        'Num_docs',\n",
    "        'Rnd_seed',\n",
    "        'Model',\n",
    "        'Num_topics',\n",
    "        'Model_params',\n",
    "        'Cv_score',\n",
    "        'Cuci_score',\n",
    "        'Cnpmi_score',\n",
    "        'Umass_score',\n",
    "        'Spell_checked',\n",
    "        'Text_speak',\n",
    "        'Synonyms',\n",
    "        'Hypernyms',\n",
    "        'Keyphrases',\n",
    "        'Runtime',\n",
    "        'Timestamp',\n",
    "        'Topic_words',\n",
    "        'Doc_topics',\n",
    "        'Flan_topic',\n",
    "        'Cosine_similarity',\n",
    "        'Topic_words2',\n",
    "        'Flan_topic2'\n",
    "    ])\n",
    "\n",
    "# Initialise the results dataframe\n",
    "init_result_df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fd82c-a353-49bf-a0d2-f9c602c11ec7",
   "metadata": {},
   "source": [
    "### Bag of words modeling\n",
    "\n",
    "This section performs topic modeling using the bag-of-words family of models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c716270c-f93f-4a77-9642-cbced7bb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section performs BoW topic modeling using SpaCy and Gensim; it uses Gensim to create a dictionary and a corpus,\n",
    "# the dictionary being simply the full list of tokens, the corpus being a list of tuples: \n",
    "# (word index in the dictionary, number of instances found in the corpus)\n",
    "\n",
    "# Function to perform NMF modeling\n",
    "def perform_bow_model(the_model, proc_docs, topn_words, which_coh_scores, do_print_topics, model_params):\n",
    "    \"\"\"\n",
    "    Purpose:                     To perform topic modeling using the specified bag-of-words model and calculating coherence scores.\n",
    "    Parameters:\n",
    "        the_model                Which model to run.\n",
    "        proc_docs                The set of preprocessed documents as a list of strings.\n",
    "        topn_words               The top N words representing the topic for this set of documents.\n",
    "        which_coh_scores         List of coherence scores to calculate (one or more of ['c_v', 'c_uci', 'c_npmi', 'u_mass'])\n",
    "        do_print_topics          Boolean indicating whether to print the list of topics to STDOUT.\n",
    "        model_params             Dictionary of model-specific parameters to run.\n",
    "    Returns:\n",
    "        res_model                The resulting model object.\n",
    "        doc_topics               The document-to-topic mapping keyed on topic number.\n",
    "        coh_scores               The list of coherence scores calculated for this model run.\n",
    "        all_topics               Dictionary of topic representations keyed on topic number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init dict to hold documents keyed on topic\n",
    "    doc_topics = {}\n",
    "    \n",
    "    # Create a dictionary using Gensim; this is just a simple list of words that appear in the entire corpus\n",
    "    dict = corpora.Dictionary(proc_docs)\n",
    "    print(f'size of dictionary: {len(dict)}')\n",
    "    \n",
    "    # Create a BoW corpus using Gensim; this is a list of tuples: \n",
    "    # (index of the word in the dictionary, number of instances of that word in the entire corpus)\n",
    "    corpus = [dict.doc2bow(doc) for doc in proc_docs]\n",
    "    print(f'size of BoW corpus: {len(corpus)}')\n",
    "\n",
    "    # Build the model using Gensim\n",
    "    print(f'running {the_model} model')\n",
    "    if the_model == 'lda':\n",
    "        res_model = LdaModel(corpus, num_topics=model_params['num_topics'], id2word=dict, passes=model_params['num_passes'])\n",
    "    elif the_model == 'lsi':\n",
    "        res_model = LsiModel(corpus=corpus, num_topics=model_params['num_topics'], id2word=dict)\n",
    "    elif the_model == 'nmf':\n",
    "        res_model = Nmf(corpus=corpus, num_topics=model_params['num_topics'], id2word=dict, passes=model_params['num_passes'])\n",
    "\n",
    "    # Get the topn words for each topic; this will return the topic word and the topic vector for each topic\n",
    "    all_topics = res_model.show_topics(num_topics=model_params['num_topics'], num_words=topn_words, formatted=False)\n",
    "\n",
    "    # Print the topics\n",
    "    if do_print_topics:\n",
    "        print('all topics:')\n",
    "        [print(f\"\\t{this_topic}\\n\") for this_topic in all_topics]\n",
    "        print()\n",
    "\n",
    "    # This part isn't needed anymore; we'll just return the full topic list containing the tuples (topic word, word probability)\n",
    "    if 1 == 0:\n",
    "        \n",
    "        # This part creates a list so we can find the most likely words associated with each topic.\n",
    "        # First, initialise a list of size num_topics; each element in the list will hold another list: the list of topic words for that topic\n",
    "        topic_words = [[]] * model_params['num_topics']\n",
    "    \n",
    "        # Iterate over the topics to get the key words in each topic\n",
    "        for i, the_topic in all_topics:\n",
    "            words_cur_topic = [topic_word for topic_word, topic_vector in the_topic]\n",
    "            topic_words[i] = words_cur_topic\n",
    "\n",
    "    # Iterate over each document in the corpus, showing the most likely topic for each one\n",
    "    i = 0  # doc counter\n",
    "    for doc in corpus:\n",
    "\n",
    "        # Get topics for this document; lda is different than lsi and nmf models\n",
    "        if the_model == 'lda':\n",
    "            \n",
    "            # Get topics for this document by comparing words that are most frequent in this document \n",
    "            # to those that are most frequent in the words for each topic\n",
    "            cur_topics_unsorted = res_model.get_document_topics(doc)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Get the vector representation of the current document (i.e., the line in the document matrix)\n",
    "            cur_topics_unsorted = res_model[doc]\n",
    "\n",
    "        # The topic with the highest absolute value will the the most likely topic;\n",
    "        # in the case of lda, abs() isn't needed, but it don't hurt nuthin'\n",
    "        cur_topics_sorted = sorted(cur_topics_unsorted, key=lambda x: abs(x[1]), reverse=True)\n",
    "        #print('sorted topics for the current doc:', cur_topics_sorted)\n",
    "\n",
    "        # Make sure there is at least one topic for this document\n",
    "        if len(cur_topics_sorted) > 0:\n",
    "            \n",
    "            # The sorted list will be a list of tuples, like this: [(0, 1.49962121734348), (1, -4.494637970805636)]\n",
    "            # The first tuple will be the most likely topic (list element #0).\n",
    "            # The first element in the tuple is the topic number (tuple element #0).\n",
    "            topic_num = cur_topics_sorted[0][0]\n",
    "            #print(f\"most likely: topic #{topic_num} ({topic_words[topic_num]})\")\n",
    "            #print()\n",
    "    \n",
    "            # Append this doc to the corresponding entry in the doc_topics dict\n",
    "            if topic_num not in doc_topics.keys(): doc_topics[topic_num] = []  # Init the topic number in the dict\n",
    "            doc_topics[topic_num].append(i)\n",
    "\n",
    "        # Increment counter\n",
    "        i += 1\n",
    "\n",
    "    # Coherence scores\n",
    "    coh_scores = get_coherence_scores(which_coh_scores, res_model, proc_docs, corpus, dict, 'bow')\n",
    "\n",
    "    # Return\n",
    "    return res_model, doc_topics, coh_scores, all_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2affcd9-896d-4ce9-a3d5-2c9c0c748d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# This performs bow modeling\n",
    "##########################################\n",
    "\n",
    "# Init resultset\n",
    "init_result_df()\n",
    "\n",
    "# Define model runs\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {'model_name': 'lsi', 'model_params': {'num_topics': 15}},\n",
    "        {'model_name': 'lsi', 'model_params': {'num_topics': 30}},\n",
    "        {'model_name': 'nmf', 'model_params': {'num_topics': 15, 'num_passes': 15}},\n",
    "        {'model_name': 'nmf', 'model_params': {'num_topics': 30, 'num_passes': 15}},\n",
    "        {'model_name': 'lda', 'model_params': {'num_topics': 15, 'num_passes': 25}},\n",
    "        {'model_name': 'lda', 'model_params': {'num_topics': 30, 'num_passes': 25}}\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True],\n",
    "    'do_trans_textspeak': [True],\n",
    "    'do_add_synonyms': [False, True],\n",
    "    'do_add_hypernyms': [False, True],\n",
    "    'do_extract_keyphrases': [False]\n",
    "}\n",
    "\n",
    "# Define model runs - keyphrase extraction\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {'model_name': 'lsi', 'model_params': {'num_topics': 15}},\n",
    "        {'model_name': 'lsi', 'model_params': {'num_topics': 30}},\n",
    "        {'model_name': 'nmf', 'model_params': {'num_topics': 15, 'num_passes': 15}},\n",
    "        {'model_name': 'nmf', 'model_params': {'num_topics': 30, 'num_passes': 15}},\n",
    "        {'model_name': 'lda', 'model_params': {'num_topics': 15, 'num_passes': 25}},\n",
    "        {'model_name': 'lda', 'model_params': {'num_topics': 30, 'num_passes': 25}}\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True],\n",
    "    'do_trans_textspeak': [True],\n",
    "    'do_add_synonyms': [False],\n",
    "    'do_add_hypernyms': [False],\n",
    "    'do_extract_keyphrases': [True]\n",
    "}\n",
    "\n",
    "# Run model\n",
    "for dataset in model_runs['datasets']:    \n",
    "    for num_docs in model_runs['num_docs']:\n",
    "        for do_check_spelling in model_runs['do_check_spelling']:\n",
    "            for do_trans_textspeak in model_runs['do_trans_textspeak']:\n",
    "                for do_add_synonyms in model_runs['do_add_synonyms']:\n",
    "                    for do_add_hypernyms in model_runs['do_add_hypernyms']:\n",
    "                        for do_extract_keyphrases in model_runs['do_extract_keyphrases']:\n",
    "\n",
    "                            # Preprocess\n",
    "                            proc_docs = preprocess_docs(dataset, num_docs, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, \\\n",
    "                                           do_extract_keyphrases, print_spell_corrs=False, do_remove_stopwords=True, do_remove_punct=True, rnd_seed=77)\n",
    "                            \n",
    "                            # Model runs\n",
    "                            for the_model in model_runs['models']:\n",
    "        \n",
    "                                # Get model name and parameters\n",
    "                                model_name = the_model['model_name']\n",
    "                                model_params = the_model['model_params']\n",
    "                                \n",
    "                                # Preprocess and run the model\n",
    "                                timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "                                run_model(proc_docs=proc_docs, the_model=model_name, rnd_seed=77, num_docs=num_docs, \n",
    "                                    model_params=model_params, \n",
    "                                    which_coh_scores=['c_v', 'c_uci', 'c_npmi', 'u_mass'],\n",
    "                                    print_docs_by_topic=False, timestamp=timestamp,\n",
    "                                    dataset=dataset, do_check_spelling=do_check_spelling, do_add_synonyms=do_add_synonyms, do_trans_textspeak=do_trans_textspeak, \\\n",
    "                                    do_add_hypernyms=do_add_hypernyms, do_extract_keyphrases=do_extract_keyphrases, print_spell_corrs=False, \\\n",
    "                                    do_remove_stopwords=True, do_remove_punct=True)\n",
    "\n",
    "                                # Show/store the results\n",
    "                                display(dfr.tail(1))\n",
    "                                dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\bow\\\\dfr_' + timestamp + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b93cc-3e6e-43cf-823f-5d1f61f7a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "pd.set_option('display.max_rows', 16)\n",
    "#display(dfr.sort_values(by=['Umass_score'], ascending=False))\n",
    "display(dfr.sort_values(by=['Dataset'], ascending=True))\n",
    "dfr.plot(x='Num_topics', y='Umass_score', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7674cb-b926-40e3-963e-9e52fdfef807",
   "metadata": {},
   "source": [
    "### Embeddings-based modeling\n",
    "\n",
    "This section performs topic modeling using embeddings-based models. It also includes helper functions get_doc_vectors() and get_topn_words().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e32436-9073-4625-a0a3-9786a9523554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return document vectors for a list of processed docs, given a word2vec model\n",
    "def get_doc_vectors(proc_docs, model, vec_size):\n",
    "    \"\"\"\n",
    "    Purpose:                     To return text embedding vectors from a list of documents.\n",
    "    Parameters:\n",
    "        proc_docs                The set of preprocessed documents as a list of strings.\n",
    "        model                    The embeddings model object.\n",
    "        vec_size                 The number of vectors (features) to return.\n",
    "    Returns:\n",
    "        dv                       The list of document vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calc document vectors as the average of each list of word vectors\n",
    "    dv = []\n",
    "    \n",
    "    # Iterate through processed documents (each document will be a list of tokens)\n",
    "    for tokens in proc_docs:\n",
    "    \n",
    "        # Init vector array for this document\n",
    "        vectors = []\n",
    "    \n",
    "        # Iterate through tokens in this document\n",
    "        for token in tokens:\n",
    "\n",
    "            # Append the vectors for this word (token) to the vector array for this document (if the word exists in the model)\n",
    "            if token in model:\n",
    "                vectors.append(model[token])\n",
    "\n",
    "        # Calculate the mean of the vectors for each column in the array;\n",
    "        # i.e., for each feature in the array, calculate the value of the feature over all words for that feature\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            dv.append(vectors.mean(axis=0))\n",
    "        else:\n",
    "            dv.append(np.zeros(vec_size))\n",
    "\n",
    "    # Return\n",
    "    return dv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd3e971e-f026-44e1-9a93-ba08818b1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate list of topn words with the highest tfidf for each document;\n",
    "# the returned list of words will be in reverse tfidf order (highest tfidf first).\n",
    "# Recall that topic_list will be a list of tuples, like this:\n",
    "# (cluster_number, [list of all tokens for all documents in this cluster])\n",
    "def get_topn_words(topic_list, num_words):\n",
    "    \"\"\"\n",
    "    Purpose:                     To get the top N words with the highest TF-IDF in each document.\n",
    "    Parameters:\n",
    "        topic_list               The complete list of topic representations.\n",
    "        num_words                The number of topic words to return.\n",
    "    Returns:\n",
    "        topn_words               The top N most important topic representations in list format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return list\n",
    "    topn_words = []\n",
    "\n",
    "    # Build the dictionary, which will be the list of tokens from all topics\n",
    "    proc_docs = []\n",
    "    for topic in topic_list:\n",
    "        proc_docs.append(topic[1])\n",
    "    dct = Dictionary(proc_docs)\n",
    "\n",
    "    # Create corpus from BoW\n",
    "    corpus = [dct.doc2bow(line) for line in proc_docs]\n",
    "    \n",
    "    # Fit the model\n",
    "    tfidf_model = TfidfModel(corpus)  # fit model\n",
    "    \n",
    "    # Iterate through each topic\n",
    "    for topic in topic_list:\n",
    "\n",
    "        # Get the topic number and tokens for this topic:\n",
    "        topic_num = topic[0]\n",
    "        \n",
    "        # Get the tfidf vectors for this document\n",
    "        vectors = tfidf_model[corpus[topic_num]]\n",
    "    \n",
    "        # Sort the vectors by tfidf; each list of vectors will be a set of tuples, like this: [(0, 0.017408288781402326), etc.]\n",
    "        # where the first number in the tuple is the index of the word in the dictionary (we'll need to look this up in the dictionary)\n",
    "        # and the second number in the tuple is the tfidf value; each tuple corresponds to a token in the processed doc list (proc_docs).\n",
    "        vectors_sorted = sorted(vectors, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # Append the first [nwords] words to the list\n",
    "        topn_words.append((topic_num, [(dct[word[0]], word[1]) for word in vectors_sorted[:num_words]]))\n",
    "\n",
    "    # Return\n",
    "    return topn_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ca5f94-2b7a-4d41-94f0-dca4ecb920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform word2vec modeling\n",
    "def perform_word2vec_model(the_model, proc_docs, topn_words, which_coh_scores, do_print_topics, model_params):\n",
    "    \"\"\"\n",
    "    Purpose:                     To perform topic modeling using the specified embedding model and calculating coherence scores.\n",
    "    Parameters:\n",
    "        the_model                Which model to run.\n",
    "        proc_docs                The set of preprocessed documents as a list of strings.\n",
    "        topn_words               The top N words representing the topic for this set of documents.\n",
    "        which_coh_scores         List of coherence scores to calculate (one or more of ['c_v', 'c_uci', 'c_npmi', 'u_mass'])\n",
    "        do_print_topics          Boolean indicating whether to print the list of topics to STDOUT.\n",
    "        model_params             Dictionary of model-specific parameters to run.\n",
    "    Returns:\n",
    "        res_model                The resulting model object.\n",
    "        doc_topics               The document-to-topic mapping keyed on topic number.\n",
    "        coh_scores               The list of coherence scores calculated for this model run.\n",
    "        all_topics               Dictionary of topic representations keyed on topic number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init dict to hold documents keyed on topic\n",
    "    doc_topics = {}\n",
    "\n",
    "    # Create embeddings if needed; the length of the model will be the number of words in the vocabulary;\n",
    "    # the width will be the number of embeddings (features or \"vectors\") specified by vector_size\n",
    "    if model_params['embeddings'] == 'word2vec':\n",
    "        w2v_model = ptmodel_word2vec\n",
    "        wv = ptmodel_word2vec\n",
    "    elif model_params['embeddings'] == 'glove':\n",
    "        w2v_model = ptmodel_glove\n",
    "        wv = ptmodel_glove\n",
    "    elif model_params['embeddings'] == 'fasttext':\n",
    "        w2v_model = ptmodel_fasttext\n",
    "        wv = ptmodel_fasttext.wv\n",
    "    elif model_params['embeddings'] == 'custom':\n",
    "        w2v_model = Word2Vec(sentences=proc_docs, vector_size=model_params['vector_size'], \n",
    "                             window=5, min_count=model_params['min_count'], workers=4, seed=77)\n",
    "        wv = w2v_model.wv  # for a custom model, the word vectors are stored in .wv (unlike the pretrained model)\n",
    "    else:\n",
    "        print(f\"\\tunhandled embeddings type: {model_params['embeddings']}\")\n",
    "        return\n",
    "    print(f'\\tmodel with {len(wv)} tokens and {w2v_model.vector_size} vectors')\n",
    "    if debug:\n",
    "        print('first embedding:', wv.index_to_key[0], wv[0])\n",
    "        print('last embedding:', wv.index_to_key[len(wv) - 1], wv[len(wv) - 1])\n",
    "\n",
    "    # Get document vectors from the word vectors\n",
    "    dv = get_doc_vectors(proc_docs, wv, w2v_model.vector_size)\n",
    "\n",
    "    # Which clustering algorithm\n",
    "    if model_params['cluster_alg'] == 'kmeans':\n",
    "\n",
    "        # Run K-means clustering against the document vectors\n",
    "        print(f\"\\trunning k-means clustering\")\n",
    "        res_model = KMeans(n_clusters=model_params['n_clusters'], init='k-means++', n_init='auto', \n",
    "                           max_iter=model_params['max_iter'], tol=model_params['tol'], random_state=77)\n",
    "        y_cluster = res_model.fit_predict(dv)\n",
    "\n",
    "    elif model_params['cluster_alg'] == 'dbscan':\n",
    "\n",
    "        # Run dbscan clustering against the document vectors\n",
    "        res_model = DBSCAN(eps=model_params['eps'], min_samples=model_params['min_samples']).fit(dv)\n",
    "        y_cluster = res_model.labels_\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Unhandled\n",
    "        print(f\"\\tunhandled clustering algorithm: {model_params['cluster_alg']}\")\n",
    "        return\n",
    "\n",
    "    #print('y_cluster')\n",
    "    #print(y_cluster)\n",
    "\n",
    "    # Print cluster stats\n",
    "    print(f\"\\tnumber of clusters: {len(np.unique(y_cluster))}\")\n",
    "    print(f\"\\tunique clusters: {np.unique(y_cluster)}\")\n",
    "\n",
    "    # Iterate through clusters to get the docs in each cluster\n",
    "    print('\\tgetting docs in each cluster')\n",
    "    tokens_in_all_clusters = []\n",
    "    for cluster in np.unique(y_cluster):\n",
    "    \n",
    "        # Set the list of doc_topics keyed on this topic\n",
    "        doc_topics[cluster] = [i for i, y in enumerate(y_cluster) if y == cluster]\n",
    "    \n",
    "        # Get the documents in this cluster\n",
    "        doc_list = [proc_docs[i] for i, y in enumerate(y_cluster) if y == cluster]\n",
    "        #print(f'\\t\\tcluster {cluster}: {len(doc_list)} docs')\n",
    "    \n",
    "        # Concatenate all tokens in every document in this cluster\n",
    "        tokens_in_cluster = []\n",
    "        for doc in doc_list:\n",
    "            tokens_in_cluster.extend(doc)\n",
    "    \n",
    "        # Append this list of tokens to the overall list of cluster documents; will be a list of tuples:\n",
    "        # (cluster_number, [list of all tokens for all documents in this cluster])\n",
    "        tmp_tuple = (cluster, tokens_in_cluster)\n",
    "        tokens_in_all_clusters.append(tmp_tuple)\n",
    "\n",
    "    # Get the topn words in each cluster; all_topics will be a list of tuples, where each tuple will be another tuple:\n",
    "    # (topic_number0, [(word0, tfidf0), (word1, tfidf1), etc.])\n",
    "    all_topics = get_topn_words(tokens_in_all_clusters, topn_words)\n",
    "    #print(all_topics)\n",
    "    \n",
    "    # Create a dictionary and BoW corpus -- needed for the coherence score\n",
    "    dict = corpora.Dictionary(proc_docs)\n",
    "    corpus = [dict.doc2bow(doc) for doc in proc_docs]\n",
    "\n",
    "    # Get lists of topics without tfidf (recall that all_topics is a list of topics, where each topic is\n",
    "    # a tuple, e.g.: (0, [('learn', 0.0057238796234439435), ('thing', 0.005195910040936676) etc.])\n",
    "    all_topics_no_tfidf = []\n",
    "    for topic in all_topics:\n",
    "        all_topics_no_tfidf.append([word[0] for word in topic[1]])\n",
    "\n",
    "    # Coherence scores\n",
    "    coh_scores = get_coherence_scores(which_coh_scores, all_topics_no_tfidf, proc_docs, corpus, dict, 'word2vec')\n",
    "\n",
    "    # Return\n",
    "    return res_model, doc_topics, coh_scores, all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e18b8-8b52-4704-acbe-5dfc513456a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# This section performs embeddings-based topic modeling\n",
    "############################################################\n",
    "\n",
    "# Init resultset\n",
    "init_result_df()\n",
    "\n",
    "# Define model runs\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        }\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True],\n",
    "    'do_trans_textspeak': [True],\n",
    "    'do_add_synonyms': [False, True],\n",
    "    'do_add_hypernyms': [False, True],\n",
    "    'do_extract_keyphrases': [False]\n",
    "}\n",
    "\n",
    "# Define model runs - keyphrase extraction\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 15, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', \n",
    "                             'n_clusters': 30, 'max_iter': 300, 'tol': 1e-4}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 0.1, 'min_samples': 3}\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'word2vec', \n",
    "            'model_params': {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', \n",
    "                             'eps': 1, 'min_samples': 2}\n",
    "        }\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True],\n",
    "    'do_trans_textspeak': [True],\n",
    "    'do_add_synonyms': [False],\n",
    "    'do_add_hypernyms': [False],\n",
    "    'do_extract_keyphrases': [True]\n",
    "}\n",
    "\n",
    "# Run model\n",
    "for dataset in model_runs['datasets']:    \n",
    "    for num_docs in model_runs['num_docs']:\n",
    "        for do_check_spelling in model_runs['do_check_spelling']:\n",
    "            for do_trans_textspeak in model_runs['do_trans_textspeak']:\n",
    "                for do_add_synonyms in model_runs['do_add_synonyms']:\n",
    "                    for do_add_hypernyms in model_runs['do_add_hypernyms']:\n",
    "                        for do_extract_keyphrases in model_runs['do_extract_keyphrases']:\n",
    "\n",
    "                            # Preprocess\n",
    "                            proc_docs = preprocess_docs(dataset, num_docs, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, \\\n",
    "                                           do_extract_keyphrases, print_spell_corrs=False, do_remove_stopwords=True, do_remove_punct=True, rnd_seed=77)\n",
    "                            \n",
    "                            # Model runs\n",
    "                            for the_model in model_runs['models']:\n",
    "        \n",
    "                                # Get model name and parameters\n",
    "                                model_name = the_model['model_name']\n",
    "                                model_params = the_model['model_params']\n",
    "                                \n",
    "                                # Preprocess and run the model\n",
    "                                timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "                                run_model(proc_docs=proc_docs, the_model=model_name, rnd_seed=77, num_docs=num_docs, \n",
    "                                    model_params=model_params, \n",
    "                                    which_coh_scores=['c_v', 'c_uci', 'c_npmi', 'u_mass'],\n",
    "                                    print_docs_by_topic=False, timestamp=timestamp,\n",
    "                                    dataset=dataset, do_check_spelling=do_check_spelling, do_add_synonyms=do_add_synonyms, do_trans_textspeak=do_trans_textspeak, \\\n",
    "                                    do_add_hypernyms=do_add_hypernyms, do_extract_keyphrases=do_extract_keyphrases, print_spell_corrs=False, \\\n",
    "                                    do_remove_stopwords=True, do_remove_punct=True)\n",
    "\n",
    "                                # Show/store the results\n",
    "                                display(dfr.tail(1))\n",
    "                                dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\word2vec\\\\dfr_' + timestamp + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d8657c8-d5cf-4373-b183-215997ce5ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>Umass_score</th>\n",
       "      <th>Spell_checked</th>\n",
       "      <th>Text_speak</th>\n",
       "      <th>Synonyms</th>\n",
       "      <th>Hypernyms</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>12</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.352286</td>\n",
       "      <td>-5.703755</td>\n",
       "      <td>-0.125272</td>\n",
       "      <td>-2.239894</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>110.906155</td>\n",
       "      <td>20240324_003739</td>\n",
       "      <td>[[(Aurora, 0.16614009269096236), (aurora, 0.16...</td>\n",
       "      <td>{0: [29, 54, 112, 126], 1: [8, 23, 43, 47, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>12</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.421545</td>\n",
       "      <td>-4.431563</td>\n",
       "      <td>-0.092500</td>\n",
       "      <td>-2.435616</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>39.682985</td>\n",
       "      <td>20240324_005430</td>\n",
       "      <td>[[(Yoyoyo, 0.42563122338685), (hellllo, 0.4256...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>12</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.413367</td>\n",
       "      <td>-4.834086</td>\n",
       "      <td>-0.086393</td>\n",
       "      <td>-2.670544</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>75.969831</td>\n",
       "      <td>20240324_004400</td>\n",
       "      <td>[[(evaluate, 0.05634086138429385), (pass judgm...</td>\n",
       "      <td>{0: [0, 1, 5, 6, 7, 9, 10, 11, 12, 14, 17, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>12</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.390319</td>\n",
       "      <td>-5.289748</td>\n",
       "      <td>-0.116678</td>\n",
       "      <td>-2.896704</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>30.656932</td>\n",
       "      <td>20240324_010040</td>\n",
       "      <td>[[(evaluate, 0.05861838270469555), (pass judgm...</td>\n",
       "      <td>{0: [0, 1, 5, 6, 7, 9, 10, 11, 12, 14, 17, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>8</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.398597</td>\n",
       "      <td>-5.716121</td>\n",
       "      <td>-0.130474</td>\n",
       "      <td>-2.922690</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>89.531780</td>\n",
       "      <td>20240324_002504</td>\n",
       "      <td>[[(create, 0.05771883768457536), (change, 0.05...</td>\n",
       "      <td>{0: [10, 23, 25, 34, 37, 56, 57, 97, 108, 111,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>500</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>32</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.509880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>169.048307</td>\n",
       "      <td>20240324_051537</td>\n",
       "      <td>[[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>500</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>32</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.677352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>54.870675</td>\n",
       "      <td>20240324_052018</td>\n",
       "      <td>[[(Yoyoyo, 0.37796447300922725), (evenign, 0.3...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>500</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>32</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.508395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>84.988423</td>\n",
       "      <td>20240324_052203</td>\n",
       "      <td>[[(Yoyoyo, 0.37796447300922725), (evenign, 0.3...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>500</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>32</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.498596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>152.546034</td>\n",
       "      <td>20240324_052421</td>\n",
       "      <td>[[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>500</td>\n",
       "      <td>77</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>32</td>\n",
       "      <td>{'embeddings': 'pretrained', 'vector_size': 20...</td>\n",
       "      <td>0.509210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>165.340549</td>\n",
       "      <td>20240324_052840</td>\n",
       "      <td>[[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...</td>\n",
       "      <td>{0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset  Num_docs  Rnd_seed     Model  Num_topics  \\\n",
       "38   chitchat       200        77  word2vec          12   \n",
       "50   chitchat       200        77  word2vec          12   \n",
       "42   chitchat       200        77  word2vec          12   \n",
       "58   chitchat       200        77  word2vec          12   \n",
       "30   chitchat       200        77  word2vec           8   \n",
       "..        ...       ...       ...       ...         ...   \n",
       "246  chitchat       500        77  word2vec          32   \n",
       "248  chitchat       500        77  word2vec          32   \n",
       "250  chitchat       500        77  word2vec          32   \n",
       "252  chitchat       500        77  word2vec          32   \n",
       "254  chitchat       500        77  word2vec          32   \n",
       "\n",
       "                                          Model_params  Cv_score  Cuci_score  \\\n",
       "38   {'embeddings': 'pretrained', 'vector_size': 20...  0.352286   -5.703755   \n",
       "50   {'embeddings': 'pretrained', 'vector_size': 20...  0.421545   -4.431563   \n",
       "42   {'embeddings': 'pretrained', 'vector_size': 20...  0.413367   -4.834086   \n",
       "58   {'embeddings': 'pretrained', 'vector_size': 20...  0.390319   -5.289748   \n",
       "30   {'embeddings': 'pretrained', 'vector_size': 20...  0.398597   -5.716121   \n",
       "..                                                 ...       ...         ...   \n",
       "246  {'embeddings': 'pretrained', 'vector_size': 20...  0.509880         NaN   \n",
       "248  {'embeddings': 'pretrained', 'vector_size': 20...  0.677352         NaN   \n",
       "250  {'embeddings': 'pretrained', 'vector_size': 20...  0.508395         NaN   \n",
       "252  {'embeddings': 'pretrained', 'vector_size': 20...  0.498596         NaN   \n",
       "254  {'embeddings': 'pretrained', 'vector_size': 20...  0.509210         NaN   \n",
       "\n",
       "     Cnpmi_score  Umass_score  Spell_checked  Text_speak  Synonyms  Hypernyms  \\\n",
       "38     -0.125272    -2.239894           True        True      True       True   \n",
       "50     -0.092500    -2.435616          False        True     False       True   \n",
       "42     -0.086393    -2.670544           True       False     False       True   \n",
       "58     -0.116678    -2.896704          False       False     False       True   \n",
       "30     -0.130474    -2.922690          False       False      True       True   \n",
       "..           ...          ...            ...         ...       ...        ...   \n",
       "246          NaN          NaN          False        True      True       True   \n",
       "248          NaN          NaN          False       False     False      False   \n",
       "250          NaN          NaN          False       False     False       True   \n",
       "252          NaN          NaN          False       False      True      False   \n",
       "254          NaN          NaN          False       False      True       True   \n",
       "\n",
       "        Runtime        Timestamp  \\\n",
       "38   110.906155  20240324_003739   \n",
       "50    39.682985  20240324_005430   \n",
       "42    75.969831  20240324_004400   \n",
       "58    30.656932  20240324_010040   \n",
       "30    89.531780  20240324_002504   \n",
       "..          ...              ...   \n",
       "246  169.048307  20240324_051537   \n",
       "248   54.870675  20240324_052018   \n",
       "250   84.988423  20240324_052203   \n",
       "252  152.546034  20240324_052421   \n",
       "254  165.340549  20240324_052840   \n",
       "\n",
       "                                           Topic_words  \\\n",
       "38   [[(Aurora, 0.16614009269096236), (aurora, 0.16...   \n",
       "50   [[(Yoyoyo, 0.42563122338685), (hellllo, 0.4256...   \n",
       "42   [[(evaluate, 0.05634086138429385), (pass judgm...   \n",
       "58   [[(evaluate, 0.05861838270469555), (pass judgm...   \n",
       "30   [[(create, 0.05771883768457536), (change, 0.05...   \n",
       "..                                                 ...   \n",
       "246  [[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...   \n",
       "248  [[(Yoyoyo, 0.37796447300922725), (evenign, 0.3...   \n",
       "250  [[(Yoyoyo, 0.37796447300922725), (evenign, 0.3...   \n",
       "252  [[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...   \n",
       "254  [[(Yoyoyo, 0.41075875755320357), (hellllo, 0.4...   \n",
       "\n",
       "                                            Doc_topics  \n",
       "38   {0: [29, 54, 112, 126], 1: [8, 23, 43, 47, 64,...  \n",
       "50   {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "42   {0: [0, 1, 5, 6, 7, 9, 10, 11, 12, 14, 17, 20,...  \n",
       "58   {0: [0, 1, 5, 6, 7, 9, 10, 11, 12, 14, 17, 20,...  \n",
       "30   {0: [10, 23, 25, 34, 37, 56, 57, 97, 108, 111,...  \n",
       "..                                                 ...  \n",
       "246  {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "248  {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "250  {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "252  {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "254  {0: [2, 4, 15, 16, 18, 19, 24, 28, 33, 35, 40,...  \n",
       "\n",
       "[256 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Num_topics', ylabel='Umass_score'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results\n",
    "pd.set_option('display.max_rows', 16)\n",
    "display(dfr.sort_values(by=['Umass_score'], ascending=False))\n",
    "dfr.plot(x='Num_topics', y='Umass_score', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1e381-888f-40c4-a937-0cdc46508c02",
   "metadata": {},
   "source": [
    "### Transformer-based modeling\n",
    "\n",
    "This section performs topic modeling using transformer-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080efec0-8ad4-457a-97e8-7ad238722e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section performs transformer modeling with BERTopic\n",
    "\n",
    "# Function to perform word2vec modeling\n",
    "def perform_transformer_model(the_model, proc_docs, topn_words, which_coh_scores, do_print_topics, model_params):\n",
    "    \"\"\"\n",
    "    Purpose:                     To perform topic modeling using the specified transformer model and calculating coherence scores.\n",
    "    Parameters:\n",
    "        the_model                Which model to run.\n",
    "        proc_docs                The set of preprocessed documents as a list of strings.\n",
    "        topn_words               The top N words representing the topic for this set of documents.\n",
    "        which_coh_scores         List of coherence scores to calculate (one or more of ['c_v', 'c_uci', 'c_npmi', 'u_mass'])\n",
    "        do_print_topics          Boolean indicating whether to print the list of topics to STDOUT.\n",
    "        model_params             Dictionary of model-specific parameters to run.\n",
    "    Returns:\n",
    "        res_model                The resulting model object.\n",
    "        doc_topics               The document-to-topic mapping keyed on topic number.\n",
    "        coh_scores               The list of coherence scores calculated for this model run.\n",
    "        all_topics               Dictionary of topic representations keyed on topic number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init dict to hold documents keyed on topic\n",
    "    doc_topics = {}\n",
    "\n",
    "    # Convert tokenised/preprocessed docs back to string\n",
    "    proc_docs = [' '.join(proc_doc) for proc_doc in proc_docs]\n",
    "\n",
    "    # Choose topic representation model\n",
    "    if model_params['repr_model'] == 'none':\n",
    "        \n",
    "        # Standard BERTopic topic model\n",
    "        #res_model = BERTopic(embedding_model=sentence_model, ctfidf_model=ctfidf_model)\n",
    "        res_model = BERTopic(embedding_model=sentence_model, vectorizer_model=vectorizer_model, top_n_words=topn_words)\n",
    "        #res_model = BERTopic(embedding_model=sentence_model)\n",
    "        topics, probs = res_model.fit_transform(proc_docs)\n",
    "\n",
    "    elif model_params['repr_model'] == 'keybert':\n",
    "\n",
    "        # Fine-tune topic representations using KeyBERT inspired representations\n",
    "        repr_model = KeyBERTInspired()\n",
    "        res_model = BERTopic(representation_model=repr_model, top_n_words=topn_words)\n",
    "        topics, probs = res_model.fit_transform(proc_docs)\n",
    "\n",
    "    # Get dataframe listing each document in a row, giving the topic for that doc\n",
    "    dfdocs = res_model.get_document_info(proc_docs)\n",
    "\n",
    "    # Iterate through clusters to get the docs in each topic\n",
    "    print('\\tgetting docs in each topic')\n",
    "    all_topics = []\n",
    "    doc_topics = []\n",
    "    for topic in np.unique(topics):\n",
    "    \n",
    "        # Get topic words and probs for this topic\n",
    "        all_topics.append((topic, res_model.get_topic(topic)))\n",
    "\n",
    "        # Get the document IDs corresponding to this topic\n",
    "        doc_topics.append({topic: dfdocs.index[dfdocs['Topic']==topic].tolist()})    \n",
    "\n",
    "    # In order to calculate a coherence score, the code below needs the documents tokenised;\n",
    "    # tokenise the docs with spacy\n",
    "    proc_docs2 = []\n",
    "    for doc in proc_docs:\n",
    "        tokens = nlp_tokens(doc)\n",
    "        proc_docs2.append([token.text for token in tokens])\n",
    "    \n",
    "    # Create a dictionary and BoW corpus -- needed for the coherence score\n",
    "    dict = corpora.Dictionary(proc_docs2)\n",
    "    corpus = [dict.doc2bow(doc) for doc in proc_docs2]\n",
    "\n",
    "    # Get lists of topics without tfidf (recall that all_topics is a list, where each element is\n",
    "    # a list of tuples for that topic; each tuple contains (word, tfidf_score).\n",
    "    all_topics_no_tfidf = []\n",
    "    for topic in all_topics:\n",
    "        all_topics_no_tfidf.append([word[0] for word in topic[1]])\n",
    "\n",
    "    # Coherence scores\n",
    "    coh_scores = get_coherence_scores(which_coh_scores, all_topics_no_tfidf, proc_docs2, corpus, dict, 'transformer')\n",
    "\n",
    "    # Return\n",
    "    return res_model, doc_topics, coh_scores, all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b55b9-4bd3-4693-8be0-fa1b360f9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# This section performs transformer-based topic modeling\n",
    "###########################################################\n",
    "\n",
    "# Init resultset\n",
    "init_result_df()\n",
    "\n",
    "# Define model runs\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {'model_name': 'bertopic', 'model_params': {'repr_model': 'none'}},\n",
    "        {'model_name': 'bertopic', 'model_params': {'repr_model': 'keybert'}}\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True, False],\n",
    "    'do_trans_textspeak': [True, False],\n",
    "    'do_add_synonyms': [False],  # not relevant for transformers\n",
    "    'do_add_hypernyms': [False],  # not relevant for transformers\n",
    "    'do_extract_keyphrases': [False]  # not relevant for transformers\n",
    "}\n",
    "\n",
    "# Define model runs - keyphrase extraction\n",
    "model_runs = {\n",
    "    'datasets': ['chitchat', 'topical chat', 'ubuntu dialogue', 'enron email'],\n",
    "    'models': [\n",
    "        {'model_name': 'bertopic', 'model_params': {'repr_model': 'none'}},\n",
    "        {'model_name': 'bertopic', 'model_params': {'repr_model': 'keybert'}}\n",
    "    ],\n",
    "    'num_docs': [250],\n",
    "    'do_check_spelling': [True],\n",
    "    'do_trans_textspeak': [True],\n",
    "    'do_add_synonyms': [False],  # not relevant for transformers\n",
    "    'do_add_hypernyms': [False],  # not relevant for transformers\n",
    "    'do_extract_keyphrases': [True]  # not relevant for transformers\n",
    "}\n",
    "\n",
    "# Run model\n",
    "for dataset in model_runs['datasets']:    \n",
    "    for num_docs in model_runs['num_docs']:\n",
    "        for do_check_spelling in model_runs['do_check_spelling']:\n",
    "            for do_trans_textspeak in model_runs['do_trans_textspeak']:\n",
    "                for do_add_synonyms in model_runs['do_add_synonyms']:\n",
    "                    for do_add_hypernyms in model_runs['do_add_hypernyms']:\n",
    "                        for do_extract_keyphrases in model_runs['do_extract_keyphrases']:\n",
    "\n",
    "                            # Preprocess\n",
    "                            proc_docs = preprocess_docs(dataset, num_docs, do_check_spelling, do_add_synonyms, do_trans_textspeak, do_add_hypernyms, \\\n",
    "                                           do_extract_keyphrases, print_spell_corrs=False, do_remove_stopwords=True, do_remove_punct=True, rnd_seed=77)\n",
    "                            \n",
    "                            # Model runs\n",
    "                            for the_model in model_runs['models']:\n",
    "        \n",
    "                                # Get model name and parameters\n",
    "                                model_name = the_model['model_name']\n",
    "                                model_params = the_model['model_params']\n",
    "                                \n",
    "                                # Preprocess and run the model\n",
    "                                timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "                                run_model(proc_docs=proc_docs, the_model=model_name, rnd_seed=77, num_docs=num_docs, \n",
    "                                    model_params=model_params, \n",
    "                                    which_coh_scores=['c_v', 'c_uci', 'c_npmi', 'u_mass'],\n",
    "                                    print_docs_by_topic=False, timestamp=timestamp,\n",
    "                                    dataset=dataset, do_check_spelling=do_check_spelling, do_add_synonyms=do_add_synonyms, do_trans_textspeak=do_trans_textspeak, \\\n",
    "                                    do_add_hypernyms=do_add_hypernyms, do_extract_keyphrases=do_extract_keyphrases, print_spell_corrs=False, \\\n",
    "                                    do_remove_stopwords=True, do_remove_punct=True)\n",
    "\n",
    "                                # Show/store the results\n",
    "                                display(dfr.tail(1))\n",
    "                                dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\transformer\\\\dfr_' + timestamp + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d3bfc-b2a6-4632-ac8b-fadd3dd5ae9c",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "This section includes code to perform various postprocessing tasks:\n",
    "\n",
    "1. Load the resulting dataframes for each model run from disk and concatenate them together into a single dataframe.\n",
    "2. Postprocess the topic representations using Google's flan-t5 LLM text generator.\n",
    "3. Calculate the cosine similarity of topic representations to each document (used for topice relevance modeling; see eval1.ipynb).\n",
    "4. Add synonyms and hypernyms to postprocessed topic representations for second-round LLM topic generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c52c57aa-ff76-4d16-9d0d-2bd68ebfe1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the last pickle file of the specified type (bow, word2vec, or transformer)\n",
    "def load_last_pickle(model_type):\n",
    "    \"\"\"\n",
    "    Purpose:                     To load the last dataframe saved in pandas pickle format.\n",
    "    Parameters:\n",
    "        model_type               The model type to load, which basically just points to a certain directory to load the pickle from.\n",
    "    Returns:\n",
    "        dftmp                    The dataframe loaded from the pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    d = os.listdir(f'{pickle_dir}/{model_type}')\n",
    "    last_pickle = sorted(d, reverse=True)[0]\n",
    "    dftmp = pd.read_pickle(f'{pickle_dir}/{model_type}/{last_pickle}')\n",
    "    return dftmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709d6c1e-5d6d-48fc-a3a5-4e74444f3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>Umass_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypernyms</th>\n",
       "      <th>Keyphrases</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.401706</td>\n",
       "      <td>-8.115216</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-11.403467</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20.838450</td>\n",
       "      <td>20240504_122908</td>\n",
       "      <td>[(0, [('depression', 0.32700404971148017), ('a...</td>\n",
       "      <td>{13: [0, 20, 23, 27, 31, 43, 44, 45, 47, 52, 6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>28</td>\n",
       "      <td>{'num_topics': 30}</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>-8.521523</td>\n",
       "      <td>-0.018598</td>\n",
       "      <td>-12.967615</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>22.553983</td>\n",
       "      <td>20240504_122928</td>\n",
       "      <td>[(0, [('anxiety', -0.327060164681567), ('depre...</td>\n",
       "      <td>{14: [0, 238], 29: [1, 95, 122, 152, 174, 188,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>nmf</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15, 'num_passes': 15}</td>\n",
       "      <td>0.339439</td>\n",
       "      <td>-7.339113</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>-11.541465</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>22.713817</td>\n",
       "      <td>20240504_122951</td>\n",
       "      <td>[(0, [('facebook', 0.038304123343624014), ('Fl...</td>\n",
       "      <td>{2: [0, 11, 15, 18, 21, 91, 94, 100, 111, 113,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>nmf</td>\n",
       "      <td>30</td>\n",
       "      <td>{'num_topics': 30, 'num_passes': 15}</td>\n",
       "      <td>0.306261</td>\n",
       "      <td>-8.134659</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>-12.814216</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27.611870</td>\n",
       "      <td>20240504_123014</td>\n",
       "      <td>[(0, [('piano', 0.12928540049704657), ('classi...</td>\n",
       "      <td>{23: [0, 120, 121, 141, 152, 197], 16: [1, 24,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lda</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15, 'num_passes': 25}</td>\n",
       "      <td>0.344574</td>\n",
       "      <td>-8.195870</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-12.505199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>23.860184</td>\n",
       "      <td>20240504_123042</td>\n",
       "      <td>[(0, [('video games', 0.016271917), ('Demi Lov...</td>\n",
       "      <td>{13: [0, 1, 35, 53, 78, 88, 97, 106, 113, 123,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset Num_docs Rnd_seed Model Num_topics  \\\n",
       "0  chitchat      250       77   lsi         15   \n",
       "1  chitchat      250       77   lsi         28   \n",
       "2  chitchat      250       77   nmf         15   \n",
       "3  chitchat      250       77   nmf         30   \n",
       "4  chitchat      250       77   lda         15   \n",
       "\n",
       "                           Model_params  Cv_score  Cuci_score  Cnpmi_score  \\\n",
       "0                    {'num_topics': 15}  0.401706   -8.115216     0.013298   \n",
       "1                    {'num_topics': 30}  0.326996   -8.521523    -0.018598   \n",
       "2  {'num_topics': 15, 'num_passes': 15}  0.339439   -7.339113     0.062400   \n",
       "3  {'num_topics': 30, 'num_passes': 15}  0.306261   -8.134659     0.001097   \n",
       "4  {'num_topics': 15, 'num_passes': 25}  0.344574   -8.195870    -0.000261   \n",
       "\n",
       "   Umass_score  ... Hypernyms Keyphrases    Runtime        Timestamp  \\\n",
       "0   -11.403467  ...     False       True  20.838450  20240504_122908   \n",
       "1   -12.967615  ...     False       True  22.553983  20240504_122928   \n",
       "2   -11.541465  ...     False       True  22.713817  20240504_122951   \n",
       "3   -12.814216  ...     False       True  27.611870  20240504_123014   \n",
       "4   -12.505199  ...     False       True  23.860184  20240504_123042   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('depression', 0.32700404971148017), ('a...   \n",
       "1  [(0, [('anxiety', -0.327060164681567), ('depre...   \n",
       "2  [(0, [('facebook', 0.038304123343624014), ('Fl...   \n",
       "3  [(0, [('piano', 0.12928540049704657), ('classi...   \n",
       "4  [(0, [('video games', 0.016271917), ('Demi Lov...   \n",
       "\n",
       "                                          Doc_topics Flan_topic  \\\n",
       "0  {13: [0, 20, 23, 27, 31, 43, 44, 45, 47, 52, 6...        NaN   \n",
       "1  {14: [0, 238], 29: [1, 95, 122, 152, 174, 188,...        NaN   \n",
       "2  {2: [0, 11, 15, 18, 21, 91, 94, 100, 111, 113,...        NaN   \n",
       "3  {23: [0, 120, 121, 141, 152, 197], 16: [1, 24,...        NaN   \n",
       "4  {13: [0, 1, 35, 53, 78, 88, 97, 106, 113, 123,...        NaN   \n",
       "\n",
       "  Cosine_similarity Topic_words2 Flan_topic2  \n",
       "0               NaN          NaN         NaN  \n",
       "1               NaN          NaN         NaN  \n",
       "2               NaN          NaN         NaN  \n",
       "3               NaN          NaN         NaN  \n",
       "4               NaN          NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate resultsets of all three model familes into one df\n",
    "\n",
    "# Init results df\n",
    "init_result_df()\n",
    "\n",
    "# Add each model's latest pickle to the results df\n",
    "for mtype in ['bow', 'word2vec', 'transformer']:\n",
    "    dfr_tmp = load_last_pickle(mtype)\n",
    "    dfr = pd.concat([dfr, dfr_tmp], ignore_index=True)\n",
    "print(dfr.shape)\n",
    "display(dfr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec40b62f-7f3f-4d13-a5b1-a4e0b592d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune topic representations using Google flan-t5-base text generator\n",
    "def flan(word_list):\n",
    "    \"\"\"\n",
    "    Purpose:                     To generate topic representations using Google's flan-t5 LLM text generator.\n",
    "    Parameters:\n",
    "        word_list                List of topic representations to convert to a single topic representation.\n",
    "    Returns:\n",
    "        repr                     The topic representation generated by the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for empty list\n",
    "    if len(word_list) == 0:\n",
    "        print(\"\\tempty word list!\")\n",
    "        return []\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = f\"I have a topic described by the following keywords: {word_list}. Based on the previous keywords, what is this topic about?\"\n",
    "    if debug: print(word_list)\n",
    "\n",
    "    # Create representation model\n",
    "    repr_model = TextGeneration(generator, prompt=prompt)\n",
    "\n",
    "    # Use the representation model in BERTopic on top of the default pipeline\n",
    "    flan_tm = BERTopic(representation_model=repr_model)\n",
    "\n",
    "    # Fit the model\n",
    "    topics, probs = flan_tm.fit_transform(word_list)\n",
    "\n",
    "    # Print\n",
    "    dftopic_repr = flan_tm.get_topic_info()\n",
    "    repr = dftopic_repr['Representation'].values[0][0]\n",
    "\n",
    "    # Return\n",
    "    return repr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b00f6e77-63b4-469a-8b74-63e8a4612713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add flan topic representations to the results df\n",
    "def add_flan(topic_colname, flan_colname):\n",
    "    \"\"\"\n",
    "    Purpose:                     To iterate through the resultset dataframe, adding first- or second-round LLM-generated topic representations.\n",
    "    Parameters:\n",
    "        topic_colname            The column name in the resultset dataframe containing the list of topic words from which to generate representations.\n",
    "        flan_colname             The column name in which to store the LLM-generated topic representations.\n",
    "    Returns:\n",
    "        None                     The 'dfr' dataframe is updated as a global variable.\n",
    "    \"\"\"\n",
    "\n",
    "    global dfr\n",
    "    \n",
    "    # Iterate through results dataframe\n",
    "    for i, row in dfr.iterrows():\n",
    "    \n",
    "        # Skip rows that have already been done\n",
    "        if not pd.isna(row[flan_colname]):\n",
    "            continue\n",
    "        print(f'working on row {i}')\n",
    "    \n",
    "        # Init flan topic list\n",
    "        flan_topics = []\n",
    "    \n",
    "        # Iterate through each topic in this result row\n",
    "        #print(row[topic_colname])\n",
    "        if type(row[topic_colname]) == str:\n",
    "            row[topic_colname] = json.loads(row[topic_colname])\n",
    "        for topic in row[topic_colname]:\n",
    "            word_list = [word[0] for word in topic[1]][:num_topics_for_flan]\n",
    "            print(f\"\\ttopic #{topic[0]}\")\n",
    "            print(f\"\\t\\t{word_list}\")\n",
    "            topic_repr = ''\n",
    "            try:\n",
    "                topic_repr = flan(word_list)\n",
    "            except Exception as ex:\n",
    "                if ex == 'ValueError: k must be less than or equal to the number of training points':\n",
    "                    topic_repr = flan(word_list * 2)\n",
    "            print(f\"\\t\\ttopic representation: {topic_repr}\")\n",
    "            flan_topics.append((topic[0], topic_repr))\n",
    "    \n",
    "        # Convert the numpy int32s to plain ints so that json.dumps() can handle them\n",
    "        new_flan_topics = []\n",
    "        for t in flan_topics:\n",
    "            newt = (int(t[0]), t[1])\n",
    "            new_flan_topics.append(newt)\n",
    "    \n",
    "        # Add the flan topics to the data frame\n",
    "        dfr.at[i, flan_colname] = json.dumps(new_flan_topics)\n",
    "    \n",
    "        # Pickle\n",
    "        timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "        dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\flan\\\\dfr_' + timestamp + '.pkl')\n",
    "        display(dfr.iloc[i:i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef11e7-e91d-4222-b32c-881ed4ebe917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flan topics\n",
    "add_flan('Topic_words', 'Flan_topic')\n",
    "display(dfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "204434ba-66d9-430b-bc55-3a67bbac1f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>Umass_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypernyms</th>\n",
       "      <th>Keyphrases</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.401706</td>\n",
       "      <td>-8.115216</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-11.403467</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20.838450</td>\n",
       "      <td>20240504_122908</td>\n",
       "      <td>[(0, [('depression', 0.32700404971148017), ('a...</td>\n",
       "      <td>{13: [0, 20, 23, 27, 31, 43, 44, 45, 47, 52, 6...</td>\n",
       "      <td>[[0, \"Mental health\"], [1, \"Disney movies\"], [...</td>\n",
       "      <td>{\"13\": [[0, 0.004727383144199848], [20, 0.1268...</td>\n",
       "      <td>[[0, [[\"outcome\", 0], [\"sensitise\", 0], [\"prov...</td>\n",
       "      <td>[[0, \"psychology\"], [1, \"Christmas music\"], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>28</td>\n",
       "      <td>{'num_topics': 30}</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>-8.521523</td>\n",
       "      <td>-0.018598</td>\n",
       "      <td>-12.967615</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>22.553983</td>\n",
       "      <td>20240504_122928</td>\n",
       "      <td>[(0, [('anxiety', -0.327060164681567), ('depre...</td>\n",
       "      <td>{14: [0, 238], 29: [1, 95, 122, 152, 174, 188,...</td>\n",
       "      <td>[[0, \"Mental health\"], [1, \"Disney movies\"], [...</td>\n",
       "      <td>{\"14\": [[0, 0.0741310641169548], [238, 0.13844...</td>\n",
       "      <td>[[0, [[\"hand\", 0], [\"outcome\", 0], [\"straddle\"...</td>\n",
       "      <td>[[0, \"quinoa\"], [1, \"Christmas music\"], [2, \"I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>nmf</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15, 'num_passes': 15}</td>\n",
       "      <td>0.339439</td>\n",
       "      <td>-7.339113</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>-11.541465</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>22.713817</td>\n",
       "      <td>20240504_122951</td>\n",
       "      <td>[(0, [('facebook', 0.038304123343624014), ('Fl...</td>\n",
       "      <td>{2: [0, 11, 15, 18, 21, 91, 94, 100, 111, 113,...</td>\n",
       "      <td>[[0, \"Florida man\"], [1, \"Science/Tech\"], [2, ...</td>\n",
       "      <td>{\"2\": [[0, 0.04024161398410797], [11, 0.136339...</td>\n",
       "      <td>[[0, [[\"record\", 0], [\"cantonment\", 0], [\"jam\"...</td>\n",
       "      <td>[[0, \"Film\"], [1, \"Humans\"], [2, \"Animal\"], [3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>nmf</td>\n",
       "      <td>30</td>\n",
       "      <td>{'num_topics': 30, 'num_passes': 15}</td>\n",
       "      <td>0.306261</td>\n",
       "      <td>-8.134659</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>-12.814216</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27.611870</td>\n",
       "      <td>20240504_123014</td>\n",
       "      <td>[(0, [('piano', 0.12928540049704657), ('classi...</td>\n",
       "      <td>{23: [0, 120, 121, 141, 152, 197], 16: [1, 24,...</td>\n",
       "      <td>[[0, \"music\"], [1, \"Science/Tech\"], [2, \"psych...</td>\n",
       "      <td>{\"23\": [[0, 0.09239575266838074], [120, 0.2001...</td>\n",
       "      <td>[[0, [[\"role\", 0], [\"shape\", 0], [\"forte\", 0],...</td>\n",
       "      <td>[[0, \"Industrial design\"], [1, \"My Hero Academ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lda</td>\n",
       "      <td>15</td>\n",
       "      <td>{'num_topics': 15, 'num_passes': 25}</td>\n",
       "      <td>0.344574</td>\n",
       "      <td>-8.195870</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-12.505199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>23.860184</td>\n",
       "      <td>20240504_123042</td>\n",
       "      <td>[(0, [('video games', 0.016271917), ('Demi Lov...</td>\n",
       "      <td>{13: [0, 1, 35, 53, 78, 88, 97, 106, 113, 123,...</td>\n",
       "      <td>[[0, \"Science/Tech\"], [1, \"Science/Tech\"], [2,...</td>\n",
       "      <td>{\"13\": [[0, 0.08658034354448318], [1, 0.149860...</td>\n",
       "      <td>[[0, [[\"video\", 0], [\"clothing\", 0], [\"moon\", ...</td>\n",
       "      <td>[[0, \"Science fiction\"], [1, \"Criminal institu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset Num_docs Rnd_seed Model Num_topics  \\\n",
       "0  chitchat      250       77   lsi         15   \n",
       "1  chitchat      250       77   lsi         28   \n",
       "2  chitchat      250       77   nmf         15   \n",
       "3  chitchat      250       77   nmf         30   \n",
       "4  chitchat      250       77   lda         15   \n",
       "\n",
       "                           Model_params  Cv_score  Cuci_score  Cnpmi_score  \\\n",
       "0                    {'num_topics': 15}  0.401706   -8.115216     0.013298   \n",
       "1                    {'num_topics': 30}  0.326996   -8.521523    -0.018598   \n",
       "2  {'num_topics': 15, 'num_passes': 15}  0.339439   -7.339113     0.062400   \n",
       "3  {'num_topics': 30, 'num_passes': 15}  0.306261   -8.134659     0.001097   \n",
       "4  {'num_topics': 15, 'num_passes': 25}  0.344574   -8.195870    -0.000261   \n",
       "\n",
       "   Umass_score  ... Hypernyms Keyphrases    Runtime        Timestamp  \\\n",
       "0   -11.403467  ...     False       True  20.838450  20240504_122908   \n",
       "1   -12.967615  ...     False       True  22.553983  20240504_122928   \n",
       "2   -11.541465  ...     False       True  22.713817  20240504_122951   \n",
       "3   -12.814216  ...     False       True  27.611870  20240504_123014   \n",
       "4   -12.505199  ...     False       True  23.860184  20240504_123042   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('depression', 0.32700404971148017), ('a...   \n",
       "1  [(0, [('anxiety', -0.327060164681567), ('depre...   \n",
       "2  [(0, [('facebook', 0.038304123343624014), ('Fl...   \n",
       "3  [(0, [('piano', 0.12928540049704657), ('classi...   \n",
       "4  [(0, [('video games', 0.016271917), ('Demi Lov...   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {13: [0, 20, 23, 27, 31, 43, 44, 45, 47, 52, 6...   \n",
       "1  {14: [0, 238], 29: [1, 95, 122, 152, 174, 188,...   \n",
       "2  {2: [0, 11, 15, 18, 21, 91, 94, 100, 111, 113,...   \n",
       "3  {23: [0, 120, 121, 141, 152, 197], 16: [1, 24,...   \n",
       "4  {13: [0, 1, 35, 53, 78, 88, 97, 106, 113, 123,...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"Mental health\"], [1, \"Disney movies\"], [...   \n",
       "1  [[0, \"Mental health\"], [1, \"Disney movies\"], [...   \n",
       "2  [[0, \"Florida man\"], [1, \"Science/Tech\"], [2, ...   \n",
       "3  [[0, \"music\"], [1, \"Science/Tech\"], [2, \"psych...   \n",
       "4  [[0, \"Science/Tech\"], [1, \"Science/Tech\"], [2,...   \n",
       "\n",
       "                                   Cosine_similarity  \\\n",
       "0  {\"13\": [[0, 0.004727383144199848], [20, 0.1268...   \n",
       "1  {\"14\": [[0, 0.0741310641169548], [238, 0.13844...   \n",
       "2  {\"2\": [[0, 0.04024161398410797], [11, 0.136339...   \n",
       "3  {\"23\": [[0, 0.09239575266838074], [120, 0.2001...   \n",
       "4  {\"13\": [[0, 0.08658034354448318], [1, 0.149860...   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"outcome\", 0], [\"sensitise\", 0], [\"prov...   \n",
       "1  [[0, [[\"hand\", 0], [\"outcome\", 0], [\"straddle\"...   \n",
       "2  [[0, [[\"record\", 0], [\"cantonment\", 0], [\"jam\"...   \n",
       "3  [[0, [[\"role\", 0], [\"shape\", 0], [\"forte\", 0],...   \n",
       "4  [[0, [[\"video\", 0], [\"clothing\", 0], [\"moon\", ...   \n",
       "\n",
       "                                         Flan_topic2  \n",
       "0  [[0, \"psychology\"], [1, \"Christmas music\"], [2...  \n",
       "1  [[0, \"quinoa\"], [1, \"Christmas music\"], [2, \"I...  \n",
       "2  [[0, \"Film\"], [1, \"Humans\"], [2, \"Animal\"], [3...  \n",
       "3  [[0, \"Industrial design\"], [1, \"My Hero Academ...  \n",
       "4  [[0, \"Science fiction\"], [1, \"Criminal institu...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init results df\n",
    "init_result_df()\n",
    "\n",
    "# Add each model's latest pickle to the results df\n",
    "dfr = load_last_pickle('flan')\n",
    "print(dfr.shape)\n",
    "display(dfr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4328cd9-1e73-43fb-9a33-740474f32564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the cosine similarity scores between each topic in a certain row of the result dataset\n",
    "# and all the documents assigned to that topic. The return value will be a dict with the format:\n",
    "# {topic_num: [(doc_num, cosine_similarity), (doc_num, cosine_similarity) ...]}\n",
    "# the \"repr\" parameter is the column name defining which set of topic representions to use (the non-postprocessed representions, flan-postprocessed, syn/hyper-post-processed)\n",
    "\n",
    "def get_cos_sims(dfr, row_in_dfr, repr_fld):\n",
    "    \"\"\"\n",
    "    Purpose:                 To calculate the cosine similarity scores for a given row in the resultset dataframe.\n",
    "    Parameters:\n",
    "        row_in_dfr           The index of the row in the resultset dataframe for which scores will be calculated.\n",
    "        repr_fld             The column name containing the topic representations with which calculations should be performed.\n",
    "    Returns:\n",
    "        r                    List of calculated cosine similarity scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init return value\n",
    "    r = {}\n",
    "\n",
    "    # Get the dataset tested in this row\n",
    "    dataset = dfr.loc[row_in_dfr, 'Dataset']\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"row {row_in_dfr}, dataset {dataset}\")\n",
    "    \n",
    "    # Get list of document indices\n",
    "    if dataset == 'chitchat':\n",
    "        alldocs_i = list(dfcc.index)\n",
    "        dfdata = dfcc\n",
    "    elif dataset == 'topical chat':\n",
    "        alldocs_i = list(dftc.index)\n",
    "        dfdata = dftc\n",
    "    elif dataset == 'ubuntu dialogue':\n",
    "        alldocs_i = list(dfud.index)\n",
    "        dfdata = dfud\n",
    "    elif dataset == 'enron email':\n",
    "        alldocs_i = list(dfee.index)\n",
    "        dfdata = dfee\n",
    "    \n",
    "    # Since we chose 250 docs at random, this will be the document # in the Doc_topics column;\n",
    "    # we'll need to find which document id in the chitchat dataset this belongs to\n",
    "    random.seed(dfr.loc[row_in_dfr, 'Rnd_seed'])\n",
    "    docs_i = random.sample(alldocs_i, dfr.loc[row_in_dfr, 'Num_docs'])\n",
    "    if debug:\n",
    "        print(f\"{dataset} document indexes\")\n",
    "        print(docs_i)\n",
    "        print()\n",
    "    \n",
    "    # Get the document topics for this row of the results df\n",
    "    doc_topics = dfr.loc[row_in_dfr, 'Doc_topics']\n",
    "    print(f\"\\t{len(doc_topics)} document topics, type is {type(doc_topics)}\")\n",
    "    if debug:\n",
    "        print(doc_topics)\n",
    "    print()\n",
    "    \n",
    "    # Get the topic words for this row in the results df; it will be a dict keyed on topic number, \n",
    "    # with the values being the list of documents that correspond to that topic #\n",
    "    all_topic_words = dfr.loc[row_in_dfr, repr_fld]\n",
    "\n",
    "    # Convert all_topic_words to list if necessary\n",
    "    if type(all_topic_words) == str:\n",
    "        all_topic_words = json.loads(all_topic_words)\n",
    "    print(f\"\\t{len(all_topic_words)} sets of topic words for this row, type is {type(all_topic_words)}\")\n",
    "    if debug:\n",
    "        print(all_topic_words)\n",
    "    print()\n",
    "\n",
    "    # Convert doc_topics to dict, if necessary;\n",
    "    # doc_topics will either be a dict that looks like this:\n",
    "    # {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...\n",
    "    # or a list that looks like this:\n",
    "    # [{-1: [0, 1, 10, 13, 34, ...\n",
    "    if type(doc_topics) == list:\n",
    "        tmp = {}\n",
    "        for d in doc_topics:\n",
    "            for k in d.keys():\n",
    "                tmp[k] = d[k]\n",
    "        doc_topics = tmp\n",
    "                \n",
    "    # Iterate through the document topics for this row of the result df\n",
    "    for which_topic in doc_topics.keys():\n",
    "\n",
    "        # Print the topic words for this topic\n",
    "        print(f\"\\t\\ttopic {which_topic}\")\n",
    "        print()\n",
    "        \n",
    "        # Extract the topic words for this topic\n",
    "        topic_words = [t[0] for t in all_topic_words[which_topic][1]]\n",
    "        if debug:\n",
    "            print(\"\\t\\ttopic words for this topic\")\n",
    "            print(topic_words)\n",
    "            print()\n",
    "    \n",
    "        # Init an array that will hold the text of the documents assigned to this topic;\n",
    "        # each element in the array will be the full text of the document\n",
    "        docs_list = []\n",
    "        \n",
    "        # Iterate through the documents assigned to this topic\n",
    "        for which_doc in doc_topics[which_topic]:\n",
    "    \n",
    "            # Print which topic and document # we're working with\n",
    "            if debug:\n",
    "                print(f\"\\t\\tdocument #{which_doc} has topic #{which_topic} assigned to it\")\n",
    "                print(f\"\\t\\tdocument #{which_doc} in the {dataset} dataset has document id {docs_i[which_doc]}\")\n",
    "                print()\n",
    "            \n",
    "            # Print the first few lines of the document this corresponds to\n",
    "            s = dfdata.loc[docs_i[which_doc], 'txt']\n",
    "            if debug:\n",
    "                print(\"\\t\\tdocument text\")\n",
    "                print(s[:128])\n",
    "                print()\n",
    "    \n",
    "            # Append the document text to the array\n",
    "            docs_list.append(s)\n",
    "    \n",
    "        # Calculate the cosine similarity between the document text and the topic words.\n",
    "        # util.cos_sim takes two lists and calculates the cos similarity between all combinations of each item in both lists.\n",
    "        # So if you have 2 elements in the first list and 3 in the second, you will get a 2x3 tensor returned.\n",
    "        # Since we are only passing a single topic in the first list and a list of documents in the second, the cos_sim will return\n",
    "        # a 1x[num_docs] tensor, and we only need the first element in the outer list since there is only one element. Hence, the [0] below.\n",
    "        embeddings1 = sentence_model.encode([' '.join(topic_words)])\n",
    "        embeddings2 = sentence_model.encode(docs_list)\n",
    "        cos_scores = util.cos_sim(embeddings1, embeddings2).tolist()[0]\n",
    "\n",
    "        # Since cos_sim returns a simple list, we have no idea which document number corresponds to which score in the list;\n",
    "        # we need to backreference it by building a document map: {cos score: document index}.\n",
    "        # We can build this from the doc_topics dict, which looks like this:\n",
    "        # {topic_num: [(topic_word, score), (topic_word, score) ...]}\n",
    "        doc_map = [docid for docid in doc_topics[which_topic]]\n",
    "        \n",
    "        # Add scores to the result dict\n",
    "        r[int(which_topic)] = [(int(doc_map[i]), score) for i, score in enumerate(cos_scores)]\n",
    "        print(f\"\\t\\t\\tgot {len(cos_scores)} cosine similarity scores for topic {which_topic}\")\n",
    "        if debug:\n",
    "            print(cos_scores.tolist())\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    # Return\n",
    "    if debug:\n",
    "        print(r)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f4ec9bd0-b26b-4085-ab56-0078bc018e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 25)\n"
     ]
    }
   ],
   "source": [
    "# Temp - fix cos sims for certain rows\n",
    "#print(dfr.loc[dfr.index>=96])\n",
    "#print(dfr.shape)\n",
    "#dfr.loc[dfr.index>=96, 'Cosine_similarity'] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dda543c-50a9-48d5-840e-8e31e8b626ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity over a set of rows in the result dataframe\n",
    "def calc_cos_sim(cos_sim_fld, repr_fld):\n",
    "    \"\"\"\n",
    "    Purpose:            To iterate through the resultset dataframe, calculating cosine similarity scores between documents and topic representations.\n",
    "    Parameters:\n",
    "        cos_sim_fld     The column name in in which to store the calculated cosine similarity socres.\n",
    "        repr_fld        The column name containing the topic representations used for calculations.\n",
    "    Returns:\n",
    "        None            The 'dfr' dataframe is updated as a global variable.\n",
    "    \"\"\"\n",
    "\n",
    "    global dfr\n",
    "    \n",
    "    # Iterate over the result df\n",
    "    for i, row in dfr.iterrows():\n",
    "    \n",
    "        # Skip rows that have already been done\n",
    "        if not pd.isna(row[cos_sim_fld]):\n",
    "            continue\n",
    "            \n",
    "        # Calc the cos similarity score\n",
    "        print(i)\n",
    "        cos_sims = get_cos_sims(dfr, i, repr_fld)\n",
    "    \n",
    "        # Convert the numpy int32s to plain ints so that json.dumps() can handle them\n",
    "        #new_cos_sims = []\n",
    "        #for t in cos_sims,keys():\n",
    "        #    print(t)\n",
    "        #    newt = (int(t[0]), t[1])\n",
    "        #    new_cos_sims.append(newt)\n",
    "    \n",
    "        # Update df\n",
    "        dfr.loc[i, cos_sim_fld] = json.dumps(cos_sims)\n",
    "    \n",
    "        # Pickle\n",
    "        timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "        dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\sim\\\\dfr_' + timestamp + '.pkl')\n",
    "        display(dfr.iloc[i:i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c05b17-c3e5-4c4c-aeb5-68189719651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section calculates document similarity scores between each topic and the documents assigned to that topic\n",
    "#calc_cos_sim('Cosine_similarity', 'Topic_words')  # done\n",
    "#calc_cos_sim('Cosine_similarity_flan', 'Flan_topic') # done\n",
    "#calc_cos_sim('Cosine_similarity_flan2', 'Flan_topic2') # done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2488fa0-d62b-4a94-80da-a5593dbc4bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>8</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>-12.541200</td>\n",
       "      <td>-0.442860</td>\n",
       "      <td>...</td>\n",
       "      <td>77.495494</td>\n",
       "      <td>20240401_133320</td>\n",
       "      <td>[(0, [('tear', 0.1492861747332724), ('eye', 0....</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.378848</td>\n",
       "      <td>-14.478989</td>\n",
       "      <td>-0.512248</td>\n",
       "      <td>...</td>\n",
       "      <td>93.398113</td>\n",
       "      <td>20240401_133437</td>\n",
       "      <td>[(0, [('change', 0.041310894306892804), ('expe...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...</td>\n",
       "      <td>{\"0\": [[0, 0.05856030061841011], [1, 0.0758973...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...</td>\n",
       "      <td>[[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>1</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>-13.535590</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>...</td>\n",
       "      <td>133.312275</td>\n",
       "      <td>20240401_133611</td>\n",
       "      <td>[(0, [('pass', 0.02115486804364902), ('break',...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...</td>\n",
       "      <td>{\"0\": [[0, 0.0338706336915493], [1, -0.0393538...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...</td>\n",
       "      <td>[[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.662265</td>\n",
       "      <td>-13.528606</td>\n",
       "      <td>-0.484894</td>\n",
       "      <td>...</td>\n",
       "      <td>137.099871</td>\n",
       "      <td>20240401_133824</td>\n",
       "      <td>[(0, [('experience', 0.01997448274133498), ('p...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"game\"], [2, \"Electrical out...</td>\n",
       "      <td>{\"0\": [[0, 0.05463644862174988], [1, 0.0174532...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...</td>\n",
       "      <td>[[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>11</td>\n",
       "      <td>{'num_topics': 30}</td>\n",
       "      <td>0.249136</td>\n",
       "      <td>-12.437530</td>\n",
       "      <td>-0.436446</td>\n",
       "      <td>...</td>\n",
       "      <td>91.362547</td>\n",
       "      <td>20240401_134041</td>\n",
       "      <td>[(0, [('tear', 0.14928553793317761), ('eye', 0...</td>\n",
       "      <td>{0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Dataset Num_docs Rnd_seed Model Num_topics        Model_params  \\\n",
       "0      0  chitchat      250       77   lsi          8  {'num_topics': 15}   \n",
       "1      1  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "2      2  chitchat      250       77   lsi          1  {'num_topics': 15}   \n",
       "3      3  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "4      4  chitchat      250       77   lsi         11  {'num_topics': 30}   \n",
       "\n",
       "   Cv_score  Cuci_score  Cnpmi_score  ...     Runtime        Timestamp  \\\n",
       "0  0.256103  -12.541200    -0.442860  ...   77.495494  20240401_133320   \n",
       "1  0.378848  -14.478989    -0.512248  ...   93.398113  20240401_133437   \n",
       "2  0.618475  -13.535590    -0.484803  ...  133.312275  20240401_133611   \n",
       "3  0.662265  -13.528606    -0.484894  ...  137.099871  20240401_133824   \n",
       "4  0.249136  -12.437530    -0.436446  ...   91.362547  20240401_134041   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('tear', 0.1492861747332724), ('eye', 0....   \n",
       "1  [(0, [('change', 0.041310894306892804), ('expe...   \n",
       "2  [(0, [('pass', 0.02115486804364902), ('break',...   \n",
       "3  [(0, [('experience', 0.01997448274133498), ('p...   \n",
       "4  [(0, [('tear', 0.14928553793317761), ('eye', 0...   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "1  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "2  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "3  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "4  {0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "1  [[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...   \n",
       "2  [[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...   \n",
       "3  [[0, \"game\"], [1, \"game\"], [2, \"Electrical out...   \n",
       "4  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "\n",
       "                                   Cosine_similarity Model_family  \\\n",
       "0  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "1  {\"0\": [[0, 0.05856030061841011], [1, 0.0758973...          bow   \n",
       "2  {\"0\": [[0, 0.0338706336915493], [1, -0.0393538...          bow   \n",
       "3  {\"0\": [[0, 0.05463644862174988], [1, 0.0174532...          bow   \n",
       "4  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "1  [[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...   \n",
       "2  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...   \n",
       "3  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...   \n",
       "4  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "\n",
       "                                         Flan_topic2 Keyphrases  \n",
       "0  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "1  [[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...      False  \n",
       "2  [[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...      False  \n",
       "3  [[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...      False  \n",
       "4  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init results df\n",
    "init_result_df()\n",
    "\n",
    "# Add each model's latest pickle to the results df\n",
    "dfr = load_last_pickle('sim')\n",
    "print(dfr.shape)\n",
    "display(dfr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642c56c-929c-4a29-98f6-4f08f7c3164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synonyms and hypernyms to the flan topic words\n",
    "for i, row in dfr.iterrows():\n",
    "\n",
    "    # Skip rows that have already been done\n",
    "    if not pd.isna(row['Topic_words2']):\n",
    "        continue\n",
    "    print(i)\n",
    "\n",
    "    # Get topic words for all topics in this row\n",
    "    all_topic_words = row['Topic_words']\n",
    "    all_topic_words2 = []\n",
    "\n",
    "    # Iterate through each topic\n",
    "    for topic in all_topic_words:\n",
    "\n",
    "        # Get first ten words of this topic\n",
    "        this_topic_words = [word[0] for word in topic[1]][:10]\n",
    "        words = ' '.join(this_topic_words)\n",
    "\n",
    "        # Get synonyms\n",
    "        doc = nlp_tokens(words)\n",
    "        syns = get_synonyms(doc)[:10]\n",
    "\n",
    "        # Get hypernyms\n",
    "        hypernyms = get_hypernyms(doc)[:10]\n",
    "\n",
    "        # Add the synonyms and hypernyms to the topic words for this document\n",
    "        this_topic_words.extend(syns)\n",
    "        this_topic_words.extend(hypernyms)\n",
    "        this_topic_words = list(set(this_topic_words))\n",
    "\n",
    "        # Convert to tuples with arbitrary value\n",
    "        topic_tuples = (int(topic[0]), [(word, 0) for word in this_topic_words])\n",
    "\n",
    "        # Append\n",
    "        all_topic_words2.append(topic_tuples)\n",
    "        \n",
    "    dfr.at[i, 'Topic_words2'] =json.dumps(all_topic_words2)\n",
    "    #display(dfr.iloc[i:i+1])\n",
    "\n",
    "display(dfr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77041e6e-41f2-4559-bb22-71740fd9b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flan to new topic word list with synonyms/hypernyms\n",
    "add_flan('Topic_words2', 'Flan_topic2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "07cb2743-6692-4190-88b2-052082b04bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>8</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>-12.541200</td>\n",
       "      <td>-0.442860</td>\n",
       "      <td>...</td>\n",
       "      <td>77.495494</td>\n",
       "      <td>20240401_133320</td>\n",
       "      <td>[(0, [('tear', 0.1492861747332724), ('eye', 0....</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.378848</td>\n",
       "      <td>-14.478989</td>\n",
       "      <td>-0.512248</td>\n",
       "      <td>...</td>\n",
       "      <td>93.398113</td>\n",
       "      <td>20240401_133437</td>\n",
       "      <td>[(0, [('change', 0.041310894306892804), ('expe...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...</td>\n",
       "      <td>{\"0\": [[0, 0.05856030061841011], [1, 0.0758973...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...</td>\n",
       "      <td>[[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>1</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>-13.535590</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>...</td>\n",
       "      <td>133.312275</td>\n",
       "      <td>20240401_133611</td>\n",
       "      <td>[(0, [('pass', 0.02115486804364902), ('break',...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...</td>\n",
       "      <td>{\"0\": [[0, 0.0338706336915493], [1, -0.0393538...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...</td>\n",
       "      <td>[[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.662265</td>\n",
       "      <td>-13.528606</td>\n",
       "      <td>-0.484894</td>\n",
       "      <td>...</td>\n",
       "      <td>137.099871</td>\n",
       "      <td>20240401_133824</td>\n",
       "      <td>[(0, [('experience', 0.01997448274133498), ('p...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"game\"], [2, \"Electrical out...</td>\n",
       "      <td>{\"0\": [[0, 0.05463644862174988], [1, 0.0174532...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...</td>\n",
       "      <td>[[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>11</td>\n",
       "      <td>{'num_topics': 30}</td>\n",
       "      <td>0.249136</td>\n",
       "      <td>-12.437530</td>\n",
       "      <td>-0.436446</td>\n",
       "      <td>...</td>\n",
       "      <td>91.362547</td>\n",
       "      <td>20240401_134041</td>\n",
       "      <td>[(0, [('tear', 0.14928553793317761), ('eye', 0...</td>\n",
       "      <td>{0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Dataset Num_docs Rnd_seed Model Num_topics        Model_params  \\\n",
       "0      0  chitchat      250       77   lsi          8  {'num_topics': 15}   \n",
       "1      1  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "2      2  chitchat      250       77   lsi          1  {'num_topics': 15}   \n",
       "3      3  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "4      4  chitchat      250       77   lsi         11  {'num_topics': 30}   \n",
       "\n",
       "   Cv_score  Cuci_score  Cnpmi_score  ...     Runtime        Timestamp  \\\n",
       "0  0.256103  -12.541200    -0.442860  ...   77.495494  20240401_133320   \n",
       "1  0.378848  -14.478989    -0.512248  ...   93.398113  20240401_133437   \n",
       "2  0.618475  -13.535590    -0.484803  ...  133.312275  20240401_133611   \n",
       "3  0.662265  -13.528606    -0.484894  ...  137.099871  20240401_133824   \n",
       "4  0.249136  -12.437530    -0.436446  ...   91.362547  20240401_134041   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('tear', 0.1492861747332724), ('eye', 0....   \n",
       "1  [(0, [('change', 0.041310894306892804), ('expe...   \n",
       "2  [(0, [('pass', 0.02115486804364902), ('break',...   \n",
       "3  [(0, [('experience', 0.01997448274133498), ('p...   \n",
       "4  [(0, [('tear', 0.14928553793317761), ('eye', 0...   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "1  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "2  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "3  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "4  {0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "1  [[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...   \n",
       "2  [[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...   \n",
       "3  [[0, \"game\"], [1, \"game\"], [2, \"Electrical out...   \n",
       "4  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "\n",
       "                                   Cosine_similarity Model_family  \\\n",
       "0  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "1  {\"0\": [[0, 0.05856030061841011], [1, 0.0758973...          bow   \n",
       "2  {\"0\": [[0, 0.0338706336915493], [1, -0.0393538...          bow   \n",
       "3  {\"0\": [[0, 0.05463644862174988], [1, 0.0174532...          bow   \n",
       "4  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "1  [[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...   \n",
       "2  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...   \n",
       "3  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...   \n",
       "4  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "\n",
       "                                         Flan_topic2 Keyphrases  \n",
       "0  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "1  [[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...      False  \n",
       "2  [[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...      False  \n",
       "3  [[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...      False  \n",
       "4  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>topical chat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>9</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>0.368992</td>\n",
       "      <td>-9.861310</td>\n",
       "      <td>-0.197724</td>\n",
       "      <td>...</td>\n",
       "      <td>11.998523</td>\n",
       "      <td>20240504_155104</td>\n",
       "      <td>[(-1, [('troopers', 0.5052358), ('military', 0...</td>\n",
       "      <td>[{-1: [2, 4, 6, 11, 12, 21, 22, 35, 36, 38, 41...</td>\n",
       "      <td>[[-1, \"World\"], [0, \"Science/Tech\"], [1, \"Aero...</td>\n",
       "      <td>{\"-1\": [[2, 0.08672984689474106], [4, 0.014690...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[-1, [[\"bad person\", 0], [\"chessman\", 0], [\"s...</td>\n",
       "      <td>[[-1, \"wikileaks\"], [0, \"YouTube\"], [1, \"Black...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>ubuntu dialogue</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>2</td>\n",
       "      <td>{'repr_model': 'none'}</td>\n",
       "      <td>0.542989</td>\n",
       "      <td>-14.670168</td>\n",
       "      <td>-0.459252</td>\n",
       "      <td>...</td>\n",
       "      <td>10.875522</td>\n",
       "      <td>20240504_155415</td>\n",
       "      <td>[(0, [('ubuntu', 0.14119847234575922), ('linux...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...</td>\n",
       "      <td>[[0, \"Linux\"], [1, \"World\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.2154170125722885], [1, 0.11550097...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"phone card\", 0], [\"bank bill\", 0], [\"a...</td>\n",
       "      <td>[[0, \"Science/Tech\"], [1, \"Fox\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>ubuntu dialogue</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>2</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>0.447699</td>\n",
       "      <td>-14.460991</td>\n",
       "      <td>-0.470929</td>\n",
       "      <td>...</td>\n",
       "      <td>11.713669</td>\n",
       "      <td>20240504_155426</td>\n",
       "      <td>[(0, [('ubuntu', 0.7777027), ('linux', 0.62036...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...</td>\n",
       "      <td>[[0, \"Linux\"], [1, \"World\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.1624004989862442], [1, 0.03420655...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"artifact\", 0], [\"adroitness\", 0], [\"ge...</td>\n",
       "      <td>[[0, \"xubuntu\"], [1, \"Race (United States)\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>enron email</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>3</td>\n",
       "      <td>{'repr_model': 'none'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134730</td>\n",
       "      <td>20240504_155926</td>\n",
       "      <td>[(0, [('gas', 0.07409395651732713), ('enron', ...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...</td>\n",
       "      <td>[[0, \"Energy\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.17197903990745544], [1, 0.2113838...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"futures\", 0], [\"entropy\", 0], [\"tradin...</td>\n",
       "      <td>[[0, \"Future\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>enron email</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>3</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.332339</td>\n",
       "      <td>20240504_155929</td>\n",
       "      <td>[(0, [('enron', 0.7303953), ('gas', 0.38910866...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...</td>\n",
       "      <td>[[0, \"Business\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.020173335447907448], [1, 0.006645...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"futures\", 0], [\"guard\", 0], [\"trading\"...</td>\n",
       "      <td>[[0, \"Business\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index          Dataset Num_docs Rnd_seed     Model Num_topics  \\\n",
       "395    395     topical chat      250       77  bertopic          9   \n",
       "396    396  ubuntu dialogue      250       77  bertopic          2   \n",
       "397    397  ubuntu dialogue      250       77  bertopic          2   \n",
       "398    398      enron email      250       77  bertopic          3   \n",
       "399    399      enron email      250       77  bertopic          3   \n",
       "\n",
       "                  Model_params  Cv_score  Cuci_score  Cnpmi_score  ...  \\\n",
       "395  {'repr_model': 'keybert'}  0.368992   -9.861310    -0.197724  ...   \n",
       "396     {'repr_model': 'none'}  0.542989  -14.670168    -0.459252  ...   \n",
       "397  {'repr_model': 'keybert'}  0.447699  -14.460991    -0.470929  ...   \n",
       "398     {'repr_model': 'none'}       NaN         NaN          NaN  ...   \n",
       "399  {'repr_model': 'keybert'}       NaN         NaN          NaN  ...   \n",
       "\n",
       "       Runtime        Timestamp  \\\n",
       "395  11.998523  20240504_155104   \n",
       "396  10.875522  20240504_155415   \n",
       "397  11.713669  20240504_155426   \n",
       "398   3.134730  20240504_155926   \n",
       "399   3.332339  20240504_155929   \n",
       "\n",
       "                                           Topic_words  \\\n",
       "395  [(-1, [('troopers', 0.5052358), ('military', 0...   \n",
       "396  [(0, [('ubuntu', 0.14119847234575922), ('linux...   \n",
       "397  [(0, [('ubuntu', 0.7777027), ('linux', 0.62036...   \n",
       "398  [(0, [('gas', 0.07409395651732713), ('enron', ...   \n",
       "399  [(0, [('enron', 0.7303953), ('gas', 0.38910866...   \n",
       "\n",
       "                                            Doc_topics  \\\n",
       "395  [{-1: [2, 4, 6, 11, 12, 21, 22, 35, 36, 38, 41...   \n",
       "396  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...   \n",
       "397  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...   \n",
       "398  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...   \n",
       "399  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...   \n",
       "\n",
       "                                            Flan_topic  \\\n",
       "395  [[-1, \"World\"], [0, \"Science/Tech\"], [1, \"Aero...   \n",
       "396                       [[0, \"Linux\"], [1, \"World\"]]   \n",
       "397                       [[0, \"Linux\"], [1, \"World\"]]   \n",
       "398                  [[0, \"Energy\"], [1, \"\"], [2, \"\"]]   \n",
       "399                [[0, \"Business\"], [1, \"\"], [2, \"\"]]   \n",
       "\n",
       "                                     Cosine_similarity Model_family  \\\n",
       "395  {\"-1\": [[2, 0.08672984689474106], [4, 0.014690...  transformer   \n",
       "396  {\"0\": [[0, 0.2154170125722885], [1, 0.11550097...  transformer   \n",
       "397  {\"0\": [[0, 0.1624004989862442], [1, 0.03420655...  transformer   \n",
       "398  {\"0\": [[0, 0.17197903990745544], [1, 0.2113838...  transformer   \n",
       "399  {\"0\": [[0, 0.020173335447907448], [1, 0.006645...  transformer   \n",
       "\n",
       "                                          Topic_words2  \\\n",
       "395  [[-1, [[\"bad person\", 0], [\"chessman\", 0], [\"s...   \n",
       "396  [[0, [[\"phone card\", 0], [\"bank bill\", 0], [\"a...   \n",
       "397  [[0, [[\"artifact\", 0], [\"adroitness\", 0], [\"ge...   \n",
       "398  [[0, [[\"futures\", 0], [\"entropy\", 0], [\"tradin...   \n",
       "399  [[0, [[\"futures\", 0], [\"guard\", 0], [\"trading\"...   \n",
       "\n",
       "                                           Flan_topic2 Keyphrases  \n",
       "395  [[-1, \"wikileaks\"], [0, \"YouTube\"], [1, \"Black...       True  \n",
       "396                  [[0, \"Science/Tech\"], [1, \"Fox\"]]       True  \n",
       "397      [[0, \"xubuntu\"], [1, \"Race (United States)\"]]       True  \n",
       "398                  [[0, \"Future\"], [1, \"\"], [2, \"\"]]       True  \n",
       "399                [[0, \"Business\"], [1, \"\"], [2, \"\"]]       True  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfr.head())\n",
    "display(dfr.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e46728-8aee-452d-9c38-e0ceb4f42c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section splices together first and second round modeling results - not needed anymore\n",
    "\n",
    "\"\"\"\n",
    "# Load last pickle that had the 320 records from earlier model runs\n",
    "dfr1 = pd.read_pickle(f'{pickle_dir}/flan/dfr_20240421_085109.pkl')\n",
    "dfr1['Keyphrases'] = False\n",
    "print(dfr1.shape)\n",
    "display(dfr1.head())\n",
    "print(dfr1.columns)\n",
    "print()\n",
    "\n",
    "# Load latest pickle\n",
    "dfr2 = load_last_pickle('flan')\n",
    "dfr2.loc[(dfr2['Model']=='lda') | (dfr2['Model']=='lsi') | (dfr2['Model']=='nmf'), 'Model_family'] = 'bow'\n",
    "dfr2.loc[dfr2['Model']=='word2vec', 'Model_family'] = 'word2vec'\n",
    "dfr2.loc[dfr2['Model']=='bertopic', 'Model_family'] = 'transformer'\n",
    "print(dfr2.shape)\n",
    "display(dfr2.head())\n",
    "print(dfr2.columns)\n",
    "print()\n",
    "\n",
    "# Concat df's\n",
    "dfr = pd.concat([dfr1, dfr2], ignore_index=True)\n",
    "dfr['index'] = dfr.index\n",
    "print(dfr.shape)\n",
    "display(dfr.head())\n",
    "display(dfr.tail())\n",
    "print(dfr.columns)\n",
    "print()\n",
    "\n",
    "# Save the combined pickle to the sim directory\n",
    "timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "dfr.to_pickle('C:\\\\tmp\\\\pickles\\\\sim\\\\dfr_' + timestamp + '.pkl')\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
