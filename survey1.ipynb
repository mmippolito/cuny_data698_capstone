{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d83217-5532-4dcb-9aa6-6000c61259f1",
   "metadata": {},
   "source": [
    "## CUNY DATA698\n",
    "### Topic Modeling for Forensics Analysis of Text-Based Conversations\n",
    "#### Michael Ippolito\n",
    "#### May 2024\n",
    "\n",
    "This is part of a series of Python Jupyter notebooks in support of my master's capstone project. The aim of the project is to study various methods of preprocessing, topic modeling, and postprocessing text-based conversation data often extracted from electronic devices recovered during criminal or cybersecurity investigations.\n",
    "\n",
    "The Jupyter notebooks used in this project are as follows:\n",
    "\n",
    "| Module | Purpose |\n",
    "|--------|---------|\n",
    "| eda1.ipynb | Exploratory data analysis of the four datasets used in the study. |\n",
    "| modeling1.ipynb | Loads and preprocesses the datasets, performs various topic models, postprocesses the topic representations. |\n",
    "| survey1.ipynb | Generates conversation text and topic representations to submit to Mechanical Turk. It later parses the results and incorporates them into my own hand-labeled results. |\n",
    "| survey2.ipynb | Loads Mechanical Turk survey results and evaluates them for quality based on reading speed and attention questinos. |\n",
    "| eval1.ipynb | Evaluates the topic modeling and survey results based on topic coherence, semantic quality, and topic relevance. |\n",
    "\n",
    "The study uses the following four datasets:\n",
    "\n",
    "1. Chitchat\n",
    "2. Topical Chat\n",
    "3. Ubuntu Dialogue\n",
    "4. Enron Email\n",
    "\n",
    "For further details and attribution, see my paper in this github repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbedd4b1-ba5b-464b-ab3f-3f6c8b00c184",
   "metadata": {},
   "source": [
    "### Mechanical Turk Survey (Survey Preparation and Parsing)\n",
    "#### survey1.ipynb\n",
    "\n",
    "The code in tbe first half of the module generates conversations and topic representations to submit to Mechanical Turk. The second half of the module parses the results of the survey and incorporates them into my own hand-labeled results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea1e63-faaf-4ba8-863f-f9d53221aa7a",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "This section loads the required libraries and sets module-wide parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2ccb453-b816-4cff-bbf7-cb279890d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chitchat_dataset as ccc\n",
    "import random\n",
    "from collections import Counter\n",
    "import html\n",
    "import csv\n",
    "import random\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76716e91-a642-4bad-bf2e-b272b7f7ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params\n",
    "capstone_dir = 'C:/Users/micha/Box Sync/cuny/698-Capstone'\n",
    "pickle_dir = 'C:/tmp/pickles'\n",
    "mturk_dir = 'C:/Users/micha/Box Sync/cuny/698-Capstone/mturk/'\n",
    "topn_words = 5\n",
    "nsamples = 10\n",
    "rnd_seed = 77\n",
    "rnd_seed2 = 777\n",
    "num_topic_words = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1477fddc-fc63-4210-aa59-121804c730fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating chitchat dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate chitchat dataset\n",
    "print('instantiating chitchat dataset')\n",
    "ccds = ccc.Dataset()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5f6462d-4e21-4b81-929a-7be6c633f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asdf': 'UserA', 2: 'UserB', 'hop': 'UserC', 'blaaaaaah': 'UserD'}\n"
     ]
    }
   ],
   "source": [
    "# Function to return friendly sender names\n",
    "def friendly_senders(senders):\n",
    "    \"\"\"\n",
    "    Purpose:              To return a list of anonymized conversation participants (e.g. UserA, UserB, UserC, etc.) from an arbitrary list of names.\n",
    "    Parameters:\n",
    "        senders           List of conversation participants to anonymize and reformat.\n",
    "    Returns:\n",
    "        r                 Anonymized list of \"friendly\" participant names.\n",
    "    \"\"\"\n",
    "    fsn_prefix = 'User'\n",
    "    return {sender: f\"{fsn_prefix}{chr(65 + i)}\" for i, sender in enumerate(senders)}\n",
    "\n",
    "# Test suite\n",
    "print(friendly_senders(['asdf', 2, 'hop', 'blaaaaaah']))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667f643-9188-4080-93bf-1eec5e686c3a",
   "metadata": {},
   "source": [
    "### Chitchat dataset\n",
    "\n",
    "Loads the Chitchat dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84bd9c1d-d6e8-4abc-800a-88fc726e6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Chitchat dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Chitchat dataset\n",
    "def load_chitchat(num_docs):\n",
    "\n",
    "    # Init convo list\n",
    "    convs_txt = []\n",
    "\n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "    \n",
    "    # Iterate through items in dataset\n",
    "    ct = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_chats_corpus = []\n",
    "    ct_words = []\n",
    "    for convo_id, convo in ccds.items():\n",
    "\n",
    "        # Conversation header info\n",
    "        #print(convo_id, convo['ratings'], convo['start'], convo['prompt'], '\\n')\n",
    "        ct += 1\n",
    "\n",
    "        # Init conversation text array\n",
    "        convo_msgs = []\n",
    "\n",
    "        # Init sender map (will be UserA, UserB)\n",
    "        sender_map = {}\n",
    "    \n",
    "        # Init \n",
    "        doc_msgs = ''\n",
    "        \n",
    "        # Iterate through messages; each message is from a single person and contain multiple chats, e.g.:\n",
    "        # {\"messages\": [[{\"text\": \"Hello\", \"timestamp\": \"2018-05-02T19:38:15Z\", \"sender\": \"720840be-e522-47ba-9e9f-143f66372673\"}...\n",
    "        ct_msgs = 0\n",
    "        ct_chats = []\n",
    "        for msg in convo['messages']:\n",
    "\n",
    "            # Concatenate all chats within this message\n",
    "            msg_chats = [chat['text'] for chat in msg]\n",
    "            msg_senders = [chat['sender'] for chat in msg]\n",
    "            ct_chat = len(msg_chats)\n",
    "            ct_chats.append(ct_chat)\n",
    "            msg_chats = ' '.join(msg_chats)\n",
    "            doc_msgs += msg_chats + ' '\n",
    "            ct_msgs += 1\n",
    "\n",
    "            # Convo text\n",
    "            convo_msgs.append({'sender': np.unique(msg_senders)[0], 'txt': msg_chats})\n",
    "            #print({'sender': np.unique(msg_senders)[0], 'txt': msg_chats})\n",
    "\n",
    "        # Append count to the overall corpus counts (for stats purposes)\n",
    "        #print(ct_msgs)\n",
    "        #print(ct_chats)\n",
    "        ct_msgs_corpus.append(ct_msgs)\n",
    "        ct_chats_corpus.append(ct_chats)\n",
    "    \n",
    "        # Append to docs list\n",
    "        docs_txt.append(doc_msgs)\n",
    "\n",
    "        # Get friendly sender names\n",
    "        senders = np.unique([e['sender'] for e in convo_msgs])\n",
    "        fsenders = friendly_senders(senders)\n",
    "\n",
    "        # Convo text\n",
    "        conv_txt = ''\n",
    "        for convo_msg in convo_msgs:\n",
    "            conv_txt += f\"{fsenders[convo_msg['sender']]}: {convo_msg['txt']}\\n\\n\"\n",
    "        convs_txt.append(conv_txt)\n",
    "\n",
    "        # Count # of words\n",
    "        ct_words.append(len(re.findall(r'\\W', conv_txt)) + 1)\n",
    "                       \n",
    "        # Show first few docs\n",
    "        \"\"\"\n",
    "        if ct < 6:\n",
    "            print(ct)\n",
    "            print(doc_msgs)\n",
    "            print()\n",
    "        \"\"\"\n",
    "    \n",
    "        # Break early\n",
    "        if num_docs > 0 and ct >= num_docs: break\n",
    "    \n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {len(docs_txt)}')\n",
    "\n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus, ct_chats_corpus, convs_txt, ct_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a873f3e4-9cc2-459d-9644-de1e2462283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs (conversations): 7168\n",
      "(7168, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "      <th>conv</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello How are you doing today? whats up MD im ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 2, ...</td>\n",
       "      <td>UserB: Hello How are you doing today?\\n\\nUserA...</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hi anyone here hey whats up yeah how are you i...</td>\n",
       "      <td>57</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, ...</td>\n",
       "      <td>UserA: hi anyone here\\n\\nUserB: hey whats up y...</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey! Hey I'm gonna close the other window if t...</td>\n",
       "      <td>18</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1, 2, ...</td>\n",
       "      <td>UserB: Hey!\\n\\nUserA: Hey I'm gonna close the ...</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I don't know what falafel is In fact I don't e...</td>\n",
       "      <td>25</td>\n",
       "      <td>[7, 2, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 4, ...</td>\n",
       "      <td>UserA: I don't know what falafel is In fact I ...</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Helllo!!! Hello!  I think this program is bugg...</td>\n",
       "      <td>140</td>\n",
       "      <td>[1, 1, 3, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 1, ...</td>\n",
       "      <td>UserA: Helllo!!!\\n\\nUserB: Hello!  I think thi...</td>\n",
       "      <td>3886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                txt  msg_ct  \\\n",
       "0      0  Hello How are you doing today? whats up MD im ...      35   \n",
       "1      1  hi anyone here hey whats up yeah how are you i...      57   \n",
       "2      2  Hey! Hey I'm gonna close the other window if t...      18   \n",
       "3      3  I don't know what falafel is In fact I don't e...      25   \n",
       "4      4  Helllo!!! Hello!  I think this program is bugg...     140   \n",
       "\n",
       "                                             chat_ct  \\\n",
       "0  [2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 2, ...   \n",
       "1  [2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1, 2, ...   \n",
       "3  [7, 2, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 4, ...   \n",
       "4  [1, 1, 3, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 1, ...   \n",
       "\n",
       "                                                conv  words  \n",
       "0  UserB: Hello How are you doing today?\\n\\nUserA...    661  \n",
       "1  UserA: hi anyone here\\n\\nUserB: hey whats up y...    835  \n",
       "2  UserB: Hey!\\n\\nUserA: Hey I'm gonna close the ...    458  \n",
       "3  UserA: I don't know what falafel is In fact I ...    488  \n",
       "4  UserA: Helllo!!!\\n\\nUserB: Hello!  I think thi...   3886  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load chitchat dataset\n",
    "cc_txt, cc_msg_ct, cc_chat_ct, cc_conv, cc_words = load_chitchat(0)\n",
    "\n",
    "# Make dataframe\n",
    "dfcc = pd.DataFrame({'txt': cc_txt, 'msg_ct': cc_msg_ct, 'chat_ct': cc_chat_ct, 'conv': cc_conv, 'words': cc_words})\n",
    "print(dfcc.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dfcc = dfcc[dfcc['msg_ct'] > 5].reset_index()\n",
    "dfcc['index'] = dfcc.index\n",
    "display(dfcc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a71dda-fd7a-4b14-8c06-240fc1a01bb4",
   "metadata": {},
   "source": [
    "### Topical Chat dataset\n",
    "\n",
    "Loads the Topical Chat dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "010b05b3-c3a7-4b07-b42f-d397f2591498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Topical Chat dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Topical Chat dataset\n",
    "def load_topical(num_docs):\n",
    "\n",
    "    # Path\n",
    "    path_to_docs = 'C:/Users/micha/Documents/698/corpora/topical_chat/train.json'\n",
    "    \n",
    "    # Init convo list\n",
    "    convs_txt = []\n",
    "\n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "\n",
    "    # Load file\n",
    "    with open(path_to_docs, 'r', encoding='latin-1') as fh:\n",
    "\n",
    "        j = json.load(fh)\n",
    "\n",
    "    # Iterate through items in dataset\n",
    "    ct = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_chats_corpus = []\n",
    "    ct_words = []\n",
    "    for k in j.keys():\n",
    "\n",
    "        ct += 1\n",
    "\n",
    "        # Get conversation\n",
    "        conv = j[k]['content']\n",
    "\n",
    "        # Init \n",
    "        doc_msgs = ''\n",
    "        conv_txt = ''\n",
    "        \n",
    "        # Iterate over each message in the conversation; each message is by one person and can contain multiple sentences\n",
    "        ct_msgs = 0\n",
    "        for msg in conv:\n",
    "\n",
    "            # Concatenate all chats within this message\n",
    "            msg_txt = msg['message']\n",
    "            sender = msg['agent']\n",
    "            doc_msgs += msg_txt + ' '\n",
    "            conv_txt += f\"{sender}: {msg_txt}\\n\\n\"\n",
    "            ct_msgs += 1\n",
    "\n",
    "        # Append count to the overall corpus counts (for stats purposes)\n",
    "        ct_msgs_corpus.append(ct_msgs)\n",
    "    \n",
    "        # Append to docs list\n",
    "        docs_txt.append(doc_msgs)\n",
    "\n",
    "        # Convos\n",
    "        convs_txt.append(conv_txt)\n",
    "    \n",
    "        # Count # of words\n",
    "        ct_words.append(len(re.findall(r'\\W', conv_txt)) + 1)\n",
    "                       \n",
    "        # Break early\n",
    "        if num_docs > 0 and ct >= num_docs: break\n",
    "    \n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {len(docs_txt)}')\n",
    "\n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus, convs_txt, ct_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "332ee87e-1439-4229-8c77-4037ebcb7ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs (conversations): 3000\n",
      "(3000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "      <th>conv</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Are you a fan of Google or Microsoft? Both are...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>agent_1: Are you a fan of Google or Microsoft?...</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>do you like dance? Yes  I do. Did you know Bru...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>agent_1: do you like dance?\\n\\nagent_2: Yes  I...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey what's up do use Google very often?I reall...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>agent_1: Hey what's up do use Google very ofte...</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi!  do you like to dance? I love to dance a l...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>agent_1: Hi!  do you like to dance?\\n\\nagent_2...</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>do you like dance? I love it. Did you know Bru...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>agent_1: do you like dance?\\n\\nagent_2: I love...</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                txt  msg_ct  chat_ct  \\\n",
       "0      0  Are you a fan of Google or Microsoft? Both are...      21        1   \n",
       "1      1  do you like dance? Yes  I do. Did you know Bru...      21        1   \n",
       "2      2  Hey what's up do use Google very often?I reall...      21        1   \n",
       "3      3  Hi!  do you like to dance? I love to dance a l...      23        1   \n",
       "4      4  do you like dance? I love it. Did you know Bru...      21        1   \n",
       "\n",
       "                                                conv  words  \n",
       "0  agent_1: Are you a fan of Google or Microsoft?...    493  \n",
       "1  agent_1: do you like dance?\\n\\nagent_2: Yes  I...    300  \n",
       "2  agent_1: Hey what's up do use Google very ofte...    502  \n",
       "3  agent_1: Hi!  do you like to dance?\\n\\nagent_2...    471  \n",
       "4  agent_1: do you like dance?\\n\\nagent_2: I love...    325  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load topical chat dataset\n",
    "tc_txt, tc_msg_ct, tc_conv, tc_words = load_topical(3000)\n",
    "\n",
    "# Make dataframe\n",
    "dftc = pd.DataFrame({'txt': tc_txt, 'msg_ct': tc_msg_ct, 'chat_ct': 1, 'conv': tc_conv, 'words': tc_words})\n",
    "print(dftc.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dftc = dftc[dftc['msg_ct'] > 5].reset_index()\n",
    "display(dftc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d945b-52b5-4c3d-b0ce-e73fad7861b5",
   "metadata": {},
   "source": [
    "### Ubuntu Dialogue dataset\n",
    "\n",
    "Loads the Ubuntu Dialogue dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d40eea7-99fa-4185-835a-7861c9f97a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Ubuntu Dialogue dataset\n",
    "#############################################\n",
    "\n",
    "# Function to load the Ubuntu Dialogue dataset\n",
    "def load_ubuntu(num_docs):\n",
    "\n",
    "    # Path\n",
    "    path_to_docs = 'C:/Users/micha/Documents/698/corpora/ubuntu_dialogues/dialogs'\n",
    "    \n",
    "    # Init document list\n",
    "    docs_txt = []\n",
    "\n",
    "    # Init convo list\n",
    "    convs_txt = []\n",
    "\n",
    "    # Iterate over directories\n",
    "    ct_conv = 0\n",
    "    ct_err = 0\n",
    "    ct_msgs_corpus = []\n",
    "    ct_words = []\n",
    "    for d in os.listdir(path_to_docs):\n",
    "\n",
    "        # Iterate over files in directory\n",
    "        print('dir', d)\n",
    "        for f in os.listdir(path_to_docs + '/' + d):\n",
    "\n",
    "            # Verify it's a file\n",
    "            fn = path_to_docs + '/' + d + '/' + f\n",
    "            if os.path.isfile(fn):\n",
    "\n",
    "                # Init \n",
    "                doc_msgs = ''\n",
    "                conv_txt = ''\n",
    "        \n",
    "                # Read the file\n",
    "                with open(fn, 'r', encoding='latin-1') as fh:\n",
    "\n",
    "                    # Reach each line; each line is a separate message from a single user\n",
    "                    ct_conv += 1\n",
    "                    ct_msgs = 0\n",
    "                    while True:\n",
    "\n",
    "                        # Read line\n",
    "                        l = fh.readline()\n",
    "                        if not l:\n",
    "                            break\n",
    "\n",
    "                        # Split to an array; each line will be in this format: timestamp[tab]sender[tab]receiver[tab]message\n",
    "                        # e.g.: 2005-05-26T16:54:00.000Z[tab]lifeless[tab]we2by[tab]calm down please\n",
    "                        tmp = l.strip().split('\\t')\n",
    "                        if len(tmp) == 4:\n",
    "\n",
    "                            # Concatenate to all the messages in the conversation\n",
    "                            sender = tmp[1]\n",
    "                            doc_msgs += tmp[3] + ' '\n",
    "                            conv_txt += f\"{sender}: {tmp[3]}\\n\\n\"\n",
    "                            ct_msgs += 1\n",
    "                            \n",
    "                        else:\n",
    "                            # Not the right number of fields in this message\n",
    "                            ct_err += 1\n",
    "                            \n",
    "                # Append count to the overall corpus counts (for stats purposes)\n",
    "                ct_msgs_corpus.append(ct_msgs)\n",
    "\n",
    "                # Convos\n",
    "                convs_txt.append(conv_txt)\n",
    "\n",
    "                # Append to docs list\n",
    "                docs_txt.append(doc_msgs)\n",
    "\n",
    "                # Count # of words\n",
    "                ct_words.append(len(re.findall(r'\\W', conv_txt)) + 1)\n",
    "                               \n",
    "            # Break early\n",
    "            if ct_conv % 1000 == 0: print(ct_conv)\n",
    "            if num_docs > 0 and ct_conv >= num_docs:\n",
    "                break\n",
    "                \n",
    "        # Break early\n",
    "        if ct_conv % 1000 == 0: print(ct_conv)\n",
    "        if num_docs > 0 and ct_conv >= num_docs:\n",
    "            break\n",
    "\n",
    "    # Doc summary\n",
    "    print(f'Number of docs (conversations): {ct_conv}')\n",
    "    print(f'Number of errs: {ct_err}')\n",
    "    \n",
    "    # Return\n",
    "    return docs_txt, ct_msgs_corpus, convs_txt, ct_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67cc06ed-e32a-4f76-9579-7119006d8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 10\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "10000\n",
      "Number of docs (conversations): 10000\n",
      "Number of errs: 12\n",
      "(10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "      <th>conv</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hi sudo echo Y &gt; /sys/module/usbcore/parameter...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>we2by: hi\\n\\nwe2by: sudo echo Y &gt; /sys/module/...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hmm Why doesn't GLX work with X.Org (I just ch...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>rapha: Hmm\\n\\nrapha: Why doesn't GLX work with...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hi can someone tell me where shell prompt name...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>bitnumus: hi can someone tell me where shell p...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How do I boot in safe mode with 12.04?  you me...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>MorganJarl: How do I boot in safe mode with 12...</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hello, I have a minimal linux system: how can ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>fk91: Hello, I have a minimal linux system: ho...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                txt  msg_ct  chat_ct  \\\n",
       "0      0  hi sudo echo Y > /sys/module/usbcore/parameter...      10        1   \n",
       "1      1  Hmm Why doesn't GLX work with X.Org (I just ch...      10        1   \n",
       "2      2  hi can someone tell me where shell prompt name...      10        1   \n",
       "3      3  How do I boot in safe mode with 12.04?  you me...      10        1   \n",
       "4      4  Hello, I have a minimal linux system: how can ...      10        1   \n",
       "\n",
       "                                                conv  words  \n",
       "0  we2by: hi\\n\\nwe2by: sudo echo Y > /sys/module/...     92  \n",
       "1  rapha: Hmm\\n\\nrapha: Why doesn't GLX work with...    145  \n",
       "2  bitnumus: hi can someone tell me where shell p...    267  \n",
       "3  MorganJarl: How do I boot in safe mode with 12...    271  \n",
       "4  fk91: Hello, I have a minimal linux system: ho...    107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Ubuntu Dialogue data set\n",
    "ud_txt, ud_msg_ct, ud_conv, ud_words = load_ubuntu(10000)\n",
    "\n",
    "# Make dataframe\n",
    "dfud = pd.DataFrame({'txt': ud_txt, 'msg_ct': ud_msg_ct, 'chat_ct': 1, 'conv': ud_conv, 'words': ud_words})\n",
    "print(dfud.shape)\n",
    "\n",
    "# Only take conversations with more than 5 exchanges\n",
    "dfud = dfud[dfud['msg_ct'] > 5].reset_index()\n",
    "display(dfud.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b05b09-c5af-40f3-9c66-2e5149d52e7c",
   "metadata": {},
   "source": [
    "### Enron Email dataset\n",
    "\n",
    "Loads the Enron Email dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7866a1bc-0eb1-443b-8ab2-512bde946ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Enron Email dataset\n",
    "#############################################\n",
    "\n",
    "# Preload enron emails into dataframe\n",
    "path_to_emails = 'C:/Users/micha/Documents/698/corpora/enron_emails/emails.csv'\n",
    "dfee = pd.read_csv(path_to_emails)\n",
    "dfee.drop(columns=['file'], inplace=True)\n",
    "dfee['msg_ct'] = 1  # just assume each email = 1 message = 1 conversation\n",
    "dfee['chat_ct'] = 1\n",
    "dfee.rename(columns={'message': 'txt'}, inplace=True)\n",
    "dfee = dfee[:5000]\n",
    "dfee['index'] = dfee.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "090feeec-9595-4767-b673-f18d939ad654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12APR HOUSTON TO NEW YORK = JENNIFER WHITE = TICKETED\n",
      "\n",
      "---------------------- Forwarded by John Arnold/HOU/ECT on 04/04/2001 10:40 \n",
      "PM ---------------------------\n",
      "\n",
      "\n",
      "sandra delgado <sdelgado_vitoltvl@yahoo.com> on 03/30/2001 04:27:11 PM\n",
      "To: JOHN.ARNOLD@ENRON.COM\n",
      "cc:  \n",
      "Subject: 12APR HOUSTON TO NEW YORK = JENNIFER WHITE = TICKETED\n",
      "\n",
      "\n",
      "                                          AGENT SS/SS BOOKING REF\n",
      "YFRJLU\n",
      "\n",
      "                                          WHITE/JENNIFER\n",
      "\n",
      "\n",
      "  ENRON\n",
      "  1400 SMITH\n",
      "  HOUSTON TX 77002\n",
      "  ATTN: JOHN ARNOLD\n",
      "\n",
      "\n",
      "  DATE:  MAR 30 2001                   ENRON\n",
      "\n",
      "SERVICE               DATE  FROM           TO             DEPART\n",
      "ARRIVE\n",
      "\n",
      "CONTINENTAL AIRLINES  12APR HOUSTON TX     NEW YORK NY    335P    817P\n",
      "CO 1700    V          THU   G.BUSH INTERCO LA GUARDIA\n",
      "                            TERMINAL C     TERMINAL M\n",
      "                            SNACK                         NON STOP\n",
      "                            RESERVATION CONFIRMED         3:42 DURATION\n",
      "                  AIRCRAFT: BOEING 737-300\n",
      "                            SEAT 14E NO SMOKING CONFIRMED\n",
      "WHITE/JENNIFER\n",
      "\n",
      "CONTINENTAL AIRLINES  15APR NEWARK NJ      HOUSTON TX     1100A   139P\n",
      "CO 209     Q          SUN   NEWARK INTL    G.BUSH INTERCO\n",
      "                            TERMINAL C     TERMINAL C\n",
      "                            SNACK                         NON STOP\n",
      "                            RESERVATION CONFIRMED         3:39 DURATION\n",
      "                  AIRCRAFT: MCDONNELL DOUGLAS DC-10 ALL SERIES\n",
      "                            SEAT 29L NO SMOKING CONFIRMED\n",
      "WHITE/JENNIFER\n",
      "\n",
      "      AIR FARE 248.37       TAX 27.13           TOTAL USD\n",
      "275.50\n",
      "\n",
      "                                        INVOICE TOTAL USD\n",
      "275.50\n",
      "\n",
      "PAYMENT: CCVI4128003323411978/0801/A234211\n",
      "\n",
      "RESERVATION NUMBER(S)  CO/OMMLDH\n",
      "\n",
      "WHITE/JENNIFER                           TICKET:CO/ETKT 005 7026661562\n",
      "\n",
      "**CONTINENTAL RECORD LOCATOR: OMMLDH\n",
      "THIS IS A TICKETLESS RESERVATION. PLEASE HAVE A\n",
      "PICTURE ID AVAILABLE AT THE AIRPORT. THANK YOU\n",
      "**********************************************\n",
      "NON-REFUNDABLE TKT MINIMUM $100.00 CHANGE FEE\n",
      "                  THANK YOU FOR CALLING VITOL TRAVEL\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "Do You Yahoo!?\n",
      "Get email at your own domain with Yahoo! Mail.\n",
      "http://personal.mail.yahoo.com/?.refer=text\n",
      "***************************************************************\n",
      "***************************************************************\n",
      "***************************************************************\n",
      "Message-ID: <22056085.1075857652431.JavaMail.evans@thyme>\n",
      "Date: Wed, 4 Apr 2001 15:43:00 -0700 (PDT)\n",
      "From: john.arnold@enron.com\n",
      "To: jenwhite7@zdnetmail.com\n",
      "Subject: 12APR HOUSTON TO NEW YORK = JENNIFER WHITE = TICKETED\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: John Arnold\n",
      "X-To: jenwhite7@zdnetmail.com\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\John_Arnold_Jun2001\\Notes Folders\\'sent mail\n",
      "X-Origin: Arnold-J\n",
      "X-FileName: Jarnold.nsf\n",
      "\n",
      "---------------------- Forwarded by John Arnold/HOU/ECT on 04/04/2001 10:40 \n",
      "PM ---------------------------\n",
      "\n",
      "\n",
      "sandra delgado <sdelgado_vitoltvl@yahoo.com> on 03/30/2001 04:27:11 PM\n",
      "To: JOHN.ARNOLD@ENRON.COM\n",
      "cc:  \n",
      "Subject: 12APR HOUSTON TO NEW YORK = JENNIFER WHITE = TICKETED\n",
      "\n",
      "\n",
      "                                          AGENT SS/SS BOOKING REF\n",
      "YFRJLU\n",
      "\n",
      "                                          WHITE/JENNIFER\n",
      "\n",
      "\n",
      "  ENRON\n",
      "  1400 SMITH\n",
      "  HOUSTON TX 77002\n",
      "  ATTN: JOHN ARNOLD\n",
      "\n",
      "\n",
      "  DATE:  MAR 30 2001                   ENRON\n",
      "\n",
      "SERVICE               DATE  FROM           TO             DEPART\n",
      "ARRIVE\n",
      "\n",
      "CONTINENTAL AIRLINES  12APR HOUSTON TX     NEW YORK NY    335P    817P\n",
      "CO 1700    V          THU   G.BUSH INTERCO LA GUARDIA\n",
      "                            TERMINAL C     TERMINAL M\n",
      "                            SNACK                         NON STOP\n",
      "                            RESERVATION CONFIRMED         3:42 DURATION\n",
      "                  AIRCRAFT: BOEING 737-300\n",
      "                            SEAT 14E NO SMOKING CONFIRMED\n",
      "WHITE/JENNIFER\n",
      "\n",
      "CONTINENTAL AIRLINES  15APR NEWARK NJ      HOUSTON TX     1100A   139P\n",
      "CO 209     Q          SUN   NEWARK INTL    G.BUSH INTERCO\n",
      "                            TERMINAL C     TERMINAL C\n",
      "                            SNACK                         NON STOP\n",
      "                            RESERVATION CONFIRMED         3:39 DURATION\n",
      "                  AIRCRAFT: MCDONNELL DOUGLAS DC-10 ALL SERIES\n",
      "                            SEAT 29L NO SMOKING CONFIRMED\n",
      "WHITE/JENNIFER\n",
      "\n",
      "      AIR FARE 248.37       TAX 27.13           TOTAL USD\n",
      "275.50\n",
      "\n",
      "                                        INVOICE TOTAL USD\n",
      "275.50\n",
      "\n",
      "PAYMENT: CCVI4128003323411978/0801/A234211\n",
      "\n",
      "RESERVATION NUMBER(S)  CO/OMMLDH\n",
      "\n",
      "WHITE/JENNIFER                           TICKET:CO/ETKT 005 7026661562\n",
      "\n",
      "**CONTINENTAL RECORD LOCATOR: OMMLDH\n",
      "THIS IS A TICKETLESS RESERVATION. PLEASE HAVE A\n",
      "PICTURE ID AVAILABLE AT THE AIRPORT. THANK YOU\n",
      "**********************************************\n",
      "NON-REFUNDABLE TKT MINIMUM $100.00 CHANGE FEE\n",
      "                  THANK YOU FOR CALLING VITOL TRAVEL\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "Do You Yahoo!?\n",
      "Get email at your own domain with Yahoo! Mail.\n",
      "http://personal.mail.yahoo.com/?.refer=text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to remove header info from Enron emails\n",
    "def proc_email(msg):\n",
    "    \"\"\"\n",
    "    Purpose:             To remove email headers from Enron emails.\n",
    "    Parameters:\n",
    "        num_docs         The text of the message.\n",
    "    Returns:\n",
    "        r                The text of the message with header information stripped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init processed msg\n",
    "    r = ''\n",
    "\n",
    "    # Find subject\n",
    "    subj = ''\n",
    "    i = msg.find('Subject: ')\n",
    "    if i > -1:\n",
    "        j = msg.find('\\n', i)\n",
    "        if j > -1:\n",
    "            subj = msg[i + len('Subject: '):j]\n",
    "\n",
    "    # Strip off 'Re:' and 'Fwd:'\n",
    "    while True:\n",
    "        if subj[0:3].lower() == 're:':\n",
    "            subj = subj[3:]\n",
    "        elif subj[0:4].lower() == 'fwd:':\n",
    "            subj = subj[4:]\n",
    "        else:\n",
    "            break\n",
    "    subj = subj.strip()\n",
    "\n",
    "    # Find the first double \\n; this should be the start of the message\n",
    "    i = msg.find('\\n\\n')\n",
    "    r = subj + '\\n\\n'\n",
    "    if i > -1:\n",
    "        r += msg[i + 2:]\n",
    "\n",
    "    # Return\n",
    "    return r.strip()\n",
    "\n",
    "# Test suite\n",
    "msg_before = dfee.loc[dfee.index == random.randint(0, 5000), 'txt'].values[0]\n",
    "msg_after = proc_email(msg_before)\n",
    "print(msg_after)\n",
    "print('***************************************************************')\n",
    "print('***************************************************************')\n",
    "print('***************************************************************')\n",
    "print(msg_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26dfdc48-d30d-4b7e-aacf-16d8694f774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header info from Enron emails\n",
    "dfee['txt'] = dfee['txt'].apply(proc_email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b7478b7-fe1b-4931-b0f1-a545baf9353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>msg_ct</th>\n",
       "      <th>chat_ct</th>\n",
       "      <th>index</th>\n",
       "      <th>conv</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test\\n\\ntest successful.  way to go!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>test\\n\\ntest successful.  way to go!!!</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello\\n\\nLet's shoot for Tuesday at 11:45.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Hello\\n\\nLet's shoot for Tuesday at 11:45.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  msg_ct  chat_ct  index  \\\n",
       "0                               Here is our forecast       1        1      0   \n",
       "1  Traveling to have a business meeting takes the...       1        1      1   \n",
       "2             test\\n\\ntest successful.  way to go!!!       1        1      2   \n",
       "3  Randy,\\n\\n Can you send me a schedule of the s...       1        1      3   \n",
       "4         Hello\\n\\nLet's shoot for Tuesday at 11:45.       1        1      4   \n",
       "\n",
       "                                                conv  words  \n",
       "0                               Here is our forecast      4  \n",
       "1  Traveling to have a business meeting takes the...    163  \n",
       "2             test\\n\\ntest successful.  way to go!!!     12  \n",
       "3  Randy,\\n\\n Can you send me a schedule of the s...     46  \n",
       "4         Hello\\n\\nLet's shoot for Tuesday at 11:45.     11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set conversation field\n",
    "dfee['conv'] = dfee['txt']\n",
    "\n",
    "# Count words in each doc\n",
    "dfee['words'] = dfee['conv'].apply(lambda x: len(re.findall(r'\\W', x)) + 1)\n",
    "\n",
    "# Display\n",
    "print(dfee.shape)\n",
    "display(dfee.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b54348-32e8-4ddd-ba6a-96c6b4ab8e58",
   "metadata": {},
   "source": [
    "### Prepare Survey Questions\n",
    "\n",
    "This section prepares survey questions by exploring candidate conversations and topic representations to include.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2ecdfe8-616d-4f61-887e-b84d0189e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the last pickle file of the specified type (bow, word2vec, or transformer)\n",
    "def load_last_pickle(model_type):\n",
    "\n",
    "    d = os.listdir(f'{pickle_dir}/{model_type}')\n",
    "    last_pickle = sorted(d, reverse=True)[0]\n",
    "    dftmp = pd.read_pickle(f'{pickle_dir}/{model_type}/{last_pickle}')\n",
    "    return dftmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b865662a-8463-4d6c-b850-d99f51fe50dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chitchat': {0: 1035, 1: 1334, 2: 808, 3: 985, 4: 791, 5: 471, 6: 1197, 7: 1950, 8: 2285, 9: 2504, 10: 974, 11: 595, 12: 2276, 13: 2583, 14: 2707, 15: 2050, 16: 9, 17: 1146, 18: 116, 19: 2036, 20: 707, 21: 783, 22: 2077, 23: 1591, 24: 749, 25: 1357, 26: 1316, 27: 480, 28: 357, 29: 2256, 30: 1066, 31: 2187, 32: 1923, 33: 839, 34: 924, 35: 785, 36: 294, 37: 331, 38: 819, 39: 2855, 40: 2294, 41: 1792, 42: 2438, 43: 2884, 44: 2206, 45: 312, 46: 408, 47: 2897, 48: 868, 49: 1347, 50: 359, 51: 388, 52: 2362, 53: 1889, 54: 324, 55: 2203, 56: 436, 57: 1113, 58: 1821, 59: 2568, 60: 2261, 61: 497, 62: 1439, 63: 2169, 64: 230, 65: 1655, 66: 2385, 67: 2480, 68: 2174, 69: 1015, 70: 897, 71: 2345, 72: 1934, 73: 1649, 74: 566, 75: 1574, 76: 52, 77: 143, 78: 213, 79: 2030, 80: 2590, 81: 161, 82: 1466, 83: 1772, 84: 482, 85: 1048, 86: 2064, 87: 2305, 88: 2819, 89: 2002, 90: 229, 91: 2665, 92: 1931, 93: 2834, 94: 2680, 95: 389, 96: 603, 97: 1221, 98: 2175, 99: 376, 100: 70, 101: 1988, 102: 1445, 103: 1701, 104: 2357, 105: 2, 106: 254, 107: 1971, 108: 599, 109: 109, 110: 2871, 111: 33, 112: 982, 113: 361, 114: 2730, 115: 1909, 116: 464, 117: 1598, 118: 1949, 119: 422, 120: 555, 121: 980, 122: 1071, 123: 859, 124: 352, 125: 661, 126: 672, 127: 610, 128: 102, 129: 206, 130: 955, 131: 2746, 132: 2054, 133: 1553, 134: 2361, 135: 2916, 136: 2462, 137: 1709, 138: 2005, 139: 2074, 140: 1096, 141: 2450, 142: 2546, 143: 1699, 144: 1208, 145: 2215, 146: 1131, 147: 1178, 148: 1858, 149: 774, 150: 74, 151: 2424, 152: 1992, 153: 289, 154: 146, 155: 77, 156: 578, 157: 36, 158: 368, 159: 2697, 160: 447, 161: 2090, 162: 835, 163: 2780, 164: 340, 165: 1741, 166: 677, 167: 444, 168: 2720, 169: 1162, 170: 1885, 171: 2000, 172: 2181, 173: 2847, 174: 1647, 175: 794, 176: 2662, 177: 1753, 178: 1028, 179: 1161, 180: 2144, 181: 1324, 182: 1290, 183: 793, 184: 1151, 185: 1508, 186: 2789, 187: 1958, 188: 2684, 189: 2148, 190: 1613, 191: 2065, 192: 2189, 193: 1640, 194: 1044, 195: 1550, 196: 2301, 197: 2895, 198: 2698, 199: 1266, 200: 323, 201: 1123, 202: 2765, 203: 594, 204: 563, 205: 1472, 206: 2850, 207: 200, 208: 2411, 209: 2552, 210: 2297, 211: 2478, 212: 2773, 213: 2374, 214: 233, 215: 2161, 216: 90, 217: 2951, 218: 2402, 219: 491, 220: 2619, 221: 2833, 222: 523, 223: 1109, 224: 659, 225: 2673, 226: 1405, 227: 247, 228: 2776, 229: 2112, 230: 2555, 231: 2387, 232: 1543, 233: 354, 234: 2876, 235: 1230, 236: 322, 237: 2650, 238: 2540, 239: 20, 240: 193, 241: 1842, 242: 609, 243: 2029, 244: 1657, 245: 2183, 246: 314, 247: 1760, 248: 18, 249: 448}, 'topical chat': {0: 1035, 1: 1334, 2: 808, 3: 985, 4: 791, 5: 471, 6: 1197, 7: 1950, 8: 2285, 9: 2504, 10: 974, 11: 595, 12: 2276, 13: 2583, 14: 2707, 15: 2050, 16: 9, 17: 1146, 18: 116, 19: 2036, 20: 707, 21: 783, 22: 2077, 23: 1591, 24: 749, 25: 1357, 26: 1316, 27: 480, 28: 357, 29: 2256, 30: 1066, 31: 2187, 32: 1923, 33: 839, 34: 924, 35: 785, 36: 294, 37: 331, 38: 819, 39: 2855, 40: 2294, 41: 1792, 42: 2438, 43: 2884, 44: 2206, 45: 312, 46: 408, 47: 2897, 48: 868, 49: 1347, 50: 359, 51: 388, 52: 2362, 53: 1889, 54: 324, 55: 2203, 56: 436, 57: 1113, 58: 1821, 59: 2568, 60: 2261, 61: 497, 62: 1439, 63: 2169, 64: 230, 65: 1655, 66: 2385, 67: 2480, 68: 2174, 69: 1015, 70: 897, 71: 2345, 72: 1934, 73: 1649, 74: 566, 75: 1574, 76: 52, 77: 143, 78: 213, 79: 2030, 80: 2590, 81: 161, 82: 1466, 83: 1772, 84: 482, 85: 1048, 86: 2064, 87: 2305, 88: 2819, 89: 2002, 90: 229, 91: 2665, 92: 1931, 93: 2834, 94: 2680, 95: 389, 96: 603, 97: 1221, 98: 2175, 99: 376, 100: 70, 101: 1988, 102: 1445, 103: 1701, 104: 2357, 105: 2, 106: 254, 107: 1971, 108: 599, 109: 109, 110: 2871, 111: 33, 112: 982, 113: 361, 114: 2730, 115: 1909, 116: 464, 117: 1598, 118: 1949, 119: 422, 120: 555, 121: 980, 122: 1071, 123: 859, 124: 352, 125: 661, 126: 672, 127: 610, 128: 102, 129: 206, 130: 955, 131: 2746, 132: 2054, 133: 1553, 134: 2361, 135: 2916, 136: 2999, 137: 2462, 138: 1709, 139: 2005, 140: 2074, 141: 1096, 142: 2450, 143: 2546, 144: 1699, 145: 1208, 146: 2215, 147: 1131, 148: 1178, 149: 1858, 150: 774, 151: 74, 152: 2424, 153: 1992, 154: 289, 155: 146, 156: 77, 157: 578, 158: 36, 159: 368, 160: 2697, 161: 447, 162: 2090, 163: 835, 164: 2780, 165: 340, 166: 1741, 167: 677, 168: 444, 169: 2720, 170: 1162, 171: 1885, 172: 2000, 173: 2181, 174: 2847, 175: 1647, 176: 794, 177: 2662, 178: 1753, 179: 1028, 180: 1161, 181: 2144, 182: 1324, 183: 1290, 184: 793, 185: 1151, 186: 1508, 187: 2968, 188: 2789, 189: 1958, 190: 2684, 191: 2148, 192: 1613, 193: 2065, 194: 2189, 195: 1640, 196: 1044, 197: 1550, 198: 2301, 199: 2895, 200: 2698, 201: 1266, 202: 323, 203: 1123, 204: 2765, 205: 594, 206: 563, 207: 1472, 208: 2850, 209: 200, 210: 2411, 211: 2552, 212: 2297, 213: 2478, 214: 2773, 215: 2374, 216: 233, 217: 2161, 218: 90, 219: 2951, 220: 2402, 221: 491, 222: 2619, 223: 2833, 224: 523, 225: 1109, 226: 659, 227: 2673, 228: 1405, 229: 247, 230: 2776, 231: 2112, 232: 2555, 233: 2387, 234: 1543, 235: 354, 236: 2876, 237: 1230, 238: 322, 239: 2650, 240: 2540, 241: 20, 242: 193, 243: 1842, 244: 609, 245: 2029, 246: 1657, 247: 2183, 248: 314, 249: 1760}, 'ubuntu dialogue': {0: 4140, 1: 5339, 2: 3232, 3: 3940, 4: 3164, 5: 1885, 6: 4789, 7: 7802, 8: 9140, 9: 3896, 10: 2383, 11: 9107, 12: 8202, 13: 39, 14: 4586, 15: 464, 16: 8145, 17: 2829, 18: 3133, 19: 8311, 20: 6366, 21: 2996, 22: 5431, 23: 5267, 24: 1921, 25: 1428, 26: 9027, 27: 4265, 28: 8748, 29: 7694, 30: 3358, 31: 3696, 32: 3140, 33: 1177, 34: 1326, 35: 3278, 36: 9176, 37: 7170, 38: 9753, 39: 8826, 40: 1250, 41: 1635, 42: 3474, 43: 5388, 44: 1437, 45: 1555, 46: 9451, 47: 7556, 48: 1299, 49: 8814, 50: 1744, 51: 4455, 52: 7286, 53: 9044, 54: 1990, 55: 5757, 56: 8678, 57: 923, 58: 6620, 59: 9543, 60: 9922, 61: 8699, 62: 4060, 63: 3590, 64: 9382, 65: 7737, 66: 6596, 67: 2266, 68: 6296, 69: 209, 70: 575, 71: 855, 72: 8123, 73: 646, 74: 5866, 75: 7091, 76: 1931, 77: 4194, 78: 8257, 79: 9221, 80: 8009, 81: 916, 82: 7725, 83: 1559, 84: 2415, 85: 4886, 86: 8702, 87: 1504, 88: 281, 89: 7954, 90: 5782, 91: 6807, 92: 9430, 93: 11, 94: 1019, 95: 7887, 96: 2399, 97: 437, 98: 134, 99: 3930, 100: 1444, 101: 7639, 102: 1858, 103: 6393, 104: 7797, 105: 1689, 106: 2222, 107: 3921, 108: 4284, 109: 3439, 110: 1408, 111: 2646, 112: 2691, 113: 2443, 114: 411, 115: 827, 116: 3823, 117: 8216, 118: 6212, 119: 9446, 120: 9848, 121: 6837, 122: 8020, 123: 8296, 124: 4387, 125: 9801, 126: 1688, 127: 7724, 128: 6799, 129: 4833, 130: 8862, 131: 4526, 132: 4712, 133: 7434, 134: 3098, 135: 299, 136: 9696, 137: 7970, 138: 1159, 139: 584, 140: 308, 141: 2315, 142: 144, 143: 1473, 144: 1790, 145: 8362, 146: 3342, 147: 1361, 148: 6965, 149: 2709, 150: 1777, 151: 4649, 152: 7540, 153: 8000, 154: 8724, 155: 6590, 156: 3178, 157: 7014, 158: 4112, 159: 4645, 160: 1411, 161: 8578, 162: 5299, 163: 5161, 164: 3175, 165: 4604, 166: 6032, 167: 7832, 168: 8592, 169: 6453, 170: 8262, 171: 8757, 172: 6560, 173: 4176, 174: 6201, 175: 9205, 176: 5065, 177: 1292, 178: 4493, 179: 2376, 180: 2253, 181: 5891, 182: 802, 183: 3475, 184: 9647, 185: 9191, 186: 9914, 187: 9498, 188: 933, 189: 8645, 190: 362, 191: 9609, 192: 1964, 193: 2094, 194: 4439, 195: 2636, 196: 5620, 197: 990, 198: 8448, 199: 9550, 200: 6175, 201: 1419, 202: 4922, 203: 1291, 204: 5890, 205: 7435, 206: 83, 207: 772, 208: 7370, 209: 2438, 210: 8118, 211: 6630, 212: 8734, 213: 1256, 214: 7042, 215: 75, 216: 1795, 217: 515, 218: 6158, 219: 2074, 220: 628, 221: 4287, 222: 9815, 223: 4179, 224: 2858, 225: 8894, 226: 1438, 227: 1142, 228: 738, 229: 9021, 230: 6359, 231: 8382, 232: 1172, 233: 2922, 234: 7891, 235: 8871, 236: 9302, 237: 5283, 238: 5319, 239: 7680, 240: 3762, 241: 3111, 242: 7962, 243: 9460, 244: 7146, 245: 9266, 246: 6334, 247: 9121, 248: 3205, 249: 6584}, 'enron email': {0: 2070, 1: 2669, 2: 1616, 3: 1970, 4: 1582, 5: 942, 6: 2394, 7: 3901, 8: 4570, 9: 1948, 10: 1191, 11: 4553, 12: 4101, 13: 19, 14: 2293, 15: 232, 16: 4072, 17: 1414, 18: 1566, 19: 4155, 20: 3183, 21: 1498, 22: 2715, 23: 2633, 24: 960, 25: 714, 26: 4513, 27: 2132, 28: 4374, 29: 3847, 30: 1679, 31: 1848, 32: 1570, 33: 588, 34: 663, 35: 1639, 36: 4588, 37: 3585, 38: 4876, 39: 4413, 40: 625, 41: 817, 42: 1737, 43: 2694, 44: 718, 45: 777, 46: 4725, 47: 3778, 48: 649, 49: 4407, 50: 872, 51: 2227, 52: 3643, 53: 4522, 54: 995, 55: 2878, 56: 4339, 57: 461, 58: 3310, 59: 4771, 60: 4961, 61: 4349, 62: 2030, 63: 1795, 64: 4691, 65: 3868, 66: 3298, 67: 1133, 68: 3148, 69: 104, 70: 287, 71: 427, 72: 4061, 73: 323, 74: 2933, 75: 3545, 76: 965, 77: 2097, 78: 4128, 79: 4610, 80: 4004, 81: 458, 82: 3862, 83: 779, 84: 1207, 85: 2443, 86: 4351, 87: 752, 88: 140, 89: 3977, 90: 2891, 91: 3403, 92: 4715, 93: 5, 94: 509, 95: 3943, 96: 1199, 97: 218, 98: 67, 99: 1965, 100: 722, 101: 3819, 102: 929, 103: 3196, 104: 3898, 105: 844, 106: 1111, 107: 1960, 108: 2142, 109: 1719, 110: 704, 111: 1323, 112: 1345, 113: 1221, 114: 205, 115: 413, 116: 1911, 117: 4108, 118: 3106, 119: 4723, 120: 4924, 121: 3418, 122: 4010, 123: 4148, 124: 2193, 125: 4900, 126: 3399, 127: 2416, 128: 4431, 129: 2263, 130: 2356, 131: 3717, 132: 1549, 133: 149, 134: 4848, 135: 3985, 136: 579, 137: 292, 138: 154, 139: 1157, 140: 72, 141: 736, 142: 895, 143: 4181, 144: 1671, 145: 680, 146: 3482, 147: 1354, 148: 888, 149: 2324, 150: 3770, 151: 4000, 152: 4362, 153: 3295, 154: 1589, 155: 3507, 156: 2056, 157: 2322, 158: 705, 159: 4289, 160: 2649, 161: 2580, 162: 1587, 163: 2302, 164: 3016, 165: 3916, 166: 4296, 167: 3226, 168: 4131, 169: 4378, 170: 3280, 171: 2088, 172: 3100, 173: 4602, 174: 2532, 175: 646, 176: 2246, 177: 1188, 178: 1126, 179: 2945, 180: 401, 181: 4823, 182: 4595, 183: 4957, 184: 4749, 185: 466, 186: 4322, 187: 181, 188: 4804, 189: 982, 190: 1047, 191: 2219, 192: 1318, 193: 2810, 194: 495, 195: 4224, 196: 4775, 197: 3087, 198: 709, 199: 2461, 200: 645, 201: 41, 202: 386, 203: 3685, 204: 1219, 205: 4059, 206: 3315, 207: 4367, 208: 628, 209: 3521, 210: 37, 211: 897, 212: 257, 213: 3079, 214: 1037, 215: 314, 216: 2143, 217: 4907, 218: 2089, 219: 1429, 220: 4447, 221: 719, 222: 571, 223: 369, 224: 4510, 225: 3179, 226: 4191, 227: 586, 228: 1461, 229: 3945, 230: 4435, 231: 4651, 232: 2641, 233: 2659, 234: 3840, 235: 1881, 236: 1555, 237: 3981, 238: 4730, 239: 3573, 240: 4633, 241: 3167, 242: 4560, 243: 1602, 244: 3292, 245: 789, 246: 1734, 247: 2512, 248: 2709, 249: 2781}}\n"
     ]
    }
   ],
   "source": [
    "# This section creates a document map to map document ID in the result dataframe back to a\n",
    "# document ID in the initial dataset's dataframe.\n",
    "\n",
    "# Init the docmap\n",
    "docmap = {'chitchat': {}, 'topical chat': {}, 'ubuntu dialogue': {}, 'enron email': {}}\n",
    "datasetmap = {'cc': 'chitchat', 'tc': 'topical chat', 'ud': 'ubuntu dialogue', 'ee': 'enron email'}\n",
    "\n",
    "# Iterate through result dataframe\n",
    "for dataset in docmap.keys():\n",
    "\n",
    "    # Get list of document indices\n",
    "    if dataset == 'chitchat':\n",
    "        alldocs_i = list(dfcc.index)\n",
    "    elif dataset == 'topical chat':\n",
    "        alldocs_i = list(dftc.index)\n",
    "    elif dataset == 'ubuntu dialogue':\n",
    "        alldocs_i = list(dfud.index)\n",
    "    elif dataset == 'enron email':\n",
    "        alldocs_i = list(dfee.index)\n",
    "\n",
    "    # Create a document map mapping a document number (0 to 249) to a document id in the dataset from which it originated\n",
    "    random.seed(rnd_seed)\n",
    "    docs_i = random.sample(alldocs_i, 250)\n",
    "    for i, doc_i in enumerate(docs_i):\n",
    "        docmap[dataset][i] = doc_i\n",
    "\n",
    "# Print and save docmap\n",
    "print(docmap)\n",
    "with open(f\"{capstone_dir}/docmap.json\", 'w') as fh:\n",
    "    json.dump(docmap, fh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c96a532-4ef9-4247-802a-ce93fc5b0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>8</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>-12.541200</td>\n",
       "      <td>-0.442860</td>\n",
       "      <td>...</td>\n",
       "      <td>77.495494</td>\n",
       "      <td>20240401_133320</td>\n",
       "      <td>[(0, [('tear', 0.1492861747332724), ('eye', 0....</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.378848</td>\n",
       "      <td>-14.478989</td>\n",
       "      <td>-0.512248</td>\n",
       "      <td>...</td>\n",
       "      <td>93.398113</td>\n",
       "      <td>20240401_133437</td>\n",
       "      <td>[(0, [('change', 0.041310894306892804), ('expe...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...</td>\n",
       "      <td>{\"0\": [[0, 0.05856030061841011], [1, 0.0758973...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...</td>\n",
       "      <td>[[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>1</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>-13.535590</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>...</td>\n",
       "      <td>133.312275</td>\n",
       "      <td>20240401_133611</td>\n",
       "      <td>[(0, [('pass', 0.02115486804364902), ('break',...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...</td>\n",
       "      <td>{\"0\": [[0, 0.0338706336915493], [1, -0.0393538...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...</td>\n",
       "      <td>[[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>2</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.662265</td>\n",
       "      <td>-13.528606</td>\n",
       "      <td>-0.484894</td>\n",
       "      <td>...</td>\n",
       "      <td>137.099871</td>\n",
       "      <td>20240401_133824</td>\n",
       "      <td>[(0, [('experience', 0.01997448274133498), ('p...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"game\"], [1, \"game\"], [2, \"Electrical out...</td>\n",
       "      <td>{\"0\": [[0, 0.05463644862174988], [1, 0.0174532...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...</td>\n",
       "      <td>[[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>11</td>\n",
       "      <td>{'num_topics': 30}</td>\n",
       "      <td>0.249136</td>\n",
       "      <td>-12.437530</td>\n",
       "      <td>-0.436446</td>\n",
       "      <td>...</td>\n",
       "      <td>91.362547</td>\n",
       "      <td>20240401_134041</td>\n",
       "      <td>[(0, [('tear', 0.14928553793317761), ('eye', 0...</td>\n",
       "      <td>{0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Dataset Num_docs Rnd_seed Model Num_topics        Model_params  \\\n",
       "0      0  chitchat      250       77   lsi          8  {'num_topics': 15}   \n",
       "1      1  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "2      2  chitchat      250       77   lsi          1  {'num_topics': 15}   \n",
       "3      3  chitchat      250       77   lsi          2  {'num_topics': 15}   \n",
       "4      4  chitchat      250       77   lsi         11  {'num_topics': 30}   \n",
       "\n",
       "   Cv_score  Cuci_score  Cnpmi_score  ...     Runtime        Timestamp  \\\n",
       "0  0.256103  -12.541200    -0.442860  ...   77.495494  20240401_133320   \n",
       "1  0.378848  -14.478989    -0.512248  ...   93.398113  20240401_133437   \n",
       "2  0.618475  -13.535590    -0.484803  ...  133.312275  20240401_133611   \n",
       "3  0.662265  -13.528606    -0.484894  ...  137.099871  20240401_133824   \n",
       "4  0.249136  -12.437530    -0.436446  ...   91.362547  20240401_134041   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('tear', 0.1492861747332724), ('eye', 0....   \n",
       "1  [(0, [('change', 0.041310894306892804), ('expe...   \n",
       "2  [(0, [('pass', 0.02115486804364902), ('break',...   \n",
       "3  [(0, [('experience', 0.01997448274133498), ('p...   \n",
       "4  [(0, [('tear', 0.14928553793317761), ('eye', 0...   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "1  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "2  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "3  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "4  {0: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "1  [[0, \"Change\"], [1, \"Change\"], [2, \"Dissipate\"...   \n",
       "2  [[0, \"game\"], [1, \"Science/Tech\"], [2, \"Sports...   \n",
       "3  [[0, \"game\"], [1, \"game\"], [2, \"Electrical out...   \n",
       "4  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "\n",
       "                                   Cosine_similarity Model_family  \\\n",
       "0  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "1  {\"0\": [[0, 0.05856030061841011], [1, 0.0758973...          bow   \n",
       "2  {\"0\": [[0, 0.0338706336915493], [1, -0.0393538...          bow   \n",
       "3  {\"0\": [[0, 0.05463644862174988], [1, 0.0174532...          bow   \n",
       "4  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "1  [[0, [[\"looker\", 0], [\"eddy\", 0], [\"vesture\", ...   \n",
       "2  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"alter...   \n",
       "3  [[0, [[\"follow\", 0], [\"deferment\", 0], [\"vestu...   \n",
       "4  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "\n",
       "                                         Flan_topic2 Keyphrases  \n",
       "0  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "1  [[0, \"Dog\"], [1, \"Change\"], [2, \"trespass\"], [...      False  \n",
       "2  [[0, \"Running game\"], [1, \"Deferment\"], [2, \"S...      False  \n",
       "3  [[0, \"Deferment\"], [1, \"Sports\"], [2, \"Animal\"...      False  \n",
       "4  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>topical chat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>9</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>0.368992</td>\n",
       "      <td>-9.861310</td>\n",
       "      <td>-0.197724</td>\n",
       "      <td>...</td>\n",
       "      <td>11.998523</td>\n",
       "      <td>20240504_155104</td>\n",
       "      <td>[(-1, [('troopers', 0.5052358), ('military', 0...</td>\n",
       "      <td>[{-1: [2, 4, 6, 11, 12, 21, 22, 35, 36, 38, 41...</td>\n",
       "      <td>[[-1, \"World\"], [0, \"Science/Tech\"], [1, \"Aero...</td>\n",
       "      <td>{\"-1\": [[2, 0.08672984689474106], [4, 0.014690...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[-1, [[\"bad person\", 0], [\"chessman\", 0], [\"s...</td>\n",
       "      <td>[[-1, \"wikileaks\"], [0, \"YouTube\"], [1, \"Black...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>ubuntu dialogue</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>2</td>\n",
       "      <td>{'repr_model': 'none'}</td>\n",
       "      <td>0.542989</td>\n",
       "      <td>-14.670168</td>\n",
       "      <td>-0.459252</td>\n",
       "      <td>...</td>\n",
       "      <td>10.875522</td>\n",
       "      <td>20240504_155415</td>\n",
       "      <td>[(0, [('ubuntu', 0.14119847234575922), ('linux...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...</td>\n",
       "      <td>[[0, \"Linux\"], [1, \"World\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.2154170125722885], [1, 0.11550097...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"phone card\", 0], [\"bank bill\", 0], [\"a...</td>\n",
       "      <td>[[0, \"Science/Tech\"], [1, \"Fox\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>ubuntu dialogue</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>2</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>0.447699</td>\n",
       "      <td>-14.460991</td>\n",
       "      <td>-0.470929</td>\n",
       "      <td>...</td>\n",
       "      <td>11.713669</td>\n",
       "      <td>20240504_155426</td>\n",
       "      <td>[(0, [('ubuntu', 0.7777027), ('linux', 0.62036...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...</td>\n",
       "      <td>[[0, \"Linux\"], [1, \"World\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.1624004989862442], [1, 0.03420655...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"artifact\", 0], [\"adroitness\", 0], [\"ge...</td>\n",
       "      <td>[[0, \"xubuntu\"], [1, \"Race (United States)\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>enron email</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>3</td>\n",
       "      <td>{'repr_model': 'none'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134730</td>\n",
       "      <td>20240504_155926</td>\n",
       "      <td>[(0, [('gas', 0.07409395651732713), ('enron', ...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...</td>\n",
       "      <td>[[0, \"Energy\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.17197903990745544], [1, 0.2113838...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"futures\", 0], [\"entropy\", 0], [\"tradin...</td>\n",
       "      <td>[[0, \"Future\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>enron email</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>3</td>\n",
       "      <td>{'repr_model': 'keybert'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.332339</td>\n",
       "      <td>20240504_155929</td>\n",
       "      <td>[(0, [('enron', 0.7303953), ('gas', 0.38910866...</td>\n",
       "      <td>[{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...</td>\n",
       "      <td>[[0, \"Business\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>{\"0\": [[0, 0.020173335447907448], [1, 0.006645...</td>\n",
       "      <td>transformer</td>\n",
       "      <td>[[0, [[\"futures\", 0], [\"guard\", 0], [\"trading\"...</td>\n",
       "      <td>[[0, \"Business\"], [1, \"\"], [2, \"\"]]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index          Dataset Num_docs Rnd_seed     Model Num_topics  \\\n",
       "395    395     topical chat      250       77  bertopic          9   \n",
       "396    396  ubuntu dialogue      250       77  bertopic          2   \n",
       "397    397  ubuntu dialogue      250       77  bertopic          2   \n",
       "398    398      enron email      250       77  bertopic          3   \n",
       "399    399      enron email      250       77  bertopic          3   \n",
       "\n",
       "                  Model_params  Cv_score  Cuci_score  Cnpmi_score  ...  \\\n",
       "395  {'repr_model': 'keybert'}  0.368992   -9.861310    -0.197724  ...   \n",
       "396     {'repr_model': 'none'}  0.542989  -14.670168    -0.459252  ...   \n",
       "397  {'repr_model': 'keybert'}  0.447699  -14.460991    -0.470929  ...   \n",
       "398     {'repr_model': 'none'}       NaN         NaN          NaN  ...   \n",
       "399  {'repr_model': 'keybert'}       NaN         NaN          NaN  ...   \n",
       "\n",
       "       Runtime        Timestamp  \\\n",
       "395  11.998523  20240504_155104   \n",
       "396  10.875522  20240504_155415   \n",
       "397  11.713669  20240504_155426   \n",
       "398   3.134730  20240504_155926   \n",
       "399   3.332339  20240504_155929   \n",
       "\n",
       "                                           Topic_words  \\\n",
       "395  [(-1, [('troopers', 0.5052358), ('military', 0...   \n",
       "396  [(0, [('ubuntu', 0.14119847234575922), ('linux...   \n",
       "397  [(0, [('ubuntu', 0.7777027), ('linux', 0.62036...   \n",
       "398  [(0, [('gas', 0.07409395651732713), ('enron', ...   \n",
       "399  [(0, [('enron', 0.7303953), ('gas', 0.38910866...   \n",
       "\n",
       "                                            Doc_topics  \\\n",
       "395  [{-1: [2, 4, 6, 11, 12, 21, 22, 35, 36, 38, 41...   \n",
       "396  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...   \n",
       "397  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1...   \n",
       "398  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...   \n",
       "399  [{0: [0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, ...   \n",
       "\n",
       "                                            Flan_topic  \\\n",
       "395  [[-1, \"World\"], [0, \"Science/Tech\"], [1, \"Aero...   \n",
       "396                       [[0, \"Linux\"], [1, \"World\"]]   \n",
       "397                       [[0, \"Linux\"], [1, \"World\"]]   \n",
       "398                  [[0, \"Energy\"], [1, \"\"], [2, \"\"]]   \n",
       "399                [[0, \"Business\"], [1, \"\"], [2, \"\"]]   \n",
       "\n",
       "                                     Cosine_similarity Model_family  \\\n",
       "395  {\"-1\": [[2, 0.08672984689474106], [4, 0.014690...  transformer   \n",
       "396  {\"0\": [[0, 0.2154170125722885], [1, 0.11550097...  transformer   \n",
       "397  {\"0\": [[0, 0.1624004989862442], [1, 0.03420655...  transformer   \n",
       "398  {\"0\": [[0, 0.17197903990745544], [1, 0.2113838...  transformer   \n",
       "399  {\"0\": [[0, 0.020173335447907448], [1, 0.006645...  transformer   \n",
       "\n",
       "                                          Topic_words2  \\\n",
       "395  [[-1, [[\"bad person\", 0], [\"chessman\", 0], [\"s...   \n",
       "396  [[0, [[\"phone card\", 0], [\"bank bill\", 0], [\"a...   \n",
       "397  [[0, [[\"artifact\", 0], [\"adroitness\", 0], [\"ge...   \n",
       "398  [[0, [[\"futures\", 0], [\"entropy\", 0], [\"tradin...   \n",
       "399  [[0, [[\"futures\", 0], [\"guard\", 0], [\"trading\"...   \n",
       "\n",
       "                                           Flan_topic2 Keyphrases  \n",
       "395  [[-1, \"wikileaks\"], [0, \"YouTube\"], [1, \"Black...       True  \n",
       "396                  [[0, \"Science/Tech\"], [1, \"Fox\"]]       True  \n",
       "397      [[0, \"xubuntu\"], [1, \"Race (United States)\"]]       True  \n",
       "398                  [[0, \"Future\"], [1, \"\"], [2, \"\"]]       True  \n",
       "399                [[0, \"Business\"], [1, \"\"], [2, \"\"]]       True  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Dataset', 'Num_docs', 'Rnd_seed', 'Model', 'Num_topics',\n",
      "       'Model_params', 'Cv_score', 'Cuci_score', 'Cnpmi_score', 'Umass_score',\n",
      "       'Spell_checked', 'Text_speak', 'Synonyms', 'Hypernyms', 'Runtime',\n",
      "       'Timestamp', 'Topic_words', 'Doc_topics', 'Flan_topic',\n",
      "       'Cosine_similarity', 'Model_family', 'Topic_words2', 'Flan_topic2',\n",
      "       'Keyphrases'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load latest pickle\n",
    "dfr = load_last_pickle('sim')\n",
    "print(dfr.shape)\n",
    "display(dfr.head())\n",
    "display(dfr.tail())\n",
    "print(dfr.columns)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9ebc271-9b85-44f7-bfe6-58184d7ade4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which rows correspond to which model runs\n",
    "#print(dfr.loc[(dfr['Model']=='lda') & (dfr['Model_params'].astype(str)==\"{'num_topics': 15, 'num_passes': 25}\") & (dfr['Keyphrases']==True)])\n",
    "#print(dfr.loc[(dfr['Model']=='bertopic') & (dfr['Model_params'].astype(str)==\"{'repr_model': 'none'}\") & (dfr['Keyphrases']==True)])\n",
    "#print(dfr.loc[(dfr['Model']=='word2vec') & (dfr['Model_params'].astype(str)==\"{'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\") & (dfr['Keyphrases']==True)])\n",
    "#print(dfr.loc[(dfr['Model']=='word2vec') & (dfr['Model_params'].astype(str)==\"{'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\") & (dfr['Keyphrases']==True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8cb35394-58ff-4332-a89b-ca9ad4eda564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Human_topic</th>\n",
       "      <th>Relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>i never turn mine off anyways updates would ju...</td>\n",
       "      <td>browser,college,major,BYU,student,media,marketing</td>\n",
       "      <td>traveling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>Hello fellow human! As a human with skin and h...</td>\n",
       "      <td>smell,friends,hangout,rpg,game,play,munchkin,b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>808.0</td>\n",
       "      <td>Hello! Hahaha hey again! :) I was having a con...</td>\n",
       "      <td>pizza,restaurant,topping,pepper,santa cruz,lag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>985.0</td>\n",
       "      <td>Hello? hey! Hello! Sorry, this is the first ti...</td>\n",
       "      <td>book,bank,florence,machine,panic,disco,music,j...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>791.0</td>\n",
       "      <td>Oh for sure :p oh my finally a person! I anni ...</td>\n",
       "      <td>south korea,metal,listen,music,rap,hiphop,rb,c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>enron email</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payout,investment,builders,profit,repaid,split...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>enron email</td>\n",
       "      <td>205.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>notes,outlook,migration,survey,fill,computer,p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>enron email</td>\n",
       "      <td>205.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>overdue,access,request,mat,smith,pending,approval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>enron email</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>extension, document, engineer, architect, acco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>enron email</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lady, treasure, night, dinner, behavior, offen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dataset  Doc_id                                            Snippet  \\\n",
       "0       chitchat  1035.0  i never turn mine off anyways updates would ju...   \n",
       "1       chitchat  1334.0  Hello fellow human! As a human with skin and h...   \n",
       "2       chitchat   808.0  Hello! Hahaha hey again! :) I was having a con...   \n",
       "3       chitchat   985.0  Hello? hey! Hello! Sorry, this is the first ti...   \n",
       "4       chitchat   791.0  Oh for sure :p oh my finally a person! I anni ...   \n",
       "..           ...     ...                                                ...   \n",
       "163  enron email  4128.0                                                NaN   \n",
       "164  enron email   205.0                                                NaN   \n",
       "165  enron email   205.0                                                NaN   \n",
       "166  enron email  3418.0                                                NaN   \n",
       "167  enron email  3418.0                                                NaN   \n",
       "\n",
       "                                           Topic_words Human_topic  Relevant  \n",
       "0    browser,college,major,BYU,student,media,marketing   traveling         1  \n",
       "1    smell,friends,hangout,rpg,game,play,munchkin,b...         NaN         1  \n",
       "2    pizza,restaurant,topping,pepper,santa cruz,lag...         NaN         1  \n",
       "3    book,bank,florence,machine,panic,disco,music,j...         NaN         1  \n",
       "4    south korea,metal,listen,music,rap,hiphop,rb,c...         NaN         1  \n",
       "..                                                 ...         ...       ...  \n",
       "163  payout,investment,builders,profit,repaid,split...         NaN         0  \n",
       "164  notes,outlook,migration,survey,fill,computer,p...         NaN         0  \n",
       "165  overdue,access,request,mat,smith,pending,approval         NaN         0  \n",
       "166  extension, document, engineer, architect, acco...         NaN         0  \n",
       "167  lady, treasure, night, dinner, behavior, offen...         NaN         0  \n",
       "\n",
       "[168 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the manually defined topic words\n",
    "dfkw = pd.read_excel('C:/Users/micha/Box Sync/cuny/698-Capstone/keywords.xlsx', sheet_name='human')\n",
    "display(dfkw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b0291dc-a37e-459b-9878-1290377395f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"bertopic - {'repr_model': 'keybert'}\"\n",
      " \"bertopic - {'repr_model': 'none'}\"\n",
      " \"lda - {'num_topics': 15, 'num_passes': 25}\"\n",
      " \"lda - {'num_topics': 30, 'num_passes': 25}\" \"lsi - {'num_topics': 15}\"\n",
      " \"lsi - {'num_topics': 30}\" \"nmf - {'num_topics': 15, 'num_passes': 15}\"\n",
      " \"nmf - {'num_topics': 30, 'num_passes': 15}\"\n",
      " \"word2vec - {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 0.1, 'min_samples': 3}\"\n",
      " \"word2vec - {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 1, 'min_samples': 2}\"\n",
      " \"word2vec - {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\"\n",
      " \"word2vec - {'embeddings': 'fasttext', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 30, 'max_iter': 300, 'tol': 0.0001}\"\n",
      " \"word2vec - {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 0.1, 'min_samples': 3}\"\n",
      " \"word2vec - {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 1, 'min_samples': 2}\"\n",
      " \"word2vec - {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\"\n",
      " \"word2vec - {'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 30, 'max_iter': 300, 'tol': 0.0001}\"\n",
      " \"word2vec - {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 0.1, 'min_samples': 3}\"\n",
      " \"word2vec - {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'dbscan', 'eps': 1, 'min_samples': 2}\"\n",
      " \"word2vec - {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\"\n",
      " \"word2vec - {'embeddings': 'word2vec', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 30, 'max_iter': 300, 'tol': 0.0001}\"]\n",
      "index\n",
      "Dataset\n",
      "Num_docs\n",
      "Rnd_seed\n",
      "Model\n",
      "Num_topics\n",
      "Model_params\n",
      "Cv_score\n",
      "Cuci_score\n",
      "Cnpmi_score\n",
      "Umass_score\n",
      "Spell_checked\n",
      "Text_speak\n",
      "Synonyms\n",
      "Hypernyms\n",
      "Runtime\n",
      "Timestamp\n",
      "Topic_words\n",
      "Doc_topics\n",
      "Flan_topic\n",
      "Cosine_similarity\n",
      "Model_family\n",
      "Topic_words2\n",
      "Flan_topic2\n",
      "Keyphrases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get stats on models\n",
    "print(np.unique(dfr['Model'].astype(str) + ' - ' + dfr['Model_params'].astype(str)))\n",
    "#print(np.unique(dfr['Model_params'].astype(str)))\n",
    "[print(e) for e in list(dfr.columns)]\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17af2a5b-e4e8-4656-a00f-697ecdb7c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20963,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     661\n",
       "1     835\n",
       "2     458\n",
       "3     488\n",
       "4    3886\n",
       "Name: words, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    20963.000000\n",
       "mean       381.118256\n",
       "std        696.331635\n",
       "min          1.000000\n",
       "25%        130.000000\n",
       "50%        191.000000\n",
       "75%        441.000000\n",
       "max      25471.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get word count stats\n",
    "\n",
    "\"\"\"\n",
    "print(dfcc['words'].describe())\n",
    "print()\n",
    "print(dftc['words'].describe())\n",
    "print()\n",
    "print(dfud['words'].describe())\n",
    "print()\n",
    "print(dfee['words'].describe())\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "#dftmp = pd.DataFrame(dfcc['words'])\n",
    "dftmp = pd.concat([dfcc['words'], dftc['words'], dfud['words'], dfee['words']])\n",
    "print(dftmp.shape)\n",
    "display(dftmp.head())\n",
    "print()\n",
    "display(dftmp.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0c27fe4-b5dc-4209-a794-17384b48acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc ids:\n",
      "0: 1035, 1: 1334, 2: 808, 3: 985, 4: 791, 5: 471, 6: 1197, 7: 1950, 8: 2285, 9: 2504, 10: 974, 11: 595, 12: 2276, 13: 2583, 14: 2707, 15: 2050, 16: 9, 17: 1146, 18: 116, 19: 2036, \n",
      "\n",
      "cc\n",
      "985: 121, 1950: 173, 595: 193, 2050: 210, 1197: 294, 2276: 305, 2504: 384, 471: 533, 2707: 608, 2036: 621, 1146: 744, 2583: 793, 791: 1083, 1035: 1141, 974: 1445, 808: 1565, 1334: 1676, 116: 1735, 2285: 1967, 9: 2805, \n",
      "\n",
      "tc\n",
      "2276: 382, 595: 394, 116: 405, 1197: 433, 1334: 481, 808: 494, 2050: 495, 2707: 506, 985: 550, 974: 561, 2036: 565, 2504: 579, 2583: 610, 1035: 616, 1950: 659, 791: 665, 2285: 687, 471: 736, 9: 780, 1146: 794, \n",
      "\n",
      "ud\n",
      "2036: 90, 116: 96, 2050: 105, 9: 108, 974: 112, 1146: 113, 1950: 115, 2707: 117, 595: 120, 1334: 136, 471: 145, 1197: 156, 985: 157, 808: 171, 2583: 177, 2504: 200, 2276: 248, 791: 256, 2285: 288, 1035: 334, \n",
      "\n",
      "ee\n",
      "1197: 6, 791: 14, 2707: 18, 1035: 22, 808: 23, 974: 31, 595: 60, 2036: 126, 2276: 193, 985: 243, 2583: 260, 2285: 275, 2504: 292, 1146: 304, 9: 556, 471: 577, 1950: 643, 2050: 775, 1334: 847, 116: 1386, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose 2 conversations per dataset that have between 210 and 610 words (avg words/convo = 410).\n",
    "# This is messed up because ud and ee don't follow the same indexex as cc and tc.\n",
    "# So none of the indexes I chose to hand-label for these two datasets are actually in the list of randomly chosen 250 documents below.\n",
    "# Instead of using this section to choose the ud and ee docs, use the next two sections.\n",
    "\n",
    "# See if any of the conversations that I already prelabled are in that range\n",
    "labeled = [1035, 1334, 808, 985, 791, 471, 1197, 1950, 2285, 2504, 974, 595, 2276, 2583, 2707, 2050, 9, 1146, 116, 2036]\n",
    "\n",
    "# For each dataset, generate list of word counts for the above documents\n",
    "l = {}\n",
    "l['cc'] = dfcc.loc[dfcc['index'].isin(labeled), ['index', 'words']].sort_values(['words'])\n",
    "l['tc'] = dftc.loc[dftc['index'].isin(labeled), ['index', 'words']].sort_values(['words'])\n",
    "l['ud'] = dfud.loc[dfud['index'].isin(labeled), ['index', 'words']].sort_values(['words'])\n",
    "l['ee'] = dfee.loc[dfee['index'].isin(labeled), ['index', 'words']].sort_values(['words'])\n",
    "\n",
    "# Print doc ids for reference\n",
    "print('doc ids:')\n",
    "[print(f\"{i}: {id}\", end=', ') for i, id in enumerate(labeled)]\n",
    "print('\\n')\n",
    "\n",
    "# Print word counts for each dataset\n",
    "for dataset in l.keys():\n",
    "    print(dataset)\n",
    "    [print(f\"{row['index']}: {row['words']}\", end=', ') for _, row in l[dataset].iterrows()]\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a38b4a0-1e1a-4697-a748-216b03549642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dasda: anyone else have trouble viewing ebooks in ubuntu?\n",
      "\n",
      "Squideshi: In what format and with what application?\n",
      "\n",
      "Dasda: exe and with ebookpro\n",
      "\n",
      "Squideshi: The ebook is in EXE format?\n",
      "\n",
      "Dasda: yes, thats why i wouldnt mind it if i could even get the exe to unpack but cant even do that\n",
      "\n",
      "Squideshi: EXE files are usually for Microsoft operating systems. Where did you get the file?\n",
      "\n",
      "Dasda: vgsports\n",
      "\n",
      "Dasda: Squdeshi i can give u the link for download. it is only 1.8megs\n",
      "\n",
      "Squideshi: Don't give me the link for download. Give me the link for the webpage that has the link for download.\n",
      "\n",
      "Dasda: i message it to you cause I was unsure if we are allowed to post links here\n",
      "\n",
      "\n",
      "*************************************\n",
      "aanonymouss: In order to run VNC (like vino, remote desktop) do I need to have a video card installed?  I booted up w/o vid card and I can ssh into the box but cannot vnc in and it seems like x11 applications aren't running\n",
      "\n",
      "fryguy: you'll need to run XvFB or something\n",
      "\n",
      "fryguy: or forward through a different X server via X11 forwarding or xdmcp\n",
      "\n",
      "aanonymouss: thanks I'll look into that\n",
      "\n",
      "aanonymouss: any idea why x11 apps aren't running?\n",
      "\n",
      "fryguy: you can't run X apps without an X server, and you can't run the X server that is configured out of the box in ubuntu without a video display\n",
      "\n",
      "aanonymouss: would it change anything if I originally setup the box with x11 running and had everything running, then ripped out my gfx card and now am trying to run everything?\n",
      "\n",
      "fryguy: no\n",
      "\n",
      "aanonymouss: so if there's no video card, x server wont start?\n",
      "\n",
      "fryguy: the one out of the box in ubuntu won't, which is why you should look into XvFB or another X server, or just run the X server on another machine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for a suitably sized ubuntu conversation\n",
    "docid_list = [docmap['ubuntu dialogue'][docid] for docid in docmap['ubuntu dialogue']]\n",
    "#print(dfud.loc[dfud['index'].isin(docid_list), ['index', 'words']].sort_values(by='words').values)\n",
    "print(dfud.loc[dfud['index'] == 4140, 'conv'].values[0])\n",
    "print('*************************************')\n",
    "print(dfud.loc[dfud['index'] == 6584, 'conv'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64b39c76-2019-4d13-b5ca-ee221f5c9961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacques,\n",
      "\n",
      "Still trying to close the loop on the $15,000 of extensions.  Assuming that \n",
      "it is worked out today or tomorrow, I would like to get whatever documents \n",
      "need to be\n",
      "completed to convey the partnership done.  I need to work with the engineer \n",
      "and architect to get things moving.  I am planning on  writing a personal \n",
      "check to the engineer while I am setting up new accounts.  Let me know if \n",
      "there is a reason I should not do this.\n",
      "\n",
      "Thanks for all your help so far.  Between your connections and expertise in \n",
      "structuring the loan, you saved us from getting into a bad deal.\n",
      "\n",
      "Phillip\n",
      "*************************************\n",
      "Last night\n",
      "\n",
      "Lady, c'mon...you're just one of the guys!  Wanna go to Treasures tonight?\n",
      "\n",
      "\n",
      "From: Margaret Allen@ENRON on 10/13/2000 08:39 AM\n",
      "To: John Arnold/HOU/ECT@ECT\n",
      "cc:  \n",
      "Subject: Last night\n",
      "\n",
      "Hey Buster John,\n",
      "\n",
      "Despite the X's you received last night for your ill behavior, I wanted to \n",
      "thank you for dinner because I had a great time.  Although, I do take \n",
      "personal offense to being flipped off at least 5 times in the course of \n",
      "dinner. Watch your manners when your with a lady!\n",
      "\n",
      "I hope you have a great Friday and today is one of your top 5 too!\n",
      "\n",
      "MSA\n"
     ]
    }
   ],
   "source": [
    "# Look for a suitably sized enron email\n",
    "docid_list = [docmap['enron email'][docid] for docid in docmap['enron email']]\n",
    "#print(dfee.loc[dfee['index'].isin(docid_list), ['index', 'words']].sort_values(by='words').values)\n",
    "print(dfee.loc[dfee['index'] == 995, 'conv'].values[0])\n",
    "print('*************************************')\n",
    "print(dfee.loc[dfee['index'] == 4128, 'conv'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e7d87fb-b714-4b5e-803c-ac0da8f5f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>8</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>-12.5412</td>\n",
       "      <td>-0.44286</td>\n",
       "      <td>...</td>\n",
       "      <td>77.495494</td>\n",
       "      <td>20240401_133320</td>\n",
       "      <td>[(0, [('tear', 0.1492861747332724), ('eye', 0....</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Dataset Num_docs Rnd_seed Model Num_topics        Model_params  \\\n",
       "0      0  chitchat      250       77   lsi          8  {'num_topics': 15}   \n",
       "\n",
       "   Cv_score  Cuci_score  Cnpmi_score  ...    Runtime        Timestamp  \\\n",
       "0  0.256103    -12.5412     -0.44286  ...  77.495494  20240401_133320   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('tear', 0.1492861747332724), ('eye', 0....   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "\n",
       "                                   Cosine_similarity Model_family  \\\n",
       "0  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "\n",
       "                                         Flan_topic2 Keyphrases  \n",
       "0  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             Dataset  index\n",
      "115         chitchat    115\n",
      "163     topical chat    163\n",
      "211  ubuntu dialogue    211\n",
      "259      enron email    259\n",
      "\n",
      "mturk rows with only one topic:\n",
      "Empty DataFrame\n",
      "Columns: [Num_topics, Doc_topics]\n",
      "Index: []\n",
      "mturk2 rows with only one topic:\n",
      "Empty DataFrame\n",
      "Columns: [Num_topics, Doc_topics]\n",
      "Index: []\n",
      "\n",
      "dataset cc\n",
      "\tdocid 2276\n",
      "\t\twords 290\n",
      "\tdocid 2504\n",
      "\t\twords 357\n",
      "dataset tc\n",
      "\tdocid 116\n",
      "\t\twords 384\n",
      "\tdocid 808\n",
      "\t\twords 473\n",
      "dataset ud\n",
      "\tdocid 4140\n",
      "\t\twords 143\n",
      "\tdocid 6584\n",
      "\t\twords 217\n",
      "dataset ee\n",
      "\tdocid 995\n",
      "\t\twords 131\n",
      "\tdocid 4128\n",
      "\t\twords 134\n",
      "docs: 8\n",
      "words: 2129\n",
      "words/doc: 266.125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selected mturk docs based on word count\n",
    "mturk_docs = {\n",
    "    'cc': [2276, 2504],\n",
    "    'tc': [116, 808],\n",
    "    'ud': [4140, 6584],\n",
    "    'ee': [995, 4128]\n",
    "}\n",
    "\n",
    "# Select rows\n",
    "display(dfr.head(1))\n",
    "print()\n",
    "print(dfr.loc[\n",
    "      (dfr['Model'] == 'word2vec') &\n",
    "      (dfr['Model_params'].astype('string') == \"{'embeddings': 'glove', 'vector_size': 200, 'min_count': 1, 'cluster_alg': 'kmeans', 'n_clusters': 15, 'max_iter': 300, 'tol': 0.0001}\") & \n",
    "      (dfr['Spell_checked'] == True) &\n",
    "      (dfr['Text_speak'] == True) &\n",
    "      (dfr['Synonyms'] == True) & \n",
    "      (dfr['Hypernyms'] == True),\n",
    "      ['Dataset', 'index']])\n",
    "print()\n",
    "mturk_rows = [16, 19, 288, 292, 96, 99, 112, 115, 40, 43, 296, 300, 144, 147, 160, 163, 64, 67, 304, 308, 192, 195, 208, 211, 88, 91, 312, 316, 240, 243, 256, 259]\n",
    "no_flan = [292, 300, 308, 316]  # no need to run flan on these because it was processed with keybert\n",
    "mturk_rows2 = [324, 330, 336, 342, 344, 348, 356, 360, 368, 372, 380, 384, 392, 394, 396, 398]  # with keyphrase extraction\n",
    "\n",
    "# See if any selected rows only have a single topic\n",
    "print('mturk rows with only one topic:')\n",
    "print(dfr.loc[(dfr.index.isin(mturk_rows)) & (dfr['Num_topics'] == 1), ['Num_topics', 'Doc_topics']])\n",
    "print('mturk2 rows with only one topic:')\n",
    "print(dfr.loc[(dfr.index.isin(mturk_rows2)) & (dfr['Num_topics'] == 1), ['Num_topics', 'Doc_topics']])\n",
    "print()\n",
    "\n",
    "# Average word count\n",
    "wordct = 0\n",
    "docct = 0\n",
    "for dataset in mturk_docs.keys():\n",
    "    \n",
    "    # Get the document map for this dataset\n",
    "    print(f\"dataset {dataset}\")\n",
    "    ds_docmap = docmap[datasetmap[dataset]]  # This is the docmap for this dataset\n",
    "\n",
    "    # Get handle to the dataframe\n",
    "    df = eval(f'df{dataset}')\n",
    "    #display(df.head())\n",
    "\n",
    "    # Iterate over each doc in the dataset\n",
    "    for docid in mturk_docs[dataset]:\n",
    "\n",
    "        # Get the document number from the docmap\n",
    "        print(f\"\\tdocid {docid}\")\n",
    "        #print(ds_docmap)\n",
    "        #print([docnum for docnum in ds_docmap.keys() if ds_docmap[docnum] == docid])\n",
    "        docnum = [docnum for docnum in ds_docmap.keys() if ds_docmap[docnum] == docid][0]\n",
    "\n",
    "        # Get conversation text\n",
    "        conv = df.loc[df['index'] == docid, 'conv'].values[0]\n",
    "        conv = conv.replace('\\n\\n', '\\n')\n",
    "\n",
    "        # Counts\n",
    "        words = len(re.split(r'\\W', conv))\n",
    "        print(f\"\\t\\twords {words}\")\n",
    "        wordct += words\n",
    "        docct += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"docs: {docct}\")\n",
    "print(f\"words: {wordct}\")\n",
    "print(f\"words/doc: {wordct / docct}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07faa6c2-c8bc-463c-9dfd-d17119223c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19, 288, 292, 96, 99, 112, 115, 40, 43, 296, 300, 144, 147, 160, 163, 64, 67, 304, 308, 192, 195, 208, 211, 88, 91, 312, 316, 240, 243, 256, 259]\n",
      "[292, 300, 308, 316]\n"
     ]
    }
   ],
   "source": [
    "# Prework for second round of mturk\n",
    "\n",
    "# Look at what I chose before for rows\n",
    "print(mturk_rows)\n",
    "print(no_flan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76b695a9-f3cd-444b-a7a3-2b3ce91d3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset cc\n",
      "\tdocid 2276\n",
      "\t\trowid 324\n",
      "\t\trowid 344\n",
      "\t\trowid 348\n",
      "\t\trowid 392\n",
      "\tdocid 2504\n",
      "\t\trowid 324\n",
      "\t\trowid 344\n",
      "\t\trowid 348\n",
      "\t\trowid 392\n",
      "dataset tc\n",
      "\tdocid 116\n",
      "\t\trowid 330\n",
      "\t\trowid 356\n",
      "\t\trowid 360\n",
      "\t\trowid 394\n",
      "\tdocid 808\n",
      "\t\trowid 330\n",
      "\t\trowid 356\n",
      "\t\trowid 360\n",
      "\t\trowid 394\n",
      "dataset ud\n",
      "\tdocid 4140\n",
      "\t\trowid 336\n",
      "\t\trowid 368\n",
      "\t\trowid 372\n",
      "\t\trowid 396\n",
      "\tdocid 6584\n",
      "\t\trowid 336\n",
      "\t\trowid 368\n",
      "\t\trowid 372\n",
      "\t\trowid 396\n",
      "dataset ee\n",
      "\tdocid 995\n",
      "\t\trowid 342\n",
      "\t\trowid 380\n",
      "\t\trowid 384\n",
      "\t\trowid 398\n",
      "\tdocid 4128\n",
      "\t\trowid 342\n",
      "\t\trowid 380\n",
      "\t\trowid 384\n",
      "\t\trowid 398\n"
     ]
    }
   ],
   "source": [
    "# Generate mturk docs\n",
    "\n",
    "# Set round #\n",
    "#     first round was without keyphrase extraction\n",
    "#     second round was with keyphrase extraction\n",
    "round = 2\n",
    "which_rows = mturk_rows\n",
    "if round == 2: which_rows=mturk_rows2\n",
    "\n",
    "# Iterate over each doc; mturk_docs is keyed on dataset, with each dataset containing an array of docs\n",
    "for dataset in mturk_docs.keys():\n",
    "\n",
    "    # Get the document map for this dataset\n",
    "    print(f\"dataset {dataset}\")\n",
    "    ds_docmap = docmap[datasetmap[dataset]]  # This is the docmap for this dataset\n",
    "\n",
    "    # Get handle to the dataframe\n",
    "    df = eval(f'df{dataset}')\n",
    "    #display(df.head())\n",
    "\n",
    "    # Iterate over each doc in the dataset\n",
    "    for docid in mturk_docs[dataset]:\n",
    "\n",
    "        # Get the document number from the docmap\n",
    "        print(f\"\\tdocid {docid}\")\n",
    "        #print(ds_docmap)\n",
    "        #print([docnum for docnum in ds_docmap.keys() if ds_docmap[docnum] == docid])\n",
    "        docnum = [docnum for docnum in ds_docmap.keys() if ds_docmap[docnum] == docid][0]\n",
    "\n",
    "        # Get conversation text\n",
    "        conv = df.loc[df['index'] == docid, 'conv'].values[0]\n",
    "        conv = conv.replace('\\n\\n', '\\n')\n",
    "        o = f\"dataset={dataset}, docnum={docnum}, docid={docid}, wordct={len(re.split(r'\\W', conv))}\\n\\n\"\n",
    "        o += '[--- conv start ---]\\n'\n",
    "        o += conv + '\\n'\n",
    "        o += '[--- conv end ---]\\n\\n\\n'\n",
    "\n",
    "        # Get human-labeled keywords (only needed in round 1)\n",
    "        if round == 1:\n",
    "            topic = dfkw.loc[(dfkw['Dataset'] == datasetmap[dataset]) & (dfkw['Doc_id'] == docid), 'Topic_words'].values[0]\n",
    "            topic = topic.replace(',', ', ')\n",
    "            o += \"[--- row start ---]\\nrowid 9999, Human_keywords\\n\"\n",
    "            o += f\"[--- topic start ---]{topic}[--- topic end ---]\\n[--- row end ---]\\n\\n\"\n",
    "            #print(o)\n",
    "\n",
    "        # Get human-labeled friendly topic (only needed in round 1)\n",
    "        if round == 1:\n",
    "            topic = dfkw.loc[(dfkw['Dataset'] == datasetmap[dataset]) & (dfkw['Doc_id'] == docid), 'Human_topic'].values[0]\n",
    "            topic = topic.replace(',', ', ')\n",
    "            o += \"[--- row start ---]\\nrowid 9999, Human_friendly_topic\\n\"\n",
    "            o += f\"[--- topic start ---]{topic}[--- topic end ---]\\n[--- row end ---]\\n\\n\"\n",
    "            #print(o)\n",
    "\n",
    "        # Iterate over result rows\n",
    "        for i, row in dfr.loc[dfr.index.isin(which_rows)].iterrows():\n",
    "\n",
    "            # Make sure this row corresponds to the dataset we're currently working on\n",
    "            if row['Dataset'] != datasetmap[dataset]:\n",
    "                continue\n",
    "\n",
    "            # Get topic number\n",
    "            print(f\"\\t\\trowid {i}\")\n",
    "            all_topics = row['Doc_topics']\n",
    "            if type(all_topics) == list:  # bertopic Doc_topics will be a list of dicts, with each dict having a single entry having a key equal to the topic number\n",
    "                tmp = {}\n",
    "                for d in all_topics:\n",
    "                    for k in d.keys():\n",
    "                        tmp[k] = d[k]\n",
    "                all_topics = tmp\n",
    "            topicnum = [topicnum for topicnum in all_topics.keys() if docnum in all_topics[topicnum]][0]\n",
    "            #print(topicnum)\n",
    "\n",
    "            # Get topic representations\n",
    "            # Do this three times: once without flan, once with flan, and once with enhanced flan;\n",
    "            # enhanced flan is when I ran the resulting topic representations from the existing model\n",
    "            # through the synonym and hypernym functions\n",
    "            for colname in ['Topic_words', 'Flan_topic', 'Flan_topic2']:\n",
    "\n",
    "                # Skip this row if it's flan and in the no_flan list; these are rows that used keybert topic representations,\n",
    "                # so it will only have a single topic word; it would be stupid to generate a flan representation for a single\n",
    "                # word, because it would just *be* that word!\n",
    "                #if (colname == 'Flan_topic' or colname == 'Flan_topic2') and row['index'] in no_flan:\n",
    "                #    continue\n",
    "\n",
    "                # Get topics\n",
    "                all_topics = row[colname]\n",
    "                if colname == 'Topic_words':\n",
    "                    topic = [word[0] for word in [topic_tuple[1][:num_topic_words] for topic_tuple in all_topics if topic_tuple[0]==topicnum][0] if len(word[0])>0]\n",
    "                    topic = ', '.join(topic)\n",
    "                elif colname == 'Flan_topic' or colname == 'Flan_topic2':\n",
    "                    all_topics = json.loads(all_topics)\n",
    "                    topic = [topic[1] for topic in all_topics if topic[0] == topicnum][0]\n",
    "                otmp = f\"[--- row start ---]\\nrowid {i}, {colname}\\n\"\n",
    "                otmp += f\"[--- topic start ---]{topic}[--- topic end ---]\\n[--- row end ---]\\n\\n\"\n",
    "                o += otmp\n",
    "                #print(otmp)\n",
    "\n",
    "        # Write to output file\n",
    "        with open(f\"{mturk_dir}round{round}/{dataset}_{docid}.txt\", 'w') as fh:\n",
    "            fh.write(o)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf7e49-ce9f-4998-85fc-0d73b0dbd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a check of topic numbers across the various columns to make sure they match up\n",
    "\n",
    "#display(dfr[(dfr.index>=100) & (dfr.index<=111)])\n",
    "\n",
    "for i, row in dfr.iterrows():\n",
    "\n",
    "    if i >= 97 and i <=319:\n",
    "        print(row['index'], row['Dataset'], row['Model'], row['Num_topics'])\n",
    "        print(row['Model_params'])\n",
    "\n",
    "        nt1 = row['Num_topics']\n",
    "        nt2 = len([topic for topic in row['Topic_words']])\n",
    "        nt3 = len(row['Doc_topics'])\n",
    "        nt4 = len(json.loads(row['Flan_topic']))\n",
    "        print(row['index'], nt1, nt2, nt3, nt4)\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3c7f378-d084-4e84-a854-4055a1c8f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_2276.txt\n",
      "cc_2504.txt\n",
      "\tno topics!\n",
      "ee_4128.txt\n",
      "\tno topics!\n",
      "\tno topics!\n",
      "\tno topics!\n",
      "ee_995.txt\n",
      "\tno topics!\n",
      "\tno topics!\n",
      "\tno topics!\n",
      "tc_116.txt\n",
      "tc_808.txt\n",
      "ud_4140.txt\n",
      "ud_6584.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>topic</th>\n",
       "      <th>rowid</th>\n",
       "      <th>setid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>docnum</th>\n",
       "      <th>docid</th>\n",
       "      <th>wordct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UserB: Traveling the world&lt;br /&gt;UserA: I would...</td>\n",
       "      <td>weddings, engagements, Europe, China, Korea, i...</td>\n",
       "      <td>324</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>cc</td>\n",
       "      <td>12</td>\n",
       "      <td>2276</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UserB: Traveling the world&lt;br /&gt;UserA: I would...</td>\n",
       "      <td>weddings</td>\n",
       "      <td>324</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>cc</td>\n",
       "      <td>12</td>\n",
       "      <td>2276</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UserB: Traveling the world&lt;br /&gt;UserA: I would...</td>\n",
       "      <td>World</td>\n",
       "      <td>324</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>cc</td>\n",
       "      <td>12</td>\n",
       "      <td>2276</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UserB: Traveling the world&lt;br /&gt;UserA: I would...</td>\n",
       "      <td>China, India, Africa, Europe, Germany, Greece,...</td>\n",
       "      <td>344</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>cc</td>\n",
       "      <td>12</td>\n",
       "      <td>2276</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UserB: Traveling the world&lt;br /&gt;UserA: I would...</td>\n",
       "      <td>World</td>\n",
       "      <td>344</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>cc</td>\n",
       "      <td>12</td>\n",
       "      <td>2276</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Science/Tech</td>\n",
       "      <td>372</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>ud</td>\n",
       "      <td>249</td>\n",
       "      <td>6584</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>film industry</td>\n",
       "      <td>372</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>ud</td>\n",
       "      <td>249</td>\n",
       "      <td>6584</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>ubuntu, linux, package, firebox, manager, grub...</td>\n",
       "      <td>396</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>ud</td>\n",
       "      <td>249</td>\n",
       "      <td>6584</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Linux</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>ud</td>\n",
       "      <td>249</td>\n",
       "      <td>6584</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Science/Tech</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>ud</td>\n",
       "      <td>249</td>\n",
       "      <td>6584</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         conversation  \\\n",
       "0   UserB: Traveling the world<br />UserA: I would...   \n",
       "1   UserB: Traveling the world<br />UserA: I would...   \n",
       "2   UserB: Traveling the world<br />UserA: I would...   \n",
       "3   UserB: Traveling the world<br />UserA: I would...   \n",
       "4   UserB: Traveling the world<br />UserA: I would...   \n",
       "..                                                ...   \n",
       "84  aanonymouss: In order to run VNC (like vino, r...   \n",
       "85  aanonymouss: In order to run VNC (like vino, r...   \n",
       "86  aanonymouss: In order to run VNC (like vino, r...   \n",
       "87  aanonymouss: In order to run VNC (like vino, r...   \n",
       "88  aanonymouss: In order to run VNC (like vino, r...   \n",
       "\n",
       "                                                topic rowid        setid  \\\n",
       "0   weddings, engagements, Europe, China, Korea, i...   324  Topic_words   \n",
       "1                                            weddings   324   Flan_topic   \n",
       "2                                               World   324  Flan_topic2   \n",
       "3   China, India, Africa, Europe, Germany, Greece,...   344  Topic_words   \n",
       "4                                               World   344   Flan_topic   \n",
       "..                                                ...   ...          ...   \n",
       "84                                       Science/Tech   372   Flan_topic   \n",
       "85                                      film industry   372  Flan_topic2   \n",
       "86  ubuntu, linux, package, firebox, manager, grub...   396  Topic_words   \n",
       "87                                              Linux   396   Flan_topic   \n",
       "88                                       Science/Tech   396  Flan_topic2   \n",
       "\n",
       "   dataset docnum docid wordct  \n",
       "0       cc     12  2276    290  \n",
       "1       cc     12  2276    290  \n",
       "2       cc     12  2276    290  \n",
       "3       cc     12  2276    290  \n",
       "4       cc     12  2276    290  \n",
       "..     ...    ...   ...    ...  \n",
       "84      ud    249  6584    217  \n",
       "85      ud    249  6584    217  \n",
       "86      ud    249  6584    217  \n",
       "87      ud    249  6584    217  \n",
       "88      ud    249  6584    217  \n",
       "\n",
       "[89 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This section creates a csv to upload to mechanical turk\n",
    "\n",
    "# Set round #\n",
    "#     first round was without keyphrase extraction\n",
    "#     second round was with keyphrase extraction\n",
    "round = 2\n",
    "\n",
    "# Set whether this is the file to send to AWS or the one for my reference (which will have row IDs, etc)\n",
    "aws_version = True\n",
    "\n",
    "# Init result\n",
    "d = {'conversation': [], 'topic': [], 'rowid': [], 'setid': [], 'dataset': [], 'docnum': [], 'docid': [], 'wordct': []}\n",
    "\n",
    "# Iterate through files in dir\n",
    "for f in os.listdir(f\"{mturk_dir}round{round}\"):\n",
    "\n",
    "    # Read the file\n",
    "    print(f)\n",
    "    with open(f\"{mturk_dir}round{round}/{f}\", 'r') as fh:\n",
    "        s = fh.read()\n",
    "\n",
    "    # Read first row to get doc params - looks like this: dataset=cc, docnum=12, docid=2276, wordct=290\n",
    "    m = re.findall(r'^(.+?)\\r?\\n', s)\n",
    "    if len(m) != 1:\n",
    "        print('\\tstop!')\n",
    "        break\n",
    "\n",
    "    # Get doc params\n",
    "    mm = re.findall(r'dataset=(.+?), docnum=(\\d+), docid=(\\d+), wordct=(\\d+)', m[0])\n",
    "    if len(mm) == 0 or len(mm[0]) != 4:\n",
    "        print('\\tno doc params!')\n",
    "        break\n",
    "    dataset = mm[0][0]\n",
    "    docnum = mm[0][1]\n",
    "    docid = mm[0][2]\n",
    "    wordct = mm[0][3]\n",
    "\n",
    "    # Get conversation\n",
    "    conv = ''\n",
    "    m = re.findall(r'\\[--- conv start ---\\]\\r?\\n(.+?)\\r?\\n\\[--- conv end ---\\]', s, re.DOTALL)\n",
    "    if len(m) == 0:\n",
    "        print('\\tzero-length conversation!')\n",
    "        break\n",
    "    conv = m[0]\n",
    "    #conv = html.escape(conv).encode('ascii', 'xmlcharrefreplace')\n",
    "    conv = html.escape(conv)\n",
    "    conv = re.sub(r'\\r?\\n', '<br />', conv)\n",
    "    #print(conv)\n",
    "    #print()\n",
    "\n",
    "    # Read rows\n",
    "    m = re.findall(r'\\[--- row start ---\\]\\r?\\n(.+?)\\r?\\n\\[--- row end ---\\]', s, re.DOTALL)\n",
    "    if len(m) == 0:\n",
    "        print('\\tno rows!')\n",
    "        break\n",
    "\n",
    "    # Iterate over topic rows\n",
    "    for row in m:\n",
    "\n",
    "        mm = re.findall(r'^rowid (\\d+), (.+?)\\r?\\n\\[--- topic start ---\\]', row)\n",
    "        rowid = mm[0][0]\n",
    "        setid = mm[0][1]\n",
    "\n",
    "        # Read topics\n",
    "        mm = re.findall(r'\\[--- topic start ---\\](.+?)\\[--- topic end ---\\]', row)\n",
    "        if len(mm) == 0:\n",
    "            print('\\tno topics!')\n",
    "            continue\n",
    "\n",
    "        # Append topic to conversation\n",
    "        topic = mm[0]\n",
    "        #conv_to_add = conv + '<br /><br />Topic:&nbsp:' + html.escape(topic) + '<br /><br />'  # Uncomment to add the topic to the end of the conversation\n",
    "        conv_to_add = conv + '<br />'\n",
    "\n",
    "        # Add topic to result\n",
    "        #print(topic)\n",
    "        d['conversation'].append(conv_to_add)\n",
    "        d['topic'].append(topic)\n",
    "        d['rowid'].append(rowid)\n",
    "        d['setid'].append(setid)\n",
    "        d['dataset'].append(dataset)\n",
    "        d['docnum'].append(docnum)\n",
    "        d['docid'].append(docid)\n",
    "        d['wordct'].append(wordct)\n",
    "\n",
    "# Convert result dict to df and write to csv\n",
    "df = pd.DataFrame(d)\n",
    "display(df)\n",
    "if aws_version:\n",
    "    df[['conversation', 'topic']].to_csv(f'C:/Users/micha/Box Sync/cuny/698-Capstone/mturk_upload_round{round}.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "else:\n",
    "    df.to_csv(f'C:/Users/micha/Box Sync/cuny/698-Capstone/mturk_upload_round{round}_full.csv', index=False, quoting=csv.QUOTE_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9699c4b4-8257-4a72-b2bb-502c13687fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the topic number from a list or dict of topics\n",
    "def get_topic_num(all_topics, docnum):\n",
    "\n",
    "    # all_topics can either look like this:\n",
    "    # {0: [2, 10, 24, 25, 55, 66, 74, 80, 83, 92, 10...\n",
    "    # or like this:\n",
    "    # [{-1: [0, 1, 2, 3, 4, 5, 10, 11, 16, 17, 18, 2...\n",
    "    \n",
    "    # See if it's a list or a dict\n",
    "    if type(all_topics) == list:  # bertopic Doc_topics will be a list of dicts, with each dict having a single entry having a key equal to the topic number\n",
    "        tmp = {}\n",
    "        for d in all_topics:\n",
    "            for k in d.keys():\n",
    "                tmp[k] = d[k]\n",
    "        all_topics = tmp\n",
    "    topicnum = [topicnum for topicnum in all_topics.keys() if docnum in all_topics[topicnum]][0]\n",
    "    return topicnum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1a315-3dff-4fd9-a4aa-d32ac0744bc3",
   "metadata": {},
   "source": [
    "### Hand-Label Relevance Dataset\n",
    "\n",
    "This section prompts me with a conversation and a randomly selected topic representation that I can hand-label in preparation for topic relevance modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c1cbea6-ee64-4db3-9a77-0f820be39c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get topic representations\n",
    "def get_topic_repr(all_topics, colname, topicid):\n",
    "\n",
    "    # See if we're looking for the standard topic words or the flan topic word\n",
    "    if colname == 'Topic_words':\n",
    "        topic = [word[0] for word in [topic_tuple[1][:num_topic_words] for topic_tuple in all_topics if topic_tuple[0] == topicid][0]]\n",
    "        topic = ', '.join(topic)\n",
    "    elif colname == 'Flan_topic' or colname == 'Flan_topic2':\n",
    "        all_topics = json.loads(all_topics)\n",
    "        try:\n",
    "            topic = [topic[1] for topic in all_topics if topic[0] == topicid][0]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(colname, topicid)\n",
    "            print(all_topics)\n",
    "            topic = ''\n",
    "\n",
    "    # Return\n",
    "    if type(topic) == list and len(topic) == 0:\n",
    "        topic = ''\n",
    "    return topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e400d6e-3392-4283-a50c-778468520b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ee': {'70': {'72': {'Topic_words': 3, 'Flan_topic': 1, 'Flan_topic2': 1}}, '235': {'242': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 3}, '75': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '262': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '278': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '281': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '252': {'Topic_words': 1, 'Flan_topic': 3, 'Flan_topic2': 3}, '253': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '90': {'254': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '248': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '256': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '280': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '245': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 2}, '313': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 1}, '269': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '275': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}}, '81': {'263': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 2}, '273': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '250': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '287': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '314': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '244': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '254': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '312': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '85': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '313': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 2}, '87': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '283': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '280': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '274': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '253': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '112': {'84': {'Topic_words': 3, 'Flan_topic': 3, 'Flan_topic2': 3}, '90': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '81': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '263': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 3}, '270': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '281': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '262': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '313': {'Topic_words': 3, 'Flan_topic': 3, 'Flan_topic2': 1}}, '60': {'89': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '240': {'Topic_words': 2, 'Flan_topic': 3, 'Flan_topic2': 2}, '251': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '255': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 1}, '275': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '257': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '87': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '242': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 3}}}, 'cc': {'15': {'21': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '132': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '109': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '136': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '189': {'133': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '124': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '289': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '113': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '5': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '293': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 2}, '291': {'Topic_words': 2, 'Flan_topic': 3, 'Flan_topic2': 2}, '111': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '183': {'295': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '132': {'Topic_words': 4, 'Flan_topic': 1, 'Flan_topic2': 4}, '130': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '138': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '110': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '10': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '18': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '2': {'Topic_words': 2, 'Flan_topic': 3, 'Flan_topic2': 1}}, '28': {'130': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 2}, '101': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '108': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '126': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '100': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '134': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 5}, '14': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '123': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '168': {'121': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '113': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '108': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '123': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '98': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '19': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '125': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '295': {'Topic_words': 3, 'Flan_topic': 3, 'Flan_topic2': 2}}, '76': {'12': {'Topic_words': 5, 'Flan_topic': 2, 'Flan_topic2': 1}, '99': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '127': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '139': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '115': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '134': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '112': {'Topic_words': 3, 'Flan_topic': 3, 'Flan_topic2': 4}, '8': {'Topic_words': 3, 'Flan_topic': 5, 'Flan_topic2': 4}}, '190': {'4': {'Topic_words': 2, 'Flan_topic': 3, 'Flan_topic2': 1}, '140': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '135': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '7': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '14': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '288': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '127': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}}, 'ud': {'246': {'50': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '56': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 1}, '60': {'Topic_words': 2, 'Flan_topic': 4, 'Flan_topic2': 1}, '238': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '227': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '234': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '212': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '228': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}}, '107': {'210': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '50': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '197': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '200': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '233': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '58': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '211': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '179': {'51': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '58': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '66': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '221': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '239': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '57': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '213': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '230': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}}, 'tc': {'35': {'166': {'Topic_words': 3, 'Flan_topic': 1, 'Flan_topic2': 3}, '161': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 1}, '180': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 3}, '45': {'Topic_words': 2, 'Flan_topic': 2, 'Flan_topic2': 2}, '152': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '35': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 2}, '167': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}, '172': {'Topic_words': 1, 'Flan_topic': 2, 'Flan_topic2': 2}}, '76': {'26': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '301': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '163': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '146': {'Topic_words': 3, 'Flan_topic': 1, 'Flan_topic2': 4}, '178': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '42': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '165': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 3}, '161': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}}, '43': {'150': {'Topic_words': 4, 'Flan_topic': 1, 'Flan_topic2': 1}, '165': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '181': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '162': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '300': {'Topic_words': 5, 'Flan_topic': 5, 'Flan_topic2': 5}, '25': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '24': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '180': {'Topic_words': 2, 'Flan_topic': 1, 'Flan_topic2': 1}}, '242': {'298': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '46': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '300': {'Topic_words': 3, 'Flan_topic': 1, 'Flan_topic2': 2}, '164': {'Topic_words': 3, 'Flan_topic': 1, 'Flan_topic2': 4}, '47': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '166': {'Topic_words': 1, 'Flan_topic': 1, 'Flan_topic2': 1}, '24': {'Topic_words': 1, 'Flan_topic': 3, 'Flan_topic2': 1}}}}\n"
     ]
    }
   ],
   "source": [
    "# Init resultset for hand-labeling of topic relevance\n",
    "d = {}\n",
    "with open('relevance.json', 'r') as fh:\n",
    "    d = json.load(fh)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f47b8-8b6c-4499-880e-6483a2828c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section prompts me to label a conversation as relevant or not\n",
    "\n",
    "# Iterate until I get sick of it\n",
    "stop = False\n",
    "while stop == False:\n",
    "\n",
    "    # Select document at random\n",
    "    ds = random.sample(list(datasetmap.keys()), 1)[0]\n",
    "    docnum = int(random.randint(0, 249))\n",
    "    docid = int(docmap[datasetmap[ds]][docnum])\n",
    "\n",
    "    # Temp\n",
    "    if ds != 'ee': continue\n",
    "\n",
    "    # Get handle to dataframe\n",
    "    dfconv = eval(f\"df{ds}\")\n",
    "    conv = dfconv.loc[docid, 'conv']\n",
    "\n",
    "    # Skip long conversations\n",
    "    wordct = len(re.findall(r'\\W', conv))\n",
    "    if wordct > 500:\n",
    "        continue\n",
    "\n",
    "    # Do multiple topic sets with the same document to be more efficient\n",
    "    i = 0\n",
    "    while i < 8:\n",
    "\n",
    "        # Select row at random\n",
    "        #row = dfr.sample(1)\n",
    "        row = dfr[dfr['index']>=320].sample(1)  # only rows with keyphrase extraction\n",
    "        #rowid = int(row.index[0])\n",
    "        rowid = int(row['index'])\n",
    "\n",
    "        # Make sure this row corresponds to the dataset\n",
    "        if row['Dataset'].values[0] != datasetmap[ds]:\n",
    "            continue\n",
    "\n",
    "        # Get topic number of this docnum\n",
    "        topicid = int(get_topic_num(row['Doc_topics'].values[0], docnum))\n",
    "        \n",
    "        # Get topic representations for each topic type\n",
    "        print(f\"row {rowid}, topicid {topicid}\")\n",
    "        for colname in ['Topic_words', 'Flan_topic', 'Flan_topic2']:\n",
    "    \n",
    "            # Get topic representation\n",
    "            print(f\"before topicid {topicid}\")\n",
    "            topic = get_topic_repr(row[colname].values[0], colname, topicid)\n",
    "            print(f\"after topicid {topicid}\")\n",
    "\n",
    "            # Print\n",
    "            if colname == 'Topic_words':\n",
    "                clear_output()\n",
    "                print(\"**************************************************************************************************\")\n",
    "                print(f\"dataset {ds}, docnum {docnum}, docid {docid}, topicid {topicid}\")\n",
    "                print()\n",
    "                print(conv)\n",
    "                print()\n",
    "\n",
    "            # Make sure we haven't recorded this answer yet\n",
    "            if ds in d.keys() and docnum in d[ds].keys() and rowid in d[ds][docnum].keys() and colname in d[ds][docnum][rowid].keys():\n",
    "                print(\"already done:\")\n",
    "                print(f\"dataset {ds}, docnum {docnum}, docid {docid}, topicid {topicid}\")\n",
    "                print(f\"row {rowid}, column {colname}, topic {topicid}: {topic}\")\n",
    "                print()\n",
    "                continue\n",
    "    \n",
    "            # Always print out the topic\n",
    "            print(f\"row {rowid}, column {colname}, topic {topicid}:\\n{topic}\")\n",
    "            print()\n",
    "\n",
    "            # Get user input\n",
    "            time.sleep(0.25)\n",
    "            yn = input(\"Relevance? (1-5/c/s)\")\n",
    "            if yn == 'c':  # cancel\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "            # See if we're skipping this one\n",
    "            if yn != 's':  # skip\n",
    "\n",
    "                # Update dict\n",
    "                if ds not in d.keys(): d[ds] = {}\n",
    "                if docnum not in d[ds].keys(): d[ds][docnum] = {}\n",
    "                if rowid not in d[ds][docnum].keys(): d[ds][docnum][rowid] = {}\n",
    "                #if colname not in d[ds][docnum][rowid].keys():\n",
    "                d[ds][docnum][rowid][colname] = int(yn)\n",
    "\n",
    "        # Increment counter\n",
    "        i += 1\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "print(d)\n",
    "with open('relevance.json', 'w') as fh:\n",
    "    fh.write(json.dumps(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e44c8-dbe4-43d6-a190-85fb7e569eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert int64s to ints\n",
    "dnew = {}\n",
    "for ds in d.keys():\n",
    "    #print(f\"dataset {ds}\")\n",
    "    if ds not in dnew.keys(): dnew[ds] = {}\n",
    "    for docnum in d[ds].keys():\n",
    "        #print(f\"\\tdocunum {docnum}, {type(docnum)}\")\n",
    "        newdocnum = int(docnum)\n",
    "        if newdocnum not in dnew[ds].keys(): dnew[ds][newdocnum] = {}\n",
    "        for rowid in d[ds][docnum].keys():\n",
    "            #print(f\"\\t\\trowid {rowid}\")\n",
    "            newrowid = int(rowid)\n",
    "            if newrowid not in dnew[ds][newdocnum].keys(): dnew[ds][newdocnum][newrowid] = {}\n",
    "            for colname in d[ds][docnum][rowid].keys():\n",
    "                #print(f\"\\t\\t\\tcolname {colname}\")\n",
    "                if colname not in dnew[ds][newdocnum].keys(): dnew[ds][newdocnum][newrowid][colname] = {}\n",
    "                if ds in d.keys() and docnum in d[ds].keys() and rowid in d[ds][docnum].keys() and colname in d[ds][docnum][rowid].keys():\n",
    "                    result = int(d[ds][docnum][rowid][colname])\n",
    "                    dnew[ds][newdocnum][newrowid][colname] = result\n",
    "\n",
    "print(json.dumps(dnew))\n",
    "\n",
    "with open('relevance_new.json', 'w') as fh:\n",
    "    fh.write(json.dumps(dnew))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8de1ad-7123-4012-aa3f-2fb719dffb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section adds the hand-labeled quality result to the relevance df\n",
    "\n",
    "# Walk resultset\n",
    "ct = 0\n",
    "cty = 0\n",
    "ctn = 0\n",
    "d2 = {'dataset': [], 'docnum': [], 'rowid': [], 'colname': [], 'result': []}\n",
    "for ds in d.keys():\n",
    "    #print(f\"dataset {ds}\")\n",
    "    for docnum in d[ds].keys():\n",
    "        #print(f\"\\tdocunum {docnum}\")\n",
    "        newdocnum = int(docnum)\n",
    "        for rowid in d[ds][docnum].keys():\n",
    "            #print(f\"\\t\\trowid {rowid}\")\n",
    "            newrowid = int(rowid)\n",
    "            for colname in d[ds][docnum][rowid].keys():\n",
    "                #print(f\"\\t\\t\\tcolname {colname}\")\n",
    "                if ds in d.keys() and docnum in d[ds].keys() and rowid in d[ds][docnum].keys() and colname in d[ds][docnum][rowid].keys():\n",
    "                    result = int(d[ds][docnum][rowid][colname])\n",
    "                    #print(f\"result: {result} (dataset {ds}, docunum {docnum}, rowid {rowid}, colname {colname})\")\n",
    "                    d2['dataset'].append(ds)\n",
    "                    d2['docnum'].append(newdocnum)\n",
    "                    d2['rowid'].append(newrowid)\n",
    "                    d2['colname'].append(colname)\n",
    "                    d2['result'].append(result)\n",
    "                    ct += 1\n",
    "                    if result > 2:\n",
    "                        cty += 1\n",
    "                    else:\n",
    "                        ctn += 1\n",
    "\n",
    "# Print summary\n",
    "print()\n",
    "print(f\"ct={ct}, cty={cty}, ctn={ctn}\")\n",
    "print()\n",
    "\n",
    "print(d2)\n",
    "\n",
    "# Create dataframe\n",
    "dfrel = pd.DataFrame(d2)\n",
    "display(dfrel)\n",
    "timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "dfrel.to_pickle('C:\\\\tmp\\\\pickles\\\\relevance\\\\dfrel_' + timestamp + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "adef7689-8744-4ed7-b9b9-e129cdd197eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(104, 319):\\n    clear_output(wait=True)\\n    print(i, dfr.loc[dfr.index == i, 'Dataset'].values)\\n    print()\\n    print(dfr.loc[dfr.index == i, 'Doc_topics'].values)\\n    print()\\n    print(dfr.loc[dfr.index == i, 'Flan_topic'].values)\\n    print()\\n    print(dfr.loc[dfr.index == i, 'Flan_topic2'].values)\\n    print()\\n    print(dfr.loc[dfr.index == i, 'Topic_words'].values[0])\\n    print()\\n    time.sleep(0.25)\\n    input('here')\\n\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This was used for troubleshooting flan topics - can be ignored now that it is fixed\n",
    "\"\"\"\n",
    "for i in range(104, 319):\n",
    "    clear_output(wait=True)\n",
    "    print(i, dfr.loc[dfr.index == i, 'Dataset'].values)\n",
    "    print()\n",
    "    print(dfr.loc[dfr.index == i, 'Doc_topics'].values)\n",
    "    print()\n",
    "    print(dfr.loc[dfr.index == i, 'Flan_topic'].values)\n",
    "    print()\n",
    "    print(dfr.loc[dfr.index == i, 'Flan_topic2'].values)\n",
    "    print()\n",
    "    print(dfr.loc[dfr.index == i, 'Topic_words'].values[0])\n",
    "    print()\n",
    "    time.sleep(0.25)\n",
    "    input('here')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "33ebf8fb-b86d-4a31-9a94-e68a38980844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Init dict to store new irrelevant doc data\\ndirr = {'Dataset': [], 'Doc_id': [], 'Snippet': [], 'Topic_words': [], 'Human_topic': [], 'Relevant': []}\\n\\n# Iterate over keyword df\\nfor i, row in dfkw.iterrows():\\n\\n    dirr['Dataset'].append(row['Dataset'])\\n    dirr['Doc_id'].append(row['irr_id1'])\\n    dirr['Snippet'].append('')\\n    dirr['Topic_words'].append(row['Topic_words'])\\n    dirr['Human_topic'].append('')\\n    dirr['Relevant'].append(0)\\n\\ndftmp = pd.DataFrame(dirr)\\ndftmp.to_csv('tmpkw.csv', index=False)\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This section adds irrelevant documents to the keyword dataframe;\n",
    "# not needed anymore because it's all done.\n",
    "\n",
    "\"\"\"\n",
    "# Init dict to store new irrelevant doc data\n",
    "dirr = {'Dataset': [], 'Doc_id': [], 'Snippet': [], 'Topic_words': [], 'Human_topic': [], 'Relevant': []}\n",
    "\n",
    "# Iterate over keyword df\n",
    "for i, row in dfkw.iterrows():\n",
    "\n",
    "    dirr['Dataset'].append(row['Dataset'])\n",
    "    dirr['Doc_id'].append(row['irr_id1'])\n",
    "    dirr['Snippet'].append('')\n",
    "    dirr['Topic_words'].append(row['Topic_words'])\n",
    "    dirr['Human_topic'].append('')\n",
    "    dirr['Relevant'].append(0)\n",
    "\n",
    "dftmp = pd.DataFrame(dirr)\n",
    "dftmp.to_csv('tmpkw.csv', index=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357d257-4392-4117-86c6-851499bb68e4",
   "metadata": {},
   "source": [
    "### Parse Survey Results\n",
    "\n",
    "This section reads the results of the mechanical turk survey and incorporates them into the hand-labeled relevance dataframe. The code in survey2.ipynb should be run before proceeding with this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "43add8d2-fbde-400c-acae-31dee3c838a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>HITId</th>\n",
       "      <th>HITTypeId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Reward</th>\n",
       "      <th>CreationTime</th>\n",
       "      <th>MaxAssignments</th>\n",
       "      <th>RequesterAnnotation</th>\n",
       "      <th>...</th>\n",
       "      <th>Answer.relevance.label</th>\n",
       "      <th>Approve</th>\n",
       "      <th>Reject</th>\n",
       "      <th>rowid</th>\n",
       "      <th>setid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>docnum</th>\n",
       "      <th>docid</th>\n",
       "      <th>wordct</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3XEIP58NMJ7N4WYIKUQFPJFMW7SZLF</td>\n",
       "      <td>3U2Q8YJASS9X6MVGG7Q77MB02CJT5L</td>\n",
       "      <td>How relevant is the conversation to the given ...</td>\n",
       "      <td>How relevant is the conversation to the given ...</td>\n",
       "      <td>text, conversation, relevance</td>\n",
       "      <td>$0.22</td>\n",
       "      <td>Thu Apr 18 11:35:34 PDT 2024</td>\n",
       "      <td>4</td>\n",
       "      <td>BatchId:5211639;OriginalHitTemplateId:928390850;</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9999</td>\n",
       "      <td>Human_keywords</td>\n",
       "      <td>cc</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           HITId                       HITTypeId  \\\n",
       "0      1  3XEIP58NMJ7N4WYIKUQFPJFMW7SZLF  3U2Q8YJASS9X6MVGG7Q77MB02CJT5L   \n",
       "\n",
       "                                               Title  \\\n",
       "0  How relevant is the conversation to the given ...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  How relevant is the conversation to the given ...   \n",
       "\n",
       "                        Keywords Reward                  CreationTime  \\\n",
       "0  text, conversation, relevance  $0.22  Thu Apr 18 11:35:34 PDT 2024   \n",
       "\n",
       "   MaxAssignments                               RequesterAnnotation  ...  \\\n",
       "0               4  BatchId:5211639;OriginalHitTemplateId:928390850;  ...   \n",
       "\n",
       "   Answer.relevance.label  Approve Reject  rowid           setid dataset  \\\n",
       "0                       4      NaN    NaN   9999  Human_keywords      cc   \n",
       "\n",
       "  docnum   docid wordct quality  \n",
       "0   12.0  2276.0  290.0    good  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward', 'CreationTime', 'MaxAssignments', 'RequesterAnnotation', 'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds', 'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds', 'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime', 'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime', 'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate', 'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.conversation', 'Input.topic', 'Answer.relevance.label', 'Approve', 'Reject', 'rowid', 'setid', 'dataset', 'docnum', 'docid', 'wordct', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Load survey results - these were saved in survey2.ipynb\n",
    "dfsr = pd.read_pickle(f\"{capstone_dir}/mturk_results.pkl\")\n",
    "print(dfsr.shape)\n",
    "display(dfsr.head(1))\n",
    "print(list(dfsr.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50937e9c-a1e6-4f04-a211-e905eed3900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Human_topic</th>\n",
       "      <th>Relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>i never turn mine off anyways updates would ju...</td>\n",
       "      <td>browser,college,major,BYU,student,media,marketing</td>\n",
       "      <td>traveling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  Doc_id                                            Snippet  \\\n",
       "0  chitchat  1035.0  i never turn mine off anyways updates would ju...   \n",
       "\n",
       "                                         Topic_words Human_topic  Relevant  \n",
       "0  browser,college,major,BYU,student,media,marketing   traveling         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load keywords\n",
    "dfkw = pd.read_excel('C:/Users/micha/Box Sync/cuny/698-Capstone/keywords.xlsx', sheet_name='human')\n",
    "print(dfkw.shape)\n",
    "display(dfkw.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "04c245ad-10ba-494f-b448-8414e948d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>docnum</th>\n",
       "      <th>rowid</th>\n",
       "      <th>colname</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ee</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  docnum  rowid      colname  result\n",
       "0      ee      70     72  Topic_words       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load relevance results\n",
    "dfrel = load_last_pickle('relevance')\n",
    "print(dfrel.shape)\n",
    "display(dfrel.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7f95286d-f39f-4070-a8e4-245e74f6b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Num_docs</th>\n",
       "      <th>Rnd_seed</th>\n",
       "      <th>Model</th>\n",
       "      <th>Num_topics</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cv_score</th>\n",
       "      <th>Cuci_score</th>\n",
       "      <th>Cnpmi_score</th>\n",
       "      <th>...</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Doc_topics</th>\n",
       "      <th>Flan_topic</th>\n",
       "      <th>Cosine_similarity</th>\n",
       "      <th>Model_family</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Flan_topic2</th>\n",
       "      <th>Keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>250</td>\n",
       "      <td>77</td>\n",
       "      <td>lsi</td>\n",
       "      <td>8</td>\n",
       "      <td>{'num_topics': 15}</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>-12.5412</td>\n",
       "      <td>-0.44286</td>\n",
       "      <td>...</td>\n",
       "      <td>77.495494</td>\n",
       "      <td>20240401_133320</td>\n",
       "      <td>[(0, [('tear', 0.1492861747332724), ('eye', 0....</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>[[0, \"People\"], [1, \"People\"], [2, \"Science/Te...</td>\n",
       "      <td>{\"0\": [[0, 0.19291281700134277], [1, 0.0759510...</td>\n",
       "      <td>bow</td>\n",
       "      <td>[[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...</td>\n",
       "      <td>[[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Dataset Num_docs Rnd_seed Model Num_topics        Model_params  \\\n",
       "0      0  chitchat      250       77   lsi          8  {'num_topics': 15}   \n",
       "\n",
       "   Cv_score  Cuci_score  Cnpmi_score  ...    Runtime        Timestamp  \\\n",
       "0  0.256103    -12.5412     -0.44286  ...  77.495494  20240401_133320   \n",
       "\n",
       "                                         Topic_words  \\\n",
       "0  [(0, [('tear', 0.1492861747332724), ('eye', 0....   \n",
       "\n",
       "                                          Doc_topics  \\\n",
       "0  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "\n",
       "                                          Flan_topic  \\\n",
       "0  [[0, \"People\"], [1, \"People\"], [2, \"Science/Te...   \n",
       "\n",
       "                                   Cosine_similarity Model_family  \\\n",
       "0  {\"0\": [[0, 0.19291281700134277], [1, 0.0759510...          bow   \n",
       "\n",
       "                                        Topic_words2  \\\n",
       "0  [[0, [[\"job\", 0], [\"work\", 0], [\"eros\", 0], [\"...   \n",
       "\n",
       "                                         Flan_topic2 Keyphrases  \n",
       "0  [[0, \"eros\"], [1, \"eros\"], [2, \"Brazil\"], [3, ...      False  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load modeling results\n",
    "dfr = load_last_pickle('sim')\n",
    "print(dfr.shape)\n",
    "display(dfr.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ce47c-2a20-48d9-b7cd-17d31e421000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read document map from disk - this maps a document number (0-249) to a document id within a specific dataset\n",
    "with open(f\"{capstone_dir}/docmap.json\", 'r') as fh:\n",
    "    docmap = json.load(fh)\n",
    "datasetmap = {'cc': 'chitchat', 'tc': 'topical chat', 'ud': 'ubuntu dialogue', 'ee': 'enron email'}\n",
    "print(docmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "760b9a8a-5069-450d-98f9-890b66ba1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance\n",
      "(456, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>docnum</th>\n",
       "      <th>rowid</th>\n",
       "      <th>colname</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ee</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  docnum  rowid      colname  result\n",
       "0      ee      70     72  Topic_words       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dataset', 'docnum', 'rowid', 'colname', 'result'], dtype='object')\n",
      "\n",
      "survey results\n",
      "(891, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>HITId</th>\n",
       "      <th>HITTypeId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Reward</th>\n",
       "      <th>CreationTime</th>\n",
       "      <th>MaxAssignments</th>\n",
       "      <th>RequesterAnnotation</th>\n",
       "      <th>...</th>\n",
       "      <th>Answer.relevance.label</th>\n",
       "      <th>Approve</th>\n",
       "      <th>Reject</th>\n",
       "      <th>rowid</th>\n",
       "      <th>setid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>docnum</th>\n",
       "      <th>docid</th>\n",
       "      <th>wordct</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3XEIP58NMJ7N4WYIKUQFPJFMW7SZLF</td>\n",
       "      <td>3U2Q8YJASS9X6MVGG7Q77MB02CJT5L</td>\n",
       "      <td>How relevant is the conversation to the given ...</td>\n",
       "      <td>How relevant is the conversation to the given ...</td>\n",
       "      <td>text, conversation, relevance</td>\n",
       "      <td>$0.22</td>\n",
       "      <td>Thu Apr 18 11:35:34 PDT 2024</td>\n",
       "      <td>4</td>\n",
       "      <td>BatchId:5211639;OriginalHitTemplateId:928390850;</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9999</td>\n",
       "      <td>Human_keywords</td>\n",
       "      <td>cc</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           HITId                       HITTypeId  \\\n",
       "0      1  3XEIP58NMJ7N4WYIKUQFPJFMW7SZLF  3U2Q8YJASS9X6MVGG7Q77MB02CJT5L   \n",
       "\n",
       "                                               Title  \\\n",
       "0  How relevant is the conversation to the given ...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  How relevant is the conversation to the given ...   \n",
       "\n",
       "                        Keywords Reward                  CreationTime  \\\n",
       "0  text, conversation, relevance  $0.22  Thu Apr 18 11:35:34 PDT 2024   \n",
       "\n",
       "   MaxAssignments                               RequesterAnnotation  ...  \\\n",
       "0               4  BatchId:5211639;OriginalHitTemplateId:928390850;  ...   \n",
       "\n",
       "   Answer.relevance.label  Approve Reject  rowid           setid dataset  \\\n",
       "0                       4      NaN    NaN   9999  Human_keywords      cc   \n",
       "\n",
       "  docnum   docid wordct quality  \n",
       "0   12.0  2276.0  290.0    good  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'HITId', 'HITTypeId', 'Title', 'Description', 'Keywords',\n",
      "       'Reward', 'CreationTime', 'MaxAssignments', 'RequesterAnnotation',\n",
      "       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',\n",
      "       'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds',\n",
      "       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',\n",
      "       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',\n",
      "       'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate',\n",
      "       'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.conversation',\n",
      "       'Input.topic', 'Answer.relevance.label', 'Approve', 'Reject', 'rowid',\n",
      "       'setid', 'dataset', 'docnum', 'docid', 'wordct', 'quality'],\n",
      "      dtype='object')\n",
      "\n",
      "keywords\n",
      "(168, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Topic_words</th>\n",
       "      <th>Human_topic</th>\n",
       "      <th>Relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chitchat</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>i never turn mine off anyways updates would ju...</td>\n",
       "      <td>browser,college,major,BYU,student,media,marketing</td>\n",
       "      <td>traveling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  Doc_id                                            Snippet  \\\n",
       "0  chitchat  1035.0  i never turn mine off anyways updates would ju...   \n",
       "\n",
       "                                         Topic_words Human_topic  Relevant  \n",
       "0  browser,college,major,BYU,student,media,marketing   traveling         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dataset', 'Doc_id', 'Snippet', 'Topic_words', 'Human_topic',\n",
      "       'Relevant'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data prep for semantic quality and relevance\n",
    "\n",
    "# First look\n",
    "print('relevance')\n",
    "print(dfrel.shape)\n",
    "display(dfrel.head(1))\n",
    "print(dfrel.columns)\n",
    "print()\n",
    "print('survey results')\n",
    "print(dfsr.shape)\n",
    "display(dfsr.head(1))\n",
    "print(dfsr.columns)\n",
    "print()\n",
    "print('keywords')\n",
    "print(dfkw.shape)\n",
    "display(dfkw.head(1))\n",
    "print(dfkw.columns)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3c843e3e-2cc4-43ba-af77-e9944aa540d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 7)\n",
      "(891, 9)\n",
      "(1347, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Doc_num</th>\n",
       "      <th>Row_id</th>\n",
       "      <th>Set_id</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Source</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Conv2</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ee</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>3</td>\n",
       "      <td>Author</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ee</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>1</td>\n",
       "      <td>Author</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ee</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>1</td>\n",
       "      <td>Author</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ee</td>\n",
       "      <td>235.0</td>\n",
       "      <td>242</td>\n",
       "      <td>Topic_words</td>\n",
       "      <td>1</td>\n",
       "      <td>Author</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ee</td>\n",
       "      <td>235.0</td>\n",
       "      <td>242</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>1</td>\n",
       "      <td>Author</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Doc_num Row_id       Set_id  Quality  Source  Doc_id Conv2  \\\n",
       "0      ee     70.0     72  Topic_words        3  Author   287.0   NaN   \n",
       "1      ee     70.0     72   Flan_topic        1  Author   287.0   NaN   \n",
       "2      ee     70.0     72  Flan_topic2        1  Author   287.0   NaN   \n",
       "3      ee    235.0    242  Topic_words        1  Author  1881.0   NaN   \n",
       "4      ee    235.0    242   Flan_topic        1  Author  1881.0   NaN   \n",
       "\n",
       "  Topic_words2  Relevant  \n",
       "0          NaN       1.0  \n",
       "1          NaN       0.0  \n",
       "2          NaN       0.0  \n",
       "3          NaN       0.0  \n",
       "4          NaN       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Doc_num</th>\n",
       "      <th>Row_id</th>\n",
       "      <th>Set_id</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Source</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Conv2</th>\n",
       "      <th>Topic_words2</th>\n",
       "      <th>Relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>ud</td>\n",
       "      <td>249.0</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>3</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>6584.0</td>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Linux</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>ud</td>\n",
       "      <td>249.0</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic</td>\n",
       "      <td>5</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>6584.0</td>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Linux</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>ud</td>\n",
       "      <td>249.0</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>3</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>6584.0</td>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Science/Tech</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>ud</td>\n",
       "      <td>249.0</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>4</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>6584.0</td>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Science/Tech</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>ud</td>\n",
       "      <td>249.0</td>\n",
       "      <td>396</td>\n",
       "      <td>Flan_topic2</td>\n",
       "      <td>4</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>6584.0</td>\n",
       "      <td>aanonymouss: In order to run VNC (like vino, r...</td>\n",
       "      <td>Science/Tech</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  Doc_num Row_id       Set_id  Quality Source  Doc_id  \\\n",
       "1342      ud    249.0    396   Flan_topic        3  MTurk  6584.0   \n",
       "1343      ud    249.0    396   Flan_topic        5  MTurk  6584.0   \n",
       "1344      ud    249.0    396  Flan_topic2        3  MTurk  6584.0   \n",
       "1345      ud    249.0    396  Flan_topic2        4  MTurk  6584.0   \n",
       "1346      ud    249.0    396  Flan_topic2        4  MTurk  6584.0   \n",
       "\n",
       "                                                  Conv2  Topic_words2  \\\n",
       "1342  aanonymouss: In order to run VNC (like vino, r...         Linux   \n",
       "1343  aanonymouss: In order to run VNC (like vino, r...         Linux   \n",
       "1344  aanonymouss: In order to run VNC (like vino, r...  Science/Tech   \n",
       "1345  aanonymouss: In order to run VNC (like vino, r...  Science/Tech   \n",
       "1346  aanonymouss: In order to run VNC (like vino, r...  Science/Tech   \n",
       "\n",
       "      Relevant  \n",
       "1342       1.0  \n",
       "1343       1.0  \n",
       "1344       1.0  \n",
       "1345       1.0  \n",
       "1346       1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This section creates a full labeled data set using both the hand-labeled relevance data with the survey result data\n",
    "\n",
    "# Change column names so that we can merge the hand-labeled relevance df with the survey result df\n",
    "dfrel.rename(columns={'dataset': 'Dataset', 'docnum': 'Doc_num', 'rowid': 'Row_id', 'colname': 'Set_id', 'result': 'Quality'}, inplace=True)\n",
    "dfsr2 = dfsr[['dataset', 'docnum', 'docid', 'rowid', 'setid', 'Answer.relevance.label', 'Input.conversation', 'Input.topic']].copy()\n",
    "dfsr2.rename(columns={'dataset': 'Dataset', 'docnum': 'Doc_num', 'docid': 'Doc_id', 'rowid': 'Row_id', 'setid': 'Set_id', \\\n",
    "                      'Answer.relevance.label': 'Quality', 'Input.conversation': 'Conv2', 'Input.topic': 'Topic_words2'}, inplace=True)\n",
    "\n",
    "# Label result source as either hand-labeled or mturk\n",
    "dfrel['Source'] = 'Author'\n",
    "dfsr2['Source'] = 'MTurk'\n",
    "\n",
    "# Lookup docid in the hand-labeled relevance df (the mturk df already has the doc id)\n",
    "for i, row in dfrel.iterrows():\n",
    "    dfrel.at[i, 'Doc_id'] = docmap[datasetmap[row['Dataset']]][str(int(row['Doc_num']))]\n",
    "print(dfrel.shape)\n",
    "print(dfsr2.shape)\n",
    "\n",
    "# Add temp columns to dfrel; this is just added as a sanity check to make sure the convos and topic words line up\n",
    "dfrel['Conv2'] = pd.NA\n",
    "dfrel['Topic_words2'] = pd.NA\n",
    "\n",
    "# Merge\n",
    "dfrel2 = pd.concat([dfrel, dfsr2], ignore_index=True)\n",
    "\n",
    "# Set relevant or not\n",
    "dfrel2.loc[dfrel2['Quality'] > 2, 'Relevant'] = 1\n",
    "dfrel2.loc[dfrel2['Quality'] < 3, 'Relevant'] = 0\n",
    "\n",
    "# Verify\n",
    "print(dfrel2.shape)\n",
    "display(dfrel2.head())\n",
    "display(dfrel2.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fcd25c1b-b13b-472e-8ff9-76904a9b5b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Iterate over relevance results to get topic id\n",
    "for i, row in dfrel2.iterrows():\n",
    "\n",
    "    # Get conv\n",
    "    df = eval(f\"df{row['Dataset']}\")\n",
    "    conv = df.loc[df['index'] == row['Doc_id'], 'conv'].values[0]\n",
    "    conv = conv.replace('\\n\\n', '\\n')\n",
    "\n",
    "    # Get doc topics\n",
    "    if row['Set_id'] == 'Human_keywords':\n",
    "\n",
    "        # Find topic words in the keywords df\n",
    "        topicid = 999\n",
    "        topic_words = dfkw.loc[(dfkw['Dataset']==datasetmap[row['Dataset']]) & (dfkw['Doc_id']==row['Doc_id']), 'Topic_words'].values[0]\n",
    "        cs = pd.NA  # cosine sim\n",
    "        \n",
    "    elif row['Set_id'] == 'Human_friendly_topic':\n",
    "\n",
    "        # Find human topics in the keywords df\n",
    "        topicid = 9999\n",
    "        topic_words = dfkw.loc[(dfkw['Dataset']==datasetmap[row['Dataset']]) & (dfkw['Doc_id']==row['Doc_id']), 'Human_topic'].values[0]\n",
    "        cs = pd.NA  # cosine sim\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Get row in modeling results\n",
    "        row2 = dfr[dfr.index==row['Row_id']]\n",
    "\n",
    "        # Find the topic id based on the document number\n",
    "        topicid = get_topic_num(row2['Doc_topics'].values[0], row['Doc_num'])\n",
    "\n",
    "        # Find the topic words for this topic id\n",
    "        topic_words = get_topic_repr(row2[row['Set_id']].values[0], row['Set_id'], topicid)\n",
    "        topic_words = topic_words.replace(', ', ',')\n",
    "\n",
    "        # Cosine similarity\n",
    "        all_cos = row2['Cosine_similarity'].values[0]\n",
    "        if pd.isna(all_cos):\n",
    "            all_cos = {}\n",
    "        else:\n",
    "            all_cos = json.loads(all_cos)  # cos sim for all topics and all docs\n",
    "        #print(f\"dataset {row['Dataset']}, docnum {row['Doc_num']}, docid {row['Doc_id']}, setid {row['Set_id']}, topicid {topicid}, rowid {row['Row_id']}\")\n",
    "        topic_cos = all_cos[str(int(topicid))]  # cos sim for all docs with this topic id\n",
    "        if row['Row_id'] == 999:\n",
    "            print(topic_words)\n",
    "            print()\n",
    "            print(all_cos)\n",
    "            print()\n",
    "            print(topicid, topic_cos)\n",
    "            print()\n",
    "            print(row)\n",
    "            print()\n",
    "            print(row2)\n",
    "            print()\n",
    "        cs = [cs[1] for cs in topic_cos if int(cs[0]) == int(row['Doc_num'])][0]\n",
    "\n",
    "    # Set values\n",
    "    dfrel2.at[i, 'Topic_words'] = topic_words\n",
    "    dfrel2.at[i, 'Conv'] = conv\n",
    "    dfrel2.at[i, 'Cosine_similarity'] = cs\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c8ed7351-a11c-41da-88a2-e7b616d32dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns; I added these just as a sanity check to make sure the conversation and topic words matched up\n",
    "if 'Topic_words2' in dfrel2.columns:\n",
    "    dfrel2.drop(columns=['Topic_words2', 'Conv2'], inplace=True)\n",
    "\n",
    "# Save df to disk\n",
    "timestamp = str(pd.Timestamp.now())[:19].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "dfrel2.to_pickle('C:\\\\tmp\\\\pickles\\\\relevance\\\\dfrel_' + timestamp + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5aa1a8fc-7310-444b-971d-51a56c108baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# This is a sanity check between topic words and cos sim\n",
    "# to make sure the topic-to-doc mapping in cos_sim matched the topic-to-doc mapping in doc_topics\n",
    "for i, row in dfr.iterrows():\n",
    "\n",
    "    # generate dict like this:\n",
    "    # {topicid: [docnum, docnum, ...]}\n",
    "\n",
    "    # Get doc topics\n",
    "    all_topics = row['Doc_topics']\n",
    "    if type(all_topics) == list:\n",
    "        tmp = {}\n",
    "        for topic in all_topics:\n",
    "            for k in topic.keys():\n",
    "                tmp[k] = topic[k]\n",
    "        all_topics = tmp\n",
    "\n",
    "    # Get cos sim\n",
    "    all_cos = row['Cosine_similarity']\n",
    "    if not pd.isna(all_cos):\n",
    "        all_cos = json.loads(all_cos)\n",
    "        tmp = {}\n",
    "        for topicid in all_cos.keys():\n",
    "            tmp[topicid] = [doc[0] for doc in all_cos[topicid]]\n",
    "        all_cos = tmp\n",
    "\n",
    "    # Make keys ints\n",
    "    tmp = {}\n",
    "    for topicid in all_topics:\n",
    "        tmp[int(topicid)] = all_topics[topicid]\n",
    "    all_topics = tmp\n",
    "    tmp = {}\n",
    "    if not pd.isna(all_cos):\n",
    "        for topicid in all_cos:\n",
    "            tmp[int(topicid)] = all_cos[topicid]\n",
    "        all_cos = tmp\n",
    "    \n",
    "    if all_cos != all_topics:\n",
    "        print(f'NOT THE SAME: {i}')\n",
    "        print(all_topics)\n",
    "        print(all_cos)\n",
    "        print()\n",
    "        pass\n",
    "    else:\n",
    "        #print(f'same: {i}')\n",
    "        pass\n",
    "    if pd.isna(all_cos):\n",
    "        print(i, row['Model'], all_topics)\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
